\chapter{Maintenance}
\label{cha:MAINTENANCE}\index{Maintenance}
The database maintenance is something crucial for the efficiency of the data access, the integrity 
and the reliability. Any database sooner or later will need a proper maintenance plan. \newline

When a new tuple's version is generated by an update it can be put everywhere there's free space. 
Frequent updates can result in tuples moving across the data pages many and many times leaving a 
trail of dead tuples behind them. Because the dead tuples are physically stored but no 
longer visible this creates an extra overhead causing the table to bloat.
Indices makes things more complicated because when a tuple changes page the index entry is updated 
to point the new page and because of the index's ordered structure, the bloating is more 
probable than the table. 


\section{VACUUM}\index{VACUUM}
\label{sec:VACUUM}
VACUUM is a PostgreSQL specific command which reclaims back the dead tuple's space. When called 
without specifying a target table, the command processes all the tables in the database. Running 
regulary VACUUM have some beneficial effects.

\begin{itemize}
 \item It reclaims back the dead tuple's disk space.
 \item It updates the visibility map making the index scans run faster.
 \item It freezes the tuples with old XID protecting from the XID wraparound\index{XID wraparound 
failure} 
data loss
\end{itemize}

The optional ANALYZE clause also gather the statistics on processed table, more details here 
\ref{sec:ANALYZE}.\newline

A standard VACUUM's run, frees the space used by the dead rows inside the data files but doesn't 
returns the space to the operating system. VACUUM doesn't affects the common database activity 
but prevents any schema change on the processed table. Because the pages are rewritten, a VACUUM 
run increases substantially the I/O activity. \newline

The presence of one or more empty pages in the table's end can be removed by VACUUM if an 
exclusive lock on the relation can be obtained immediately. When this happens the table is scanned 
backward to find all the empty pages and then it's truncated to the first not empty page. The index 
pages are scanned as well and the dead tuples are also cleared. The VACUUM's truncate scan works 
only on the heap data files. VACUUM'S performances are influenced by the maintenance\_work\_mem 
only if the table have indices, otherwise the VACUUM will run the cleanup sequentially without 
storing the tuple's references for the index cleanup.\newline

To show the effect of the maintenance\_work\_mem  let's build build a simple table with 10 
million rows. 


\begin{lstlisting}[style=pgsql]
postgres=# CREATE TABLE t_vacuum 
        (
                i_id serial,
                t_ts_value timestamp with time zone DEFAULT clock_timestamp(),
                t_value text,
                CONSTRAINT pk_t_vacuum PRIMARY KEY  (i_id)
        )
;
CREATE TABLE

postgres=# INSERT INTO t_vacuum
        (t_value)
SELECT 
         md5(i_cnt::text)
FROM
(
        SELECT
                generate_series(1,10000000) as i_cnt
) t_cnt
;
INSERT 0 10000000


\end{lstlisting}
To have a statical environment we'll disable the table's autovacuum and the analy. More infos on 
autovacuum here 
\ref{sec:AUTOVACUUM}.
We'll also increase the session's verbosity to look out what's happening during the VACUUM's 
 run.\newline
\begin{lstlisting}[style=pgsql]
postgres=# ALTER TABLE t_vacuum 
        SET 
                (
                        autovacuum_enabled = false, 
                        toast.autovacuum_enabled = false
                )
;
ALTER TABLE


SET client_min_messages='debug';

\end{lstlisting}

We are now executing a complete table rewrite running an UPDATE without the WHERE condition. 
This will create 10 millions of dead rows.\newline

\begin{lstlisting}[style=pgsql]
postgres=# UPDATE t_vacuum 
        SET 
                t_value = md5(clock_timestamp()::text)
;
UPDATE 10000000

\end{lstlisting}

Before running the VACUUM we'll change the maintenance\_work\_mem to a small value enabling the the 
timing to check the query duration.\newline

\begin{lstlisting}[style=pgsql]
postgres=# SET maintenance_work_mem ='20MB';
SET
postgres=# \timing
Timing is on.

postgres=# VACUUM t_vacuum;
DEBUG:  vacuuming "public.t_vacuum"
DEBUG:  scanned index "pk_t_vacuum" to remove 3495007 row versions
DETAIL:  CPU 0.80s/4.56u sec elapsed 21.36 sec.
DEBUG:  "t_vacuum": removed 3495007 row versions in 36031 pages
DETAIL:  CPU 0.63s/0.56u sec elapsed 19.31 sec.
DEBUG:  scanned index "pk_t_vacuum" to remove 3495007 row versions
DETAIL:  CPU 0.67s/4.18u sec elapsed 15.28 sec.
DEBUG:  "t_vacuum": removed 3495007 row versions in 36031 pages
DETAIL:  CPU 0.67s/0.53u sec elapsed 18.07 sec.
DEBUG:  scanned index "pk_t_vacuum" to remove 3009986 row versions
DETAIL:  CPU 0.53s/2.86u sec elapsed 12.29 sec.
DEBUG:  "t_vacuum": removed 3009986 row versions in 31031 pages
DETAIL:  CPU 0.47s/0.52u sec elapsed 20.06 sec.
DEBUG:  index "pk_t_vacuum" now contains 10000000 row versions in 82352 pages
DETAIL:  10000000 index row versions were removed.
0 index pages have been deleted, 0 are currently reusable.
CPU 0.00s/0.00u sec elapsed 0.00 sec.
DEBUG:  "t_vacuum": found 10000000 removable, 10000000 nonremovable row versions in 206186 out of 
206186 pages
DETAIL:  0 dead row versions cannot be removed yet.
There were 0 unused item pointers.
0 pages are entirely empty.
CPU 5.92s/17.08u sec elapsed 154.10 sec.
DEBUG:  vacuuming "pg_toast.pg_toast_28499"
DEBUG:  index "pg_toast_28499_index" now contains 0 row versions in 1 pages
DETAIL:  0 index row versions were removed.
0 index pages have been deleted, 0 are currently reusable.
CPU 0.00s/0.00u sec elapsed 0.00 sec.
DEBUG:  "pg_toast_28499": found 0 removable, 0 nonremovable row versions in 0 out of 0 pages
DETAIL:  0 dead row versions cannot be removed yet.
There were 0 unused item pointers.
0 pages are entirely empty.
CPU 0.00s/0.00u sec elapsed 0.00 sec.
VACUUM
Time: 154143.383 ms
postgres=# 


\end{lstlisting}

During the VACUUM the the maintenance\_work\_mem is used to store an array of TCID referencing 
the dead tuples for the index cleanup. If the maintenance\_work\_mem is small and the dead rows are 
many, the memory fills up often. When this happens the table scan pauses and the index is 
scanned searching for the tuples stored into the array. When the index scan is complete the array of 
TCID is emptied and the table's scan resumes. Increasing the maintenance\_work\_mem to 2 
GB\footnote{In order to have the table in the same conditions, a VACUUM FULL and a new update has 
been runt before the conventional VACUUM.} the index scan is executed in one single run 
resulting in a VACUUM 32 seconds faster.\newline

\begin{lstlisting}[style=pgsql]
postgres=# SET maintenance_work_mem ='2GB';
SET

postgres=# VACUUM t_vacuum;
DEBUG:  vacuuming "public.t_vacuum"
DEBUG:  scanned index "pk_t_vacuum" to remove 10000000 row versions
DETAIL:  CPU 1.58s/8.45u sec elapsed 52.41 sec.
DEBUG:  "t_vacuum": removed 10000000 row versions in 103093 pages
DETAIL:  CPU 1.78s/1.41u sec elapsed 33.90 sec.
DEBUG:  index "pk_t_vacuum" now contains 10000000 row versions in 82352 pages
DETAIL:  10000000 index row versions were removed.
0 index pages have been deleted, 0 are currently reusable.
CPU 0.00s/0.00u sec elapsed 0.00 sec.
DEBUG:  "t_vacuum": found 10000000 removable, 10000000 nonremovable row versions in 206186 out of 
206186 pages
DETAIL:  0 dead row versions cannot be removed yet.
There were 0 unused item pointers.
0 pages are entirely empty.
CPU 5.62s/13.64u sec elapsed 121.99 sec.
DEBUG:  vacuuming "pg_toast.pg_toast_28499"
DEBUG:  index "pg_toast_28499_index" now contains 0 row versions in 1 pages
DETAIL:  0 index row versions were removed.
0 index pages have been deleted, 0 are currently reusable.
CPU 0.00s/0.00u sec elapsed 0.00 sec.
DEBUG:  "pg_toast_28499": found 0 removable, 0 nonremovable row versions in 0 out of 0 pages
DETAIL:  0 dead row versions cannot be removed yet.
There were 0 unused item pointers.
0 pages are entirely empty.
CPU 0.00s/0.00u sec elapsed 0.00 sec.
VACUUM
Time: 122021.251 ms


\end{lstlisting}

A table without indices does not use the maintenance\_work\_mem. For example if we run 
the VACUUM after dropping the table's primary key the execution is faster even with the low 
maintenance\_work\_mem setting.\newline

\begin{lstlisting}[style=pgsql]

postgres=# SET maintenance_work_mem ='20MB';
SET
postgres=# \timing
Timing is on.

postgres=# ALTER TABLE t_vacuum DROP CONSTRAINT pk_t_vacuum;
DEBUG:  drop auto-cascades to index pk_t_vacuum
ALTER TABLE
Time: 182.737 ms

postgres=# VACUUM t_vacuum;
DEBUG:  vacuuming "public.t_vacuum"
DEBUG:  "t_vacuum": removed 10000000 row versions in 103093 pages
DEBUG:  "t_vacuum": found 10000000 removable, 10000000 nonremovable row versions in 206186 out of 
206186 pages
DETAIL:  0 dead row versions cannot be removed yet.
There were 0 unused item pointers.
0 pages are entirely empty.
CPU 2.16s/4.53u sec elapsed 47.30 sec.
DEBUG:  vacuuming "pg_toast.pg_toast_28499"
DEBUG:  index "pg_toast_28499_index" now contains 0 row versions in 1 pages
DETAIL:  0 index row versions were removed.
0 index pages have been deleted, 0 are currently reusable.
CPU 0.00s/0.00u sec elapsed 0.00 sec.
DEBUG:  "pg_toast_28499": found 0 removable, 0 nonremovable row versions in 0 out of 0 pages
DETAIL:  0 dead row versions cannot be removed yet.
There were 0 unused item pointers.
0 pages are entirely empty.
CPU 0.00s/0.00u sec elapsed 0.00 sec.
VACUUM
Time: 48823.132 ms




\end{lstlisting}

The table seen in the example begins with a size of 806 MB . After the update the table double its 
size which remains the same during the VACUUM runs the updates. This happens because after the 
first insert the table had all the rows packed together; the update added in the table's bottom the 
new row versions leaving the previous 10 millions row on the table's top as dead tuples. The 
VACUUM's run cleared the space on the table's top but weren't able to truncate because all 
the rows packed in the table's bottom. Running a new UPDATE followed by VACUUM would free the space 
in the table's bottom and a truncate scan would succeed but only if there's no tuple in the table's 
end free space. To check if the vacuum is running effectively the tables should show an initial 
growt followed by a substantial size stability in time. This happens only if the new rows are 
versioned at the same rate of the old rows clear down.\newline

The XID wraparound failure protection is performed automatically by VACUUM which when it finds a 
live tuple with a t\_xmin's age bigger than the GUC parameter vacuum\_freeze\_min\_age, then it 
replaces the tuple's creation XID with the FrozenXID preserving the tuple's visibility forever. 
Because VACUUM by default skips the pages without dead tuples it will miss some aging tuples. 
That's the reason why it's present a second GUC parameter, vacuum\_freeze\_table\_age, which 
triggers a VACUUM's full table scan when the table's relfrozenxid age exceeds the value.\newline

VACUUM accepts the FREEZE \index{VACUUM FREEZE} clause which forces a complete tuple freeze 
regardless to the age. That's equivalent to run the VACUUM setting the vacuum\_freeze\_min\_age to 
zero.

There are few GUC parameters controlling VACUUM

\subsection{vacuum\_freeze\_table\_age}
As seen before, VACUUM this parameter triggers a whole table scan if the table's 
pg\_class.relfrozenxid field is older than the parameter's value. The default is 150 million 
transactions. The accepted values are from zero to one billion. Anyway VACUUM will silently set the 
effective value to the 95\% of autovacuum\_freeze\_max\_age, in order to give more chance to the 
manual VACUUM to run before an anti-wraparound autovacuum.

\subsection{vacuum\_freeze\_min\_age}
Sets the age in transactions when VACUUM replaces the tuple's transaction IDs with the 
FrozenXID. The default is 50 million transactions. Same as for vacuum\_freeze\_table\_age tThe 
accepted values are from zero to one billion. With this parameter VACUUM will silently set the 
effective value to half the value of autovacuum\_freeze\_max\_age in order to increase the time 
between the forced autovacuums.

\subsection{vacuum\_multixact\_freeze\_min\_age}
As from 9.3 VACUUM take care also of the multiple transaction ID, used to store the row locks in 
the tuple's header.  Because the multixact ID\index{multixact ID} is implemented same as the XID 
the value suffers of the same wraparound failure\index{multixact ID, wraparound failure}. This 
parameter sets the age's limit for VACUUM to performs a whole table scan. The  table's 
pg\_class.relminmxid is used to check the transaction's age. . The default is 150 million 
multixacts. Also VACUUM limits the effective value to 95\% of 
autovacuum\_multixact\_freeze\_max\_age, so that a periodical manual VACUUM has a chance to run 
before an anti-wraparound is launched for the table.

\subsection{vacuum\_multixact\_freeze\_table\_age}
Sets the age in multixacts when VACUUM replaces the multixact IDs with a newer transaction ID or 
multixact ID while scanning a table. The default is 5 million multixacts. The accepted values 
are from zero to one billion but VACUUM will silently limit the effective value to half the value 
of autovacuum\_multixact\_freeze\_max\_age, in order to increase the time between forced 
autovacuums.

\subsection{vacuum\_defer\_cleanup\_age}
This parameter have effect only on the master in hot standby configurations. When set to a positive  
value on the master, can reduce the risk of query conflicts on the standby. Does not have 
effect on the standby though.


\subsection{vacuum\_cost\_delay}\label{sub:VACUUMCOST}
This parameter, if set to a not zero value enables the cost based vacuum delay\index{VACUUM, cost 
based delay} and sets the time, in milliseconds, that the VACUUM process sleeps when the cost limit 
has been exceeded. The default value is zero, which disables the cost-based vacuum delay feature. 

\subsection{vacuum\_cost\_limit}
This parameter sets the value when the internal counter puts the VACUUM process to sleep. The 
default value is 200. 


\subsection{vacuum\_cost\_page\_hit}
The counter's cost is determined using arbitrary values for each vacuum operation. This parameter 
sets the estimated cost for vacuuming one buffer found in the shared buffer cache. It represents 
the cost to lock the buffer pool, lookup the shared hash table and scan the content of the page. The 
default value is one.

\subsection{vacuum\_cost\_page\_miss}
This parameter sets the estimated cost for vacuuming a buffer not present in the shared buffer. This 
represents the effort to lock the buffer pool, lookup the shared hash table, read the desired block 
in from the disk and scan its content. The default value is 10.

\subsection{vacuum\_cost\_page\_dirty}
This parameter sets the estimated cost charged when vacuum changes a previously clean block. It 
represents the extra I/O required to flush the dirty block out to disk again. The default value is 
20.

\section{ANALYZE}
\label{sec:ANALYZE}
The PostgreSQL's query optimiser is based on the costs estimates. When building the execution plans 
the planner consults the internal statistics and assigns to each node plan an estimated cost. The 
plan with the smallest total estimated cost is then executed. Having up to date and 
accurate statistics will help the database to keep up the performances.\newline

The ANALYZE\index{ANALYZE} command is used to gather the usage statistics. When launched reads the 
data, builds up the statistics and stores them into the pg\_statistics\index{pg\_statistics, 
table} system table. The command accepts the optional clause VERBOSE to increase verbosity and the 
optional target table and the column list. If ANALYZE is launched with no parameters it 
processes all the tables in the database. Launching ANALYZE with the table name only, will process 
all the table's columns.\newline

When working on large tables ANALYZE runs a sample random read on a table's portion.  The GUC 
parameter default\_statistics\_target determines the amount of entries read by the sample. The 
default limit is 100. Increasing the value will cause the planner to get better estimates. in 
particular for columns having data distributed irregularly. This accuracy costs more time for the 
statistics gathering and space because requires a bigger storage in the pg\_statistics 
table.\newline


To show how the default\_statistics\_target can affects the estimates, let's run an ANALYZE VERBOSE 
with the default setting on the table created in \ref{sec:VACUUM}.

\begin{lstlisting}[style=pgsql]
postgres=# SET default_statistics_target =100;
SET
postgres=# ANALYZE VERBOSE t_vacuum;
INFO:  analyzing "public.t_vacuum"
INFO:  "t_vacuum": scanned 30000 of 103093 pages, containing 2909979 live rows and 0 dead rows; 
30000 rows in sample, 9999985 estimated total rows
ANALYZE
\end{lstlisting}

Even if the table have 10 million rows, the analyse estimates only 2,909,979 rows, the 30\% of the 
total effective storage.

Changing the default\_statistics\_target to its maximum value of 10000 ANALYZE will get 
better estimates.


\begin{lstlisting}[style=pgsql]
SET
postgres=# ANALYZE VERBOSE t_vacuum;
INFO:  analyzing "public.t_vacuum"
INFO:  "t_vacuum": scanned 103093 of 103093 pages, containing 10000000 live rows and 0 dead rows; 
3000000 rows in sample, 10000000 estimated total rows
ANALYZE
\end{lstlisting}

This time the table estimate is correctly determined in 10 millions live rows.\newline
The pg\_statistics stores the gathered data for the database usage only. To help users and 
administrators it's present the view pg\_stats\index{pg\_stats, view} providing a human readable 
visualization of the gathered statistics.\newline

As general rule, before starting any performance tuning, it's important to check if 
database statistics are recent and accurate. The information is stored into the view 
pg\_stat\_all\_tables \footnote{The subset views pg\_stat\_user\_tables and pg\_stat\_sys\_tables 
are useful to search respectively the current user and the system tables only.}.

For example this query gets the last execution of the manual and the auto vacuum with the 
analyze and auto analyze, for a given table.

\begin{lstlisting}[style=pgsql]

postgres=# \x
Expanded display is on.
postgres=# SELECT
        schemaname,
        relname,
        last_vacuum,
        last_autovacuum,
        last_analyze,
        last_autoanalyze
FROM
         pg_stat_all_tables
WHERE
        relname='t_vacuum'
;
-[ RECORD 1 ]----+------------------------------
schemaname       | public
relname          | t_vacuum
last_vacuum      | 
last_autovacuum  | 
last_analyze     | 2014-06-17 18:48:56.359709+00
last_autoanalyze | 

postgres=# 



\end{lstlisting}


The statistics target can be set per column to fine tune the ANALYZE with the ALTER TABLE SET 
STATISTICS statement.

\begin{lstlisting}[style=pgsql]


--SET THE STATISTICS TO 1000 ON THE COLUMN i_id
ALTER TABLE t_vacuum 
        ALTER COLUMN  i_id 
                        SET STATISTICS 1000
;

\end{lstlisting}

The SET command can be used for changing the default statistics target in the 
current session. Otherwise it's possible to change the value cluster wide changing the parameter 
in the postgresql.conf file.

\section{REINDEX}\label{sec:REINDEX}
A B-tree index entry carries the indexed value with the pointer to the corresponding heap page
where the full tuple is stored. The pages are organised in the form of a balanced tree 
referring each other using the special space seen in \ref{fig:INDEX01}. Until the heap tuple 
does not change page, the index entry doesn't need update. The HOT strategy\index{HOT strategy} 
tries to achieve this goal keeping the heap tuples in the same page. When the heap tuple's page 
change then the index entry needs to be updates as well. Having the index pages fillfactor of 70 for 
the not leaf pages and a fillfactor of 90 for the leaf pages\footnote{Changing the index's 
fillfactor affects only 
the leaf pages, the value for the not leaf cannot change} the index tuple's page 
change is somewhat limited.\newline

The VACUUM is not so effective with the indices, even clearing down the dead tuple's space this is 
reusable only if compatible with the B-tree position. Only the completely empty pages can be 
recycled and this requires at least two VACUUM runs, because when an index page is empty 
then is marked as deleted, it's stamped with the next XID and becomes invisible. The empty page 
then is not immediately recycled because there might be running scans which need to access the page 
which is still visible in their snapshot. The next VACUUM run will ensure the page is no longer 
required by the running transactions and the it marks as reusable.\newline

For those reasons the indices are affected by the data bloat more than the tables. Alongside 
with a bigger disk space allocation, the bloat results in a generally bad index's performances. 
The reindex is then the only solution to restore an index good shape.\newline

Unlike the VACUUM, REINDEX have a substantial impact on the database activity. To ensure 
the data is consistently read the REINDEX sets a table lock which prevents the table's writes. The 
reads are also blocked for the SELECT queries using the rebuilding index.\newline

A B-tree index build requires a data sort to build the balanced tree. PostgreSQL comes with a handy 
GUC parameter to track the sort, the trace\_sort\index{trace\_sort}. 
The message verbosity level by the trace is DEBUG, then also the client\_min\_messages needs 
adjustment for getting the informations from the trace. Let's then reindex the table's primary key 
seen in \ref{sec:VACUUM}.

\begin{lstlisting}[style=pgsql]
postgres=# SET trace_sort =on;
SET
postgres=# SET client_min_messages ='debug';
SET
postgres=# \timing
Timing is on.

postgres=# REINDEX INDEX pk_t_vacuum ;
DEBUG:  building index "pk_t_vacuum" on table "t_vacuum"
LOG:  begin index sort: unique = t, workMem = 16384, randomAccess = f
LOG:  begin index sort: unique = f, workMem = 1024, randomAccess = f
LOG:  switching to external sort with 59 tapes: CPU 0.00s/0.08u sec elapsed 0.08 sec
LOG:  finished writing run 1 to tape 0: CPU 0.13s/3.41u sec elapsed 3.55 sec
LOG:  internal sort ended, 25 KB used: CPU 0.39s/8.35u sec elapsed 8.74 sec
LOG:  performsort starting: CPU 0.39s/8.35u sec elapsed 8.74 sec
LOG:  finished writing final run 2 to tape 1: CPU 0.39s/8.50u sec elapsed 8.89 sec
LOG:  performsort done (except 2-way final merge): CPU 0.40s/8.51u sec elapsed 8.90 sec
LOG:  external sort ended, 24438 disk blocks used: CPU 0.70s/9.67u sec elapsed 11.81 sec
REINDEX
Time: 11876.807 ms

\end{lstlisting}

The reindex performs a data sort which does not fit in the maintenance\_work\_mem. PostgreSQL then 
starts a slower disk sort to build up the index. The first LOG entry with \textit{begin index 
sort:} shows the available maintenance\_work\_mem for the index sort. If after the table scan the 
available memory is exhausted then an external sort on disk will happen. Otherwise a faster sort in 
memory will build the index. Increasing then the maintenance\_work\_mem  can improve the reindex. 
Unfortunately the determining the value when the sort in memory happens is not simple and can just 
be guessed from the index size. The previous reindex with 1 GB maintenance\_work\_mem runs 40\% 
faster.

\begin{lstlisting}[style=pgsql]
postgres=# \timing
Timing is on.
postgres=# SET maintenance_work_mem='1GB';
SET
Time: 0.193 ms
postgres=# REINDEX INDEX pk_t_vacuum ;
DEBUG:  building index "pk_t_vacuum" on table "t_vacuum"
LOG:  begin index sort: unique = t, workMem = 1048576, randomAccess = f
LOG:  begin index sort: unique = f, workMem = 1024, randomAccess = f
LOG:  internal sort ended, 25 KB used: CPU 0.45s/2.02u sec elapsed 2.47 sec
LOG:  performsort starting: CPU 0.45s/2.02u sec elapsed 2.47 sec
LOG:  performsort done: CPU 0.45s/4.36u sec elapsed 4.81 sec
LOG:  internal sort ended, 705717 KB used: CPU 0.66s/4.74u sec elapsed 6.85 sec
REINDEX
Time: 6964.196 ms


\end{lstlisting}

The reindex create a completely new file node for the index and when the build is complete, the 
pg\_class entry is then updated with the new relfilenode value and old file node is deleted. The 
entire sequence can be emulated creating a new index with a different name with the CREATE 
INDEX\index{CREATE INDEX} statement. After the index is ready, dropping the old one, renaming 
the new index to the old name, will result in a brand new index without blocking the 
reads.\newline

Since the version 8.2 PostgreSQL supports the CREATE INDEX CONCURRENTLY\index{CREATE INDEX 
CONCURRENTLY} which doesn't block reads nor writes. Using this method, the index creation 
starts adding a new invalid index in the system catalogue starting a first table scan to builds 
the entries without caring for the changing data. When the first build is complete a second table 
scan fixes the not consistent tuple's references and finally the index's status is set to valid 
becoming available. This approach, combined with the swap strategy can limit greatly the 
impact of the index maintenance. \newline

The concurrent index build have indeed some caveats and limitations.
\begin{itemize}
 \item Any problem with the table scan will fail the command and leave behind an invalid index 
which is ignored for the reads but adds overhead for the inserts and updates.
\item When building an unique index concurrently this start enforcing the uniqueness when the 
second table scan starts. Some transactions could then start reporting the uniqueness violation 
before the index becomes available and if the build fails during the second scan then 
the \textit{invalid} index continues to enforce the uniqueness.
\item Regular index builds can run in parallel on the same table. Concurrent index builds cannot.
\item Concurrent index builds cannot run within a transaction block.
\end{itemize}

With the primary keys and unique constraints is also possible to use the index swap strategy with 
an extra little trick. PostgreSQL since the version 9.1 supports the \textit{ALTER TABLE table\_name 
ADD table\_constraint using\_index} statement. Combining this statement in a single command with a 
DROP CONSTRAINT it's possible to swap the constraint's index without losing the uniqueness 
enforcement.

\begin{lstlisting}[style=pgsql]
postgres=# CREATE UNIQUE INDEX pk_t_vacuum_new 
                ON  t_vacuum USING BTREE (i_id);
CREATE INDEX
postgres=# ALTER TABLE t_vacuum
                DROP CONSTRAINT pk_t_vacuum,
                ADD CONSTRAINT pk_t_vacuum_new PRIMARY KEY  
                        USING INDEX pk_t_vacuum_new
           ;
ALTER TABLE
postgres=# ALTER INDEX pk_t_vacuum_new 
                RENAME TO pk_t_vacuum;
ALTER INDEX

\end{lstlisting}

The example uses a regular index build and then blocks the writes. It's also possible to 
build the new index concurrently.

As this method works well for the unreferenced primary or unique keys, any foreign key 
referencing the unique field will cause the drop constraint's failure.

\begin{lstlisting}[style=pgsql]
 postgres=# CREATE TABLE t_vac_foreign
                                        (
                                                i_foreign serial,
                                                i_id integer NOT NULL,
                                                t_value text
                                        )
            ;
CREATE TABLE
postgres=# ALTER TABLE t_vac_foreign 
                ADD CONSTRAINT fk_t_vac_foreign_t_vacuum_i_id
                        FOREIGN KEY (i_id) 
                        REFERENCES t_vacuum (i_id) 
                        ON DELETE CASCADE 
                        ON UPDATE RESTRICT;
ALTER TABLE

postgres=# CREATE UNIQUE INDEX pk_t_vacuum_new ON  t_vacuum USING BTREE (i_id);
CREATE INDEX
postgres=# ALTER TABLE t_vacuum
postgres-# DROP CONSTRAINT pk_t_vacuum,
postgres-# ADD CONSTRAINT pk_t_vacuum_new PRIMARY KEY  USING INDEX pk_t_vacuum_new;
ERROR:  cannot drop constraint pk_t_vacuum on table t_vacuum because other objects depend on it
DETAIL:  constraint fk_t_vac_foreign_t_vacuum_i_id on table t_vac_foreign depends on index 
pk_t_vacuum
HINT:  Use DROP ... CASCADE to drop the dependent objects too.


\end{lstlisting}

In this case the safest way to proceed is the conventional REINDEX. 

\begin{comment}


\end{comment}


\section{VACUUM FULL and CLUSTER}\index{VACUUM FULL}\index{CLUSTER}
\label{sec:VACFULL}
The CLUSTER command rebuilds a completely new table with the tuples in the same order of the 
clustered index. The clustered index can be set using the command \textit{ALTER TABLE table\_name 
CLUSTER ON index\_name} and is used as sort key in the next CLUSTER. 
In order to get a clearer picture, let's cluster the table created in \ref{sec:VACUUM} its
primary key. 

\begin{lstlisting}[style=pgsql]
postgres=# SET trace_sort='on';
SET
postgres=# set client_min_messages ='debug';
SET
postgres=# ALTER TABLE t_vacuum CLUSTER ON pk_t_vacuum;
ALTER TABLE
postgres=# CLUSTER t_vacuum;
DEBUG:  building index "pg_toast_28608_index" on table "pg_toast_28608"
LOG:  begin index sort: unique = t, workMem = 16384, randomAccess = f
LOG:  begin index sort: unique = f, workMem = 1024, randomAccess = f
LOG:  internal sort ended, 25 KB used: CPU 0.00s/0.00u sec elapsed 0.00 sec
LOG:  performsort starting: CPU 0.00s/0.00u sec elapsed 0.00 sec
LOG:  performsort done: CPU 0.00s/0.00u sec elapsed 0.00 sec
LOG:  internal sort ended, 25 KB used: CPU 0.00s/0.00u sec elapsed 0.06 sec
DEBUG:  clustering "public.t_vacuum" using index scan on "pk_t_vacuum"
DEBUG:  "t_vacuum": found 0 removable, 10000000 nonremovable row versions in 103093 pages
DETAIL:  0 dead row versions cannot be removed yet.
CPU 1.16s/3.43u sec elapsed 7.41 sec.
DEBUG:  building index "idx_ts_value" on table "t_vacuum"
LOG:  begin index sort: unique = f, workMem = 16384, randomAccess = f
LOG:  switching to external sort with 59 tapes: CPU 0.01s/0.07u sec elapsed 0.09 sec
LOG:  performsort starting: CPU 0.45s/9.13u sec elapsed 9.57 sec
LOG:  finished writing final run 1 to tape 0: CPU 0.45s/9.27u sec elapsed 9.71 sec
LOG:  performsort done: CPU 0.45s/9.27u sec elapsed 9.71 sec
LOG:  external sort ended, 24439 disk blocks used: CPU 0.76s/10.02u sec elapsed 12.57 sec
DEBUG:  building index "pk_t_vacuum" on table "t_vacuum"
LOG:  begin index sort: unique = f, workMem = 16384, randomAccess = f
LOG:  switching to external sort with 59 tapes: CPU 0.02s/0.05u sec elapsed 0.08 sec
LOG:  performsort starting: CPU 0.44s/8.35u sec elapsed 8.79 sec
LOG:  finished writing final run 1 to tape 0: CPU 0.45s/8.49u sec elapsed 8.93 sec
LOG:  performsort done: CPU 0.45s/8.49u sec elapsed 8.93 sec
LOG:  external sort ended, 24439 disk blocks used: CPU 0.80s/9.20u sec elapsed 11.64 sec
DEBUG:  drop auto-cascades to type pg_temp_28553
DEBUG:  drop auto-cascades to type pg_temp_28553[]
DEBUG:  drop auto-cascades to toast table pg_toast.pg_toast_28608
DEBUG:  drop auto-cascades to index pg_toast.pg_toast_28608_index
DEBUG:  drop auto-cascades to type pg_toast.pg_toast_28608
CLUSTER

\end{lstlisting}

This CLUSTER's run performs a full index scan on the clustered index avoiding the table sort. The 
tuples are then stored into a new file node. When the build is complete then the relation's file 
node is swapped in the system catalogue and indices are reindexed. When the command completes the 
old file node is then removed. The entire process requires an exclusive lock on the table preventing 
the reads and the writes. Also the storage is a critical point because the disk space 
requirements are for the old relation plus the new one with the indices and the eventual sort on 
disk.\newline

Looking at source code in \textbf{src/backend/commands/cluster.c}, is clearly stated in the 
file's header that \textit{CLUSTER a table on an index.  This is now also used for VACUUM FULL}. 
The only difference between VACUUM FULL and CLUSTER, is the clustered index's OID validity. If 
it's valid then the data output is sorted on the clustered index. How the data is sorted is 
determined by the planner, which choice is the cheapest between an index scan and the sequential 
scan with a data sort. Otherwise, if the index's OID is invalid then the tuples are read using a 
plain sequential scan.\newline

VACUUM FULL and CLUSTER have beneficial effects on the storage as the space is returned to the 
operating system. Also, regenerating completely the relation's files with the reindex, it makes the 
page access more efficient and CLUSTER rebuilds the table on the clustered index order. This 
minimise the random disk seeks when accessing the data via clustered index.\newline

The disadvantages for using those commands are the complete stop of the affected table's activity. 
Also, CLUSTER and VACUUM FULL do not fix the XID wraparound risk which the conventional VACUUM 
does.\newline

As rule of thumb, in order to minimise the database's downtime, CLUSTER and the VACUUM FULL should 
be used only for extraordinary maintenance and only if the disk space is critical. For the day to 
day maintenance it's best to rely on VACUUM and occasionally the reindex as seen in \ref{sec:VACUUM} 
and \ref{sec:REINDEX}.
 


\section{The autovacuum}\index{AUTOVACUUM}
\label{sec:AUTOVACUUM}
The autovacuum daemon appeared with the revolutionary PostgreSQL version 8.0. With the version 8.3 
was also enabled by default because reliable and efficient. Having the autovacuum turned on is a 
good idea because all the maintenance is done automatically by the system. The number of workers to 
start is not simple to determine. Each process consumes a connection slot and changing the 
number of workers requires the cluster's restart. Turning off autovacuum does't disable it, the 
worker starts automatically to vacuum tables near to the transacion ID and multixact ID wraparound 
failure. In order to have autovacuum working the statistic collector must be enabled with 
track\_counts= 'on'.\newline


The autovacuum behaviour is controlled using few GUC parameters.

\subsection{autovacuum} 
This parameter is used to enable or disable the autovacuum daemon. Changing the setting requires 
the cluster's restart. Turning autovacuum off never disables the daemon completely. The autovacuum 
process will start in any case for tables with XID older than autovacuum\_freeze\_max\_age

\subsection{autovacuum\_max\_workers} 
The parameter sets the maximum number of autovacuum subprocesses. Changing the setting requires the 
cluster's restart and each subprocess consumes one PostgreSQL connection.

\subsection{autovacuum\_naptime} 
The parameter sets the delay between two autovacuum runs on a specified database.The delay is 
measured in seconds and the default value is 1 minute.


\subsection{autovacuum\_vacuum\_scale\_factor}
This parameter and the next one controls when the autovacuum is triggered. This one specifies the 
fraction of table to add to autovacuum\_vacuum\_threshold in order to determine whether start the 
vacuum. The default is 0.2,  20\% of the table. This setting can be overridden for individual 
tables by changing storage parameters.

\subsection{autovacuum\_vacuum\_threshold}
This parameter sets the minimum number of a table's updated or deleted tuples needed to trigger 
a VACUUM. The default is 50 tuples. This setting can be overridden for individual 
tables by changing storage parameters. For example, if we have a 10 million rows table with both 
parameters set to default, the autovacuum will start after 2,000,050 update or delete.

\subsection{autovacuum\_analyze\_scale\_factor}
This parameter and the next one controls when the auto analyse is triggered. This one specifies the 
fraction of table to add to autovacuum\_analyze\_threshold in order to determine whether start the 
vacuum. The default is 0.1,  10\% of the table. This setting can be overridden for individual 
tables by changing storage parameters.

\subsection{autovacuum\_analyze\_threshold}
This parameter sets the minimum number of a table's updated or deleted tuples needed to trigger an 
ANALYZE. The default is 50 tuples. This setting can be overridden for individual 
tables by changing storage parameters. For example, if we have a 10 million rows table with both 
parameters set to default, the autovacuum will start after 1,000,050 update or delete.

\subsection{autovacuum\_freeze\_max\_age}
The parameter sets the maximum age of the table's pg\_class.relfrozenxid, in transactions, after 
the VACUUM is forced to avoid the transaction ID wraparound. The process will start also if the 
autovacuum is disabled. The parameter can be set only at server's start but is possible to reduce 
the value per table by changing the storage parameter.

\subsection{autovacuum\_multixact\_freeze\_max\_age}
The parameter sets the maximum age of the table's pg\_class.relminmxid, in transactions, after 
the VACUUM is forced to avoid the multixact ID wraparound. The process will start also if the 
autovacuum is 
disabled. The parameter can be set only at server's start but is possible to reduce the value per 
table by changing the storage parameter.

\subsection{autovacuum\_vacuum\_cost\_delay}
The parameter sets the cost delay to use in automatic VACUUM operations. If set to -1, the regular 
vacuum\_cost\_delay value will be used. The default value is 20 milliseconds. 

\subsection{autovacuum\_vacuum\_cost\_limit}
The parameter sets  cost limit value to be used in automatic VACUUM operations. 
If set to -1 then the regular vacuum\_cost\_limit value will be used. The default value is -1.
The value is distributed among the running autovacuum workers. The sum of the limits of each worker 
never exceeds this variable. More informations on cost based vacuum here \ref{sub:VACUUMCOST}.



