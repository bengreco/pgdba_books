\chapter{The logical layout}
\label{cha:LOGICLAY}\index{Logical layout}
In this we'll take a look to the PostgreSQL logical layout.
We'll start with the connection process. Then we'll see the logical relations like tables, indices
and views. The chapter will end with the tablespaces and the MVCC.

\section{The connection}
When a client starts a connection to a running cluster, the process pass through few steps. \newline

The first connection's stage is the check using the host based authentication. The cluster scans the
pg\_hba.conf file searching a  match for the connection's parameters. Those are, for example, the
client's host, the user etc. The host file is usually saved inside the the data area alongside the
configuration file postgresql.conf. The pg\_hba.conf is read from the top to the bottom and the
first matching row for the client's parameters is used to determine the authentication method to
use. If PostgreSQL reaches the end of the file without match the connection is refused.\newline

The pg\_hba.conf structure is shown in \ref{tab:PGHBA}

\begin{table}[H]
  \begin{tabular}{ccccc}
    Type & Database & User & Address & Method \\ 
    \hline
    local & name & name & ipaddress/network mask & trust\\
    host & * & * & host name & reject\\
    hostssl & &  &  & md5\\
    hostnossl & &  &  & password \\
    & & &  & gss \\
    & & &  & sspi \\
    & & &  & krb5 \\
    & & &  & ident \\
    & & &  & peer \\
    & & &  & pam \\
    & & &  & ldap \\
    & & &  & radius \\
    & & &  & cert \\
  \end{tabular}
  \caption{\label{tab:PGHBA}pg\_hba.conf}
\end{table}

The column type specifies if the connection is local or host. The former is when the connection is
made using a socket. The latter when the connection uses the network. It's also possible to
specify if the host connection should be secure or plain using hostssl and hostnossl.\newline

The Database and User columns are used to match specific databases and users.\newline

The column address have sense only if the connection is host, hostssl or hostnossl. The value can
be an ip address plus the network mask. Is also possible to specify the hostname. There is the
full support for ipv4 and ipv6.

The pg\_hba.conf's last column is the authentication method for the matched row. The action to
perform after the match is done. PostgreSQL supports many methods ranging from the plain password
challenge to kerberos.\newline

We'll now take a look to the built in methods.

\begin{itemize}
 \item \textbf{trust}: The connection is authorised without any further action. Is quite useful 
if the password is lost. Use it with caution.

\item \textbf{peer}: The connection is authorised if the OS user matches the 
database user. It's useful for the local connections. 

\item \textbf{password}: The connection establishes if the connection's user and the password
matches with the values stored in the pg\_shadow system table. This method sends the password in
clear text. Should be used only on trusted networks.

\item \textbf{md5}: This method is similar to password. It uses a better security encoding the
passwords using the md5 algorithm. Because md5 is deterministic, there is pseudo random
subroutine which prevents to have the same string sent over the network.

\item \textbf{reject}: The connection is rejected. This method is very useful to keep the sessions
out of the database. e.g. maintenance requiring single user mode.

\end{itemize}

When the connection establishes the postgres main process forks a new backend process attached to
the shared buffer. The fork process is expensive. This makes the connection a potential
bottleneck. Opening new connections can degrade the operating system performance and eventually
produce zombie processes. Keeping the connections constantly connected maybe is a reasonable fix.
Unfortunately this approach have a couple of unpleasant side effects.\newline

Changing any connection related parameter like the max\_connections, requires a cluster restart.
For this reason planning the resources is absolutely vital. For each connection present in
 max\_connections the cluster allocates 400 bytes of shared memory. For each connection established 
the cluster allocates a per user memory area wich size is determined by the parameter
work\_mem.\newline

For example let's consider a cluster with a shared\_buffer set to 512 MB and the work\_mem 
set to 100MB. Setting the max\_connections to only 500 requires a potentially 49 GB of total memory
if all the connections are in use. Because the work\_mem can affects the performances, its
value should be determined carefully. Being a per user memory any change to work\_mem does not
require the cluster's start but a simple reload.\newline 

In this kind of situations a connection pooler can be a good solutions. The sophisticated
\href{http://www.pgpool.net/}{pgpool}  or the
lightweight \href{http://pgfoundry.org/projects/pgbouncer/}{pgbouncer}  can help to boost the
connection's performance.\newline

By default a fresh data area initialisation listens only on the localhost. The GUC parameter
governing this aspect is listen\_addresses. In order to have the cluster accepting connections from
the rest of the network the values should change to the correct listening addresses specified
as values separated by commas. It's also possible to set it to * as wildcard.

Changing the parameters max\_connections and listen\_addresses require the cluster restart.



\section{Databases}
\label{sec:DATABASES}
Unlikely other DBMS, a PostgreSQL connection requires the database name in the connection string.
Sometimes this can be omitted in psql when this information is supplied in another way.\newline

When omitted psql checks if the environment variable \$PGDATABASE \index{\$PGDATABASE variable} is 
set. If \$PGDATABASE is missing then psql defaults the database name to connection's username. This 
leads to confusing error messages. For example, if we have a username named test but not a database 
named test the connection will fail even with the correct credentials.

\begin{verbatim}
postgres@tardis:~$ psql -U test -h localhost
Password for user test: 
psql: FATAL:  database "test" does not exist
\end{verbatim}

This error appears because the pg\_hba.conf allow the connection for any database. Even for a not
existing one. The connection is then terminated when the backend ask to connect to the database 
named test which does not exists.\newline

This is very common for the new users. The solution is incredibly simple because in a PostgreSQL 
cluster there are at least three databases. Passing the name template1 as last parameter will do 
the trick.

\begin{verbatim}
postgres@tardis:~$ psql -U test -h localhost template1
Password for user test: 
psql (9.3.4)
SSL connection (cipher: DHE-RSA-AES256-SHA, bits: 256)
Type "help" for help.
\end{verbatim}

When the connection is established we can query the system table pg\_database to get the 
cluster's database list. 

\begin{lstlisting}[style=pgsql]
template1=> SELECT datname FROM pg_database;
    datname    
---------------
 template1
 template0
 postgres
(3 rows)

\end{lstlisting}

Database administrators coming from other DBMS can be confused by the postgres database.
This database have nothing special. Its creation was added since the version 8.4 because it was 
useful to have it. You can just ignore it or use it for testing purposes. Dropping the postgres 
database does not corrupts the cluster. Because this database is often used by third party 
tools before dropping it check if is in use in any way.\newline

The databases template0 and template1 \index{template1 database} \index{template0 
database} like the name suggests are the template databases. A template database \index{template 
database} is used to build new database copies via the physical file copy. 

When initdb initialises the data area the database template1 is populated with the correct
references to the WAL records, the system views and the procedural language PL/PgSQL. When
this is done the database template0 and the postgres databases are then created using the template1
database.

The database template0 doesn't allow the connections. It's main usage is to rebuild the
database template1 if it gets corrupted or for creating databases with a character encoding/ctype, 
different from the cluster wide settings. 
\index{CREATE DATABASE}

\begin{lstlisting}[style=pgsql]
postgres=# CREATE DATABASE db_test WITH ENCODING 'UTF8' LC_CTYPE 'en_US.UTF-8';
ERROR:  new LC_CTYPE (en_US.UTF-8) is incompatible with the LC_CTYPE of the 
template database (en_GB.UTF-8)
HINT:  Use the same LC_CTYPE as in the template database, or use template0 as 
template.

postgres=# CREATE DATABASE db_test WITH ENCODING 'UTF8' LC_CTYPE 'en_US.UTF-8' 
TEMPLATE template0;
CREATE DATABASE
postgres=# 

\end{lstlisting}

If the template is omitted the CREATE DATABASE statement will use template1 by default. 


A database can be renamed or dropped with ALTER DATABASE and DROP DATABASE \index{ALTER 
DATABASE}\index{DROP DATABASE} statements. Those operations require the exclusive access to the 
affected database. If there are connections established the drop or rename will fail.

\begin{lstlisting}[style=pgsql]
postgres=# ALTER DATABASE db_test RENAME TO db_to_drop;
ALTER DATABASE

postgres=# DROP DATABASE db_to_drop;
DROP DATABASE

\end{lstlisting}




\section{Tables}\index{Tables}
\label{sec:TABLES}
In our top down approach to the PostgreSQL's logical model, the next step is the relation.
In the PostgreSQL jargon a relation is an object which carries the data or the way to
retrieve the data. A relation can have a physical counterpart or be purely logical. We'll take a 
look in particular to three of them starting with the tables.\newline

A table is the fundamental storage unit for the data. PostgreSQL implements many kind of tables
with different levels of durability. A table is created using the SQL command CREATE TABLE. The data
is stored into a table without any specific order. Because the MVCC implementation a row update can
change the row's physical position. For more informations look to \ref{sec:MVCC}. PostgreSQL
implements three kind of tables.

\subsection{Logged tables}\index{Logged tables}
By default CREATE TABLE creates a logged table. This kind of table implements the durability
logging any change to the write ahead log. The data pages are loaded in the shared buffer and any
change to them is logged first to the WAL. The consolidation to the the data file happens later. 

\subsection{Unlogged tables}\index{Unlogged tables}
\label{sub:UNLOGGEDTABLES}
An unlogged table have the same structure like the logged table. The difference is such kind of 
tables are not crash safe. The data is still consolidated to the data file but the pages modified 
in memory do not write their changes to the WAL. The main advantage is the write operations which 
are considerably faster at the cost of the data durability. The data stored into an ulogged table
should be considered partially volatile. The database will truncate those tables when the crash
recovery occurs. Because the unlogged table don't write to the WAL, those tables are not accessible 
on a physical standby. 

\subsection{Temporary tables}\index{Temporary tables}
A temporary table is a relation which lives into the backend's local memory. When the connection 
ends the table is dropped. Those table can have the same name for all the sessions because 
they are completely isolated. If the amount of data stored into the table is lesser than 
the temp\_buffers value the table will fit in memory with great speed advantage. Otherwise the 
database will create a temporary relation on disk. The parameter temp\_buffers can be altered for 
the session but only before the first temporary table is created. 


\subsection{Foreign tables}\index{Foreign tables}
The foreign tables were first introduced with PostgreSQL 9.1 as read only relations, improving 
considerably the DBMS interconnection's capability. A foreign table works exactly like a local table 
using a foreign data
wrapper to interact with the foreign data source.\newline

There are many different foreign data wrappers available for very exotic data sources. With the 
PostgreSQL 9.3 there is the postgres\_fdw and the the foreign tables are writable. In particular 
the postgres\_fdw implementation is similar to old dblink module with a more efficient performance 
management and the connection's caching.

\section{Table inheritance}\index{Table inheritance}
Being more precise, PostgreSQL is an Object Relational Database Management System. Its intenal logic
implements some of the concepts of the object oriented programming. The relations are also called 
classes and the table's columns attributes. \newline

The table inheritance is a logical relationship between a parent table and one or more child 
tables. The child tables inherit the parent's attribute structure but not the physical storage. 

\begin{lstlisting}[style=pgsql]
db_test=#CREATE TABLE t_parent
                      (
                          i_id_data     integer,
                          v_data        character varying(300)
                      );

CREATE TABLE                     

db_test=#CREATE TABLE t_child_01
                      
                      ()
             INHERITS (t_parent)
                      ;                      
db_test=# \d t_parent
            Table "public.t_parent"
  Column   |          Type          | Modifiers 
-----------+------------------------+-----------
 i_id_data | integer                | 
 v_data    | character varying(300) | 
Number of child tables: 1 (Use \d+ to list them.)

db_test=# \d t_child_01 
           Table "public.t_child_01"
  Column   |          Type          | Modifiers 
-----------+------------------------+-----------
 i_id_data | integer                | 
 v_data    | character varying(300) | 
Inherits: t_parent

\end{lstlisting}

The inheritance can be defined at creation time or later. It's also possible to create a completely 
new table and create the inheritance with another one. In this case the child structure must match 
exactly the parent.

\begin{lstlisting}[style=pgsql]

db_test=# ALTER TABLE t_child_01 NO INHERIT t_parent;
ALTER TABLE
db_test=# ALTER TABLE t_child_01 INHERIT t_parent;
ALTER TABLE

\end{lstlisting}

Because the physical storage is not shared the unique constraints aren't globally enforced on the 
inheritance tree and this prevents the foreign keys to refer efficiently the inherited tables. This 
makes the table partitioning tricky.

\section{Indices}
An index is a structured relation where the indexed values are stored. The index entries are 
associated with pointers to the corresponding table's pages. Because PostgreSQL have a cost based 
optimiser, having an an index doesn't guarantee its usage. Reading an index page requires a random
disk seek. The cost for random is estimated by default four times more than a sequential page read
used by the sequential table scan. Those values can be adjusted using the two GUC parameters
seq\_page\_cost and random\_page\_cost, respectively the arbitrary cost of a table's page and an
index page. \newline

Designing the indices is a complex task. An unused index adds overhead and slowness to the write
operations with no benefit. Querying system views like the pg\_stat\_all\_indexes will give us
vital informations about the indices usage.

This simple finds all the indices in the public schema never used from the last statistics 
reset.

\begin{lstlisting}[style=pgsql]
SELECT
        schemaname,
        relname,
        indexrelname,
        idx_scan
FROM
         pg_stat_all_indexes
WHERE
                schemaname='public'
        AND     idx_scan=0
;

\end{lstlisting}


PostgreSQL implements many index type. Unfortunately the bitmap index, useful in the datawarehouse,
is not present. There is a plan node which can emulate partially the bitmap indices. This is
done with a sequential read over the index followed by a table's read for rematch. The tuples
are found using a bitmap generated from the index's sequential read.\newline 


The keyword USING specifies the index type at create time.
\begin{lstlisting}[style=pgsql]
 CREATE INDEX idx_test ON t_test USING hash (t_contents);
\end{lstlisting}

If omitted then the index type defaults to the B-tree.

The index maintenance is a delicate matter, the argument is described in depth
in \ref{cha:MAINTENANCE}.


\subsection{b-tree}
The general purpose B-tree\index{index,b-tree} index implements the Lehman and Yao's
high-concurrency B-tree management algorithm. The B-tree can handle equality and range queries 
returning ordered data. The indexed values are stored into the index pages with pointers to the
table's pages. The index is not TOASTable, this limit the max length for an index entry to 1/3 of
the page size. \newline

\subsection{hash}
The hash indices\index{index,hash} can handle only equality and are not WAL logged. Their changes
are not replayed if the crash recovery occurs and do not propagate to the standby servers.\newline

\subsection{GiST}
The GiST indices\index{index,GiST} are the Generalised Search Tree. The GiST is a collection of
indexing strategies organised under a common infrastructure. They can implement arbitrary indexing
schemes like B-trees, R-trees  or other. The operator classes shipped with PostgreSQL are for the
geometrical data with two elements and for the nearest-neighbour searches. The GiST indices do not
perform an exact match. The false positives are removed with second rematch on the
table's data.\newline

\subsection{GIN}
The GIN indices \index{index,GIN} are the Generalised Inverted Indices. This kind of index
is optimised for indexing the composite data types or vectors like the full text search elements.
The GIN are exact indices, when scanned the returned set doesn't require recheck.



\section{Views}
\index{views}
\label{sec:VIEWS}
A view is a relation storing the logical representation of a query. At create time all the objects
involved in the view definition are translated in the internal representation. Any wildcard is
expanded.

An example will explain better this concept. Let's consider a simple table populated with the
generate\_series() function.

\begin{lstlisting}[style=pgsql]


CREATE TABLE t_data 
        ( 
                i_id            serial,
                t_content       text
        );

ALTER TABLE t_data 
ADD CONSTRAINT pk_t_data PRIMARY KEY (i_id);


INSERT INTO t_data
        (
                t_content
        )
SELECT
        md5(i_counter::text)
FROM
        (
                SELECT
                        i_counter
                FROM
                        generate_series(1,200) as i_counter
        ) t_series;

CREATE OR REPLACE VIEW v_data 
AS 
  SELECT 
          *
  FROM 
        t_data;


\end{lstlisting}

Running a select from the view or the table returns the same columns and rows.
The stored definition in pg\_views does not have any wildcard though.


\begin{lstlisting}[style=pgsql]
db_test=# \x
db_test=# SELECT * FROM pg_views where viewname='v_data';
-[ RECORD 1 ]--------------------
schemaname | public
viewname   | v_data
viewowner  | postgres
definition |  SELECT t_data.i_id,
           |     t_data.t_content
           |    FROM t_data;


\end{lstlisting}

Adding a new column to the t\_data table will break the equality between the table and the view.

\begin{lstlisting}[style=pgsql]
 ALTER TABLE t_data ADD COLUMN d_date date NOT NULL default now()::date;
 
 db_test=# SELECT * FROM t_data LIMIT 1;
 i_id |            t_content             |   d_date   
------+----------------------------------+------------
    1 | c4ca4238a0b923820dcc509a6f75849b | 2014-05-21
(1 row)


db_test=# SELECT * FROM v_data LIMIT 1;
 i_id |            t_content             
------+----------------------------------
    1 | c4ca4238a0b923820dcc509a6f75849b
(1 row)


 
\end{lstlisting}

In order to have the view in sync with the table we need to refresh it using the CREATE OR REPLACE
VIEW statement.

\begin{lstlisting}[style=pgsql]
 CREATE OR REPLACE VIEW v_data 
AS 
  SELECT 
        *
  FROM 
        t_data;
        
db_test=# SELECT * FROM v_data LIMIT 1;
 i_id |            t_content             |   d_date   
------+----------------------------------+------------
    1 | c4ca4238a0b923820dcc509a6f75849b | 2014-05-21
(1 row)

\end{lstlisting}

PostgreSQL implements the views using the the objects identifiers. They never invalidate when
the objects are renamed. \newline

The CREATE OR REPLACE statement can update the view definition only if the column 
list adds new attributes in the end. When one or more view are pointing a relation this cannot be
dropped unless the clause CASCADE is used. Because the dependencies can be very complicated, this
approach is quite dangerous. The best is querying on the pg\_depend table and find out the full
list of dependencies.\newline

Because a view is a logical short cut to a stored SQL, PostgreSQL will run the query in the same
way as it was sent from the client. This excludes the network overhead.

A view can refer other views becoming a potential cause of performance regression. Because any view
have a specific execution plan, mixing different plans can confuse the planner causing
poor performance.

To avoid this it's a a good practice using the naming  v\_ to distinguish the views from the
tables. In \ref{cha:COUPLETHINGS} there is an example for the self explanatory database schema.

\index{view, updatable}
PostgreSQL from the version 9.3 supports the updatable views. This feature is limited just to the
simple views. A view is simple  if have the following features.

\begin{itemize}


 \item   Have exactly one entry in its FROM list, which must be a table or another updatable view.

 \item Does not contain WITH, DISTINCT, GROUP BY, HAVING,LIMIT, or OFFSET clauses at the top level.

 \item  Does not contain set operations (UNION, INTERSECT or EXCEPT) at the top level
 
 \item   All columns in the view's select list must be simple references to columns of the
underlying relation. They cannot be expressions, literals or functions. System columns cannot be
referenced, either.

 \item   Columns of the underlying relation do not appear more than once in the view's select list.

 \item   Does not have the security\_barrier property.

\end{itemize}

A complex view can be updatable using the triggers or rules.\newline

\index{view, materialised}
Another feature introduced by the 9.3 is the materialised views. This is a physical snapshot of the
saved SQL and can be refreshed with the statement REFRESH MATERIALIZED VIEW.  


\section{Tablespaces}\index{tablespaces,logical}
\label{sub:TBS-LOGICAL}
A tablespace\index{tablespace} is a logical name pointing to a physical location. 
This feature was introduced with the release 8.0 and its implementation did not change since
then. From the version 9.2 was introduced the function pg\_tablespace\_location(tablespace\_oid)
which gets the tablespace's location from the OID making the dba life easier.

When a new relation is created without the tablespace specification this defaults to the GUC
parameter default\_tablespace or to the database's default tablespace. 
On a fresh initialised cluster the initial value for the default tablespace is pg\_default which
points to to the directory \$PGDATA/base . Alongside with pg\_default there is the pg\_global as
well. This tablespace is reserved for the shared objects and points to the directory
\$PGDATA/global.

Creating a new tablespace is very simple. The physical location must be owned by the postgres
process user. 

For example let's create a tablespace pointing to the folder /var/lib/postgresql/pg\_tbs/ts\_test
with the name ts\_test.

\begin{lstlisting}[style=pgsql]
CREATE TABLESPACE ts_test 
OWNER postgres
LOCATION '/var/lib/postgresql/pg_tbs/ts_test' ;

\end{lstlisting}

Only superusers can create tablespaces. The OWNER clause is optional, if 
omitted the tablespace's owner defaults to the database user logged in. The tablespaces are cluster
wide and are listed into the pg\_tablespace system table.\newline

Creating a new relation into the tablespace ts\_test requires the TABLESPACE clause followed by the
tablespace name.

\begin{lstlisting}[style=pgsql]
CREATE TABLE t_ts_test
        (
                i_id serial,
                v_value text
        )
TABLESPACE ts_test ;

\end{lstlisting}

A relation can be moved from a tablespace to another using the ALTER command. For example let's
move the table t\_ts\_test to the  pg\_default tablespace.

\begin{lstlisting}[style=pgsql]
ALTER TABLE t_ts_test SET TABLESPACE pg_default;
\end{lstlisting}

Changing the relation's tablespace is transaction safe but requires an exclusive lock on the
affected relation. The lock prevents accessing the relation's data for the time required by the
move.If the relation have a significant size this should be considered carefully. Because the
exclusive lock conflicts with the locks issued by the backup, it's not possible to change the
relation's tablespace when pg\_dump is running.\newline


A tablespace can be removed with DROP TABLESPACE command. The tablespace must be empty before the
drop. There's no CASCADE clause to have the tablespace's contents dropped with the tablespace.

\begin{lstlisting}[style=pgsql]
postgres=# DROP TABLESPACE ts_test;
ERROR:  tablespace "ts_test" is not empty

postgres=# ALTER TABLE t_ts_test SET TABLESPACE pg_default;
ALTER TABLE
postgres=# DROP TABLESPACE ts_test;
DROP TABLESPACE

\end{lstlisting}


The tablespace feature adds flexibility to the space management. Even if not sophisticated like
other DBMS this implementation, combined with a careful design can improve sensibly the
performances.\newline

In \ref{sub:TBS-PHYSICAL} we'll take a look to the how PostgreSQL implements the tablespaces on the
physical side.



\section{Transactions}
\label{sec:TRANSACTION}
\index{transactions}
The atomicity the consistency and the isolation is implemented in PostgreSQL via the
MVCC\index{MVCC}. This is the acronym for Multi Version Concurrency Control\index{Multi Version
Concurrency Control}. This solution offers high efficiency in the concurrent user access for read
and write queries.\newline 

When a transaction starts a write operation receives an identifier, the XID \index{XID}. This is
a 32 bit quantity and is used to determine the transaction's visibility. All the transactions with
XID lesser than the current XID and committed are considered in the past and then visible. All the
transactions with XID greater than the current XID are in the future and not
visible. The not committed transactions are considered invisible as well.\newline

This comparison happens at tuple level using two system fields xmin and xmax. When a transaction
creates a new tuple the xmin is set with the transaction's xid. This field is also referred as the
insert's transaction id. When a tuple is deleted then the xmax value is updated to the transaction's
xid. The tuple when deleted is left in place, ensuring the read consistency for any transaction
which should see the previous version. The last tuple's version is a live tuple. The tuples which
versions are outdated are called dead tuples.\newline

There is no field for the update. In PostgreSQL the update is a a complete new insert. The update's
transaction id is used either for the new tuple's xmin and the old tuple's xmax. The dead tuples are
removed by VACUUM if no longer needed by existing transactions. In \ref{sec:TUPLES}
there is the tuple structure's description.\newline

Alongside with xmin and xmax there are two other system fields, the cmin and cmax. Their data
type is the command id, CID. Those fields are similar to xmin and xmax. They are used to track the
internal transaction's commands in order to avoid the command to be executed on the same
tuple multiple times. This solve the database's Halloween Problem described there
\href{http://en.wikipedia.org/wiki/Halloween_Problem}{
http://en.wikipedia.org/wiki/Halloween\_Problem}.\newline

The SQL standard defines four level of transaction's isolation levels where 
some phenomena are permitted or forbidden.
\index{transactions, isolation levels}

\begin{itemize}
 \item \textbf{dirty read}, when a transaction reads the data written by a concurrent 
uncommitted transaction

\item \textbf{nonrepeatable read} when a transaction re reads the same data and finds it changed by
another transaction which has committed since the initial read

\item \textbf{phantom read} when a transaction re executes a query returning a set 
of rows for a search condition and finds that results are changed by a recently committed 
transaction

\end{itemize}

The table \ref{tab:TRNISOLATION} shows the transaction's isolation levels with the phenomena
possible. In PostgreSQL it's possible to set all the four isolation levels but only the three
strictest levels are supported. Setting the  isolation level to read uncommited is
equivalent to set it to the read committed.

\begin{table}[H]
  \begin{tabular}{cccc}
    Isolation Level & Dirty Read    &    Nonrepeatable Read   &   Phantom 
Read\\ 
    \hline
    Read uncommitted  &  Possible    &    Possible     &   Possible\\
    Read committed    &  Not possible &  Possible     &   Possible\\
    Repeatable read   &  Not possible  & Not possible  &  Possible\\
    Serializable      &  Not possible  & Not possible   & Not possible\\
  \end{tabular}
  \caption{\label{tab:TRNISOLATION}SQL Transaction isolation levels}
\end{table}

The default the global isolation level is read committed. However is possible to change the
isolation level per session with the command:
\begin{lstlisting}[style=pgsql]
SET TRANSACTION ISOLATION LEVEL { SERIALIZABLE | REPEATABLE READ | READ 
COMMITTED | READ UNCOMMITTED }; 
\end{lstlisting}

Changing the default transaction isolation for the entire cluster is made using the GUC parameter
transaction\_isolation.


\subsection{Snapshot exports}
\label{sub:SNAPEXPORT}\index{transactions, snapshot export}
Since PostgreSQL 9.2 are supported the transaction's snapshot exports. A session with an open 
transaction, can export its consistent snapshot to any other session. The snapshot remains valid 
meanwhile the transaction is open. Using this functionality offers a way to run multiple backends 
on a consistent data set frozen in time. This feature resulted in the brilliant parallel export in 
the 9.3's pg\_dump as described in \ref{sec:PGDUMPINT}.\newline

In the following example, let's consider the table created in \ref{sec:VIEWS}. We'll first start an 
explicit transaction and then we'll export the current snapshot.

\begin{lstlisting}[style=pgsql]
postgres=# BEGIN TRANSACTION ISOLATION LEVEL REPEATABLE READ;
BEGIN
postgres=# SELECT pg_export_snapshot();
 pg_export_snapshot 
--------------------
 00001369-1
(1 row)

postgres=# SELECT count(*) FROM t_data;
 count 
-------
   200
(1 row)

\end{lstlisting}

We are first starting a transaction with the REPEATABLE READ isolation level. The second 
statement exports the current snapshot using the function pg\_export\_snapshot(). Finally we are 
checking with a simple row count the table t\_data have data inside.\newline

We can now login with in a different session and delete all the rows from the t\_data table.

\begin{lstlisting}[style=pgsql]
postgres=# DELETE FROM t_data;
DELETE 200
postgres=# SELECT count(*) FROM t_data;
 count 
-------
     0
(1 row)

\end{lstlisting}

With the table now empty let's import the snapshot exported by the first backend.

\begin{lstlisting}[style=pgsql]
postgres=# BEGIN TRANSACTION ISOLATION LEVEL REPEATABLE READ;
BEGIN
postgres=# SET TRANSACTION SNAPSHOT '00001369-1';
SET
postgres=# SELECT count(*) FROM t_data;
 count 
-------
   200
(1 row)

\end{lstlisting}

The function pg\_export\_snapshot saves the current snapshot returning the text string which 
identifies the snapshot. Passing the string to clients that want to import the snapshot gives to 
independent sessions a single consistent vision. The import is possible only until the end of the 
transaction that exported it. The export is useful only in the READ COMMITTED transactions, 
because the REPEATABLE READ and higher isolation levels use the same snapshot within their 
lifetime. 

