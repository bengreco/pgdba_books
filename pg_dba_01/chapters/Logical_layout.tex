\chapter{The logical layout}
\label{cha:LOGICLAY}\index{Logical layout}
In this we'll take a look to the PostgreSQL logical layout.
We'll start with the connection process. Then we'll see the logical relations like tables, indices
and views. The chapter will end with the tablespaces and the MVCC.

\section{The connection}
When a client starts a connection to a running cluster, the process pass through few steps. \newline

The first connection's stage is the check using the host based authentication. The cluster scans the
pg\_hba.conf file searching a  match for the connection's parameters. Those are, for example, the
client's host, the user etc. The host file is usually saved inside the the data area alongside the
configuration file postgresql.conf. The pg\_hba.conf is read from the top to the bottom and the
first matching row for the client's parameters is used to determine the authentication method to
use. If PostgreSQL reaches the end of the file without match the connection is refused.\newline

The pg\_hba.conf structure is shown in \ref{tab:PGHBA}

\begin{table}[H]
  \begin{tabular}{ccccc}
    Type & Database & User & Address & Method \\ 
    \hline
    local & name & name & ipaddress/network mask & trust\\
    host & * & * & host name & reject\\
    hostssl & &  &  & md5\\
    hostnossl & &  &  & password \\
    & & &  & gss \\
    & & &  & sspi \\
    & & &  & krb5 \\
    & & &  & ident \\
    & & &  & peer \\
    & & &  & pam \\
    & & &  & ldap \\
    & & &  & radius \\
    & & &  & cert \\
  \end{tabular}
  \caption{\label{tab:PGHBA}pg\_hba.conf}
\end{table}

The column type specifies if the connection is local or host. The former is when the connection is
made using a socket. The latter when the connection uses the network. It's also possible to
specify if the host connection should be secure or plain using hostssl and hostnossl.\newline

The Database and User columns are used to match specific databases and users.\newline

The column address have sense only if the connection is host, hostssl or hostnossl. The value can
be an ip address plus the network mask. Is also possible to specify the hostname. There is the
full support for ipv4 and ipv6.

The pg\_hba.conf's last column is the authentication method for the matched row. The action to
perform after the match is done. PostgreSQL supports many methods ranging from the plain password
challenge to kerberos.\newline

We'll now take a look to the built in methods.

\begin{itemize}
 \item \textbf{trust}: The connection is authorised without any further action. Is quite useful 
if the password is lost. Use it with caution.

\item \textbf{peer}: The connection is authorised if the OS user matches the 
database user. It's useful for the local connections. 

\item \textbf{password}: The connection establishes if the connection's user and the password
matches with the values stored in the pg\_shadow system table. This method sends the password in
clear text. Should be used only on trusted networks.

\item \textbf{md5}: This method is similar to password. It uses a better security encoding the
passwords using the md5 algorithm. Because md5 is deterministic, there is pseudo random
subroutine which prevents to have the same string sent over the network.

\item \textbf{reject}: The connection is rejected. This method is very useful to keep the sessions
out of the database. e.g. maintenance requiring single user mode.

\end{itemize}

When the connection establishes the postgres main process forks a new backend process attached to
the shared buffer. The fork process is expensive. This makes the connection a potential
bottleneck. Opening new connections can degrade the operating system performance and eventually
produce zombie processes. Keeping the connections constantly connected maybe is a reasonable fix.
Unfortunately this approach have a couple of unpleasant side effects.\newline

Changing any connection related parameter like the max\_connections, requires a cluster restart.
For this reason planning the resources is absolutely vital. For each connection present in
 max\_connections the cluster allocates 400 bytes of shared memory. For each connection established 
the cluster allocates a per user memory area wich size is determined by the parameter
work\_mem.\newline

For example let's consider a cluster with a shared\_buffer set to 512 MB and the work\_mem 
set to 100MB. Setting the max\_connections to only 500 requires a potentially 49 GB of total memory
if all the connections are in use. Because the work\_mem can affects the performances, its
value should be determined carefully. Being a per user memory any change to work\_mem does not
require the cluster's start but a simple reload.\newline 

In this kind of situations a connection pooler can be a good solutions. The sophisticated
\href{http://www.pgpool.net/}{pgpool}  or the
lightweight \href{http://pgfoundry.org/projects/pgbouncer/}{pgbouncer}  can help to boost the
connection's performance.\newline

By default a fresh data area initialisation listens only on the localhost. The GUC parameter
governing this aspect is listen\_addresses. In order to have the cluster accepting connections from
the rest of the network the values should change to the correct listening addresses specified
as values separated by commas. It's also possible to set it to * as wildcard.

Changing the parameters max\_connections and listen\_addresses require the cluster restart.



\section{Databases}
\label{sec:DATABASES}
Unlikely other DBMS, a PostgreSQL connection requires the database name in the connection string.
Sometimes this can be omitted in psql when this information is supplied in another way.\newline

When omitted psql checks if the environment variable \$PGDATABASE \index{\$PGDATABASE variable} is 
set. If \$PGDATABASE is missing then psql defaults the database name to connection's username. This 
leads to confusing error messages. For example, if we have a username named test but not a database 
named test the connection will fail even with the correct credentials.

\begin{verbatim}
postgres@tardis:~$ psql -U test -h localhost
Password for user test: 
psql: FATAL:  database "test" does not exist
\end{verbatim}

This error appears because the pg\_hba.conf allow the connection for any database. Even for a not
existing one. The connection is then terminated when the backend ask to connect to the database 
named test which does not exists.\newline

This is very common for the new users. The solution is incredibly simple because in a PostgreSQL 
cluster there are at least three databases. Passing the name template1 as last parameter will do 
the trick.

\begin{verbatim}
postgres@tardis:~$ psql -U test -h localhost template1
Password for user test: 
psql (9.3.4)
SSL connection (cipher: DHE-RSA-AES256-SHA, bits: 256)
Type "help" for help.
\end{verbatim}

When the connection is established we can query the system table pg\_database to get the 
cluster's database list. 

\begin{lstlisting}[style=pgsql]
template1=> SELECT datname FROM pg_database;
    datname    
---------------
 template1
 template0
 postgres
(3 rows)

\end{lstlisting}

Database administrators coming from other DBMS can be confused by the postgres database.
This database have nothing special. Its creation was added since the version 8.4 because it was 
useful to have it. You can just ignore it or use it for testing purposes. Dropping the postgres 
database does not corrupts the cluster. Because this database is often used by third party 
tools before dropping it check if is in use in any way.\newline

The databases template0 and template1 \index{template1 database} \index{template0 
database} like the name suggests are the template databases. A template database \index{template 
database} is used to build new database copies via the physical file copy. 

When initdb initialises the data area the database template1 is populated with the correct
references to the WAL records, the system views and the procedural language PL/PgSQL. When
this is done the database template0 and the postgres databases are then created using the template1
database.

The database template0 doesn't allow the connections. It's main usage is to rebuild the
database template1 if it gets corrupted or for creating databases with a character encoding/ctype, 
different from the cluster wide settings. 
\index{CREATE DATABASE}

\begin{lstlisting}[style=pgsql]
postgres=# CREATE DATABASE db_test WITH ENCODING 'UTF8' LC_CTYPE 'en_US.UTF-8';
ERROR:  new LC_CTYPE (en_US.UTF-8) is incompatible with the LC_CTYPE of the 
template database (en_GB.UTF-8)
HINT:  Use the same LC_CTYPE as in the template database, or use template0 as 
template.

postgres=# CREATE DATABASE db_test WITH ENCODING 'UTF8' LC_CTYPE 'en_US.UTF-8' 
TEMPLATE template0;
CREATE DATABASE
postgres=# 

\end{lstlisting}

If the template is omitted the CREATE DATABASE statement will use template1 by default. 


A database can be renamed or dropped with ALTER DATABASE and DROP DATABASE \index{ALTER 
DATABASE}\index{DROP DATABASE} statements. Those operations require the exclusive access to the 
affected database. If there are connections established the drop or rename will fail.

\begin{lstlisting}[style=pgsql]
postgres=# ALTER DATABASE db_test RENAME TO db_to_drop;
ALTER DATABASE

postgres=# DROP DATABASE db_to_drop;
DROP DATABASE

\end{lstlisting}




\section{Tables}\index{Tables}
\label{sec:TABLES}
In our top down approach to the PostgreSQL's logical model, the next step is the relation.
In the PostgreSQL jargon a relation is an object which carries the data or the way to
retrieve the data. A relation can have a physical counterpart or be purely logical. We'll take a 
look in particular to three of them starting with the tables.\newline

A table is the fundamental storage unit for the data. PostgreSQL implements many kind of tables
with different levels of durability. A table is created using the SQL command CREATE TABLE. The data
is stored into a table without any specific order. Because the MVCC implementation a row update can
change the row's physical position. For more informations look to \ref{sec:MVCC}. PostgreSQL
implements three kind of tables.

\subsection{Logged tables}\index{Logged tables}
By default CREATE TABLE creates a logged table. This kind of table implements the durability
logging any change to the write ahead log. The data pages are loaded in the shared buffer and any
change to them is logged first to the WAL. The consolidation to the the data file happens later. 

\subsection{Unlogged tables}\index{Unlogged tables}
\label{sub:UNLOGGEDTABLES}
An unlogged table have the same structure like the logged table. The difference is such kind of 
tables are not crash safe. The data is still consolidated to the data file but the pages modified 
in memory do not write their changes to the WAL. The main advantage is the write operations which 
are considerably faster at the cost of the data durability. The data stored into an ulogged table
should be considered partially volatile. The database will truncate those tables when the crash
recovery occurs. Because the unlogged table don't write to the WAL, those tables are not accessible 
on a physical standby. 

\subsection{Temporary tables}\index{Temporary tables}
A temporary table is a relation which lives into the backend's local memory. When the connection 
ends the table is dropped. Those table can have the same name for all the sessions because 
they are completely isolated. If the amount of data stored into the table is lesser than 
the temp\_buffers value the table will fit in memory with great speed advantage. Otherwise the 
database will create a temporary relation on disk. The parameter temp\_buffers can be altered for 
the session but only before the first temporary table is created. 


\subsection{Foreign tables}\index{Foreign tables}
The foreign tables were first introduced with PostgreSQL 9.1 as read only relations, improving 
considerably the DBMS interconnection's capability. A foreign table works exactly like a local table 
using a foreign data
wrapper to interact with the foreign data source.\newline

There are many different foreign data wrappers available for very exotic data sources. With the 
PostgreSQL 9.3 there is the postgres\_fdw and the the foreign tables are writable. In particular 
the postgres\_fdw implementation is similar to old dblink module with a more efficient performance 
management and the connection's caching.

\section{Table inheritance}\index{Table inheritance}
Being more precise, PostgreSQL is an Object Relational Database Management System. Its intenal logic
implements some of the concepts of the object oriented programming. The relations are also called 
classes and the table's columns attributes. \newline

The table inheritance is a logical relationship between a parent table and one or more child 
tables. The child tables inherit the parent's attribute structure but not the physical storage. 

\begin{lstlisting}[style=pgsql]
db_test=#CREATE TABLE t_parent
                      (
                          i_id_data     integer,
                          v_data        character varying(300)
                      );

CREATE TABLE                     

db_test=#CREATE TABLE t_child_01
                      
                      ()
             INHERITS (t_parent)
                      ;                      
db_test=# \d t_parent
            Table "public.t_parent"
  Column   |          Type          | Modifiers 
-----------+------------------------+-----------
 i_id_data | integer                | 
 v_data    | character varying(300) | 
Number of child tables: 1 (Use \d+ to list them.)

db_test=# \d t_child_01 
           Table "public.t_child_01"
  Column   |          Type          | Modifiers 
-----------+------------------------+-----------
 i_id_data | integer                | 
 v_data    | character varying(300) | 
Inherits: t_parent

\end{lstlisting}

The inheritance can be defined at creation time or later. It's also possible to create a completely 
new table and create the inheritance with another one. In this case the child structure must match 
exactly the parent.

\begin{lstlisting}[style=pgsql]

db_test=# ALTER TABLE t_child_01 NO INHERIT t_parent;
ALTER TABLE
db_test=# ALTER TABLE t_child_01 INHERIT t_parent;
ALTER TABLE

\end{lstlisting}

Because the physical storage is not shared the unique constraints aren't globally enforced on the 
inheritance tree and this prevents the foreign keys to refer efficiently the inherited tables. This 
makes the table partitioning tricky.

\section{Indices}
An index is a relation which stores the indexed values in an structured way. The index entries are 
associated to pointers to the corresponding table's pages. Because PostgreSQL have a cost based 
optimiser, having an an index doesn't guarantee its usage.The index pages require a random disk 
seek which cost is estimated by default four times more than a sequential page read. Those values 
can be controlled using the two GUC parameters seq\_page\_cost and random\_page\_cost. They set 
respectively the arbitrary cost of a table's page and an index page. \newline

Designing the indices is a complex task. An unused index adds overhead to the write 
operations without any benefit, slowing down the database operations. If uncertain the best choice 
is do not add it. Querying system views like the pg\_stat\_all\_indexes will give us vital 
informations about the usage.

This handy query finds all the indices in the public schema never used from the last statistics 
reset.

\begin{lstlisting}[style=pgsql]
SELECT
        schemaname,
        relname,
        indexrelname,
        idx_scan
FROM
         pg_stat_all_indexes
WHERE
                schemaname='public'
        AND     idx_scan=0
;

\end{lstlisting}



The indices implemented in PostgreSQL are various unfortunately there's still no bitmap index. The 
executor have an execution plan which can emulate partially the bitmap indices doing a sequential 
read over the B-tree and matching the pages's occurrences using the bitmap generated on the fly. 

The index type can be specified at create time. If omitted then the index type defaults to the 
B-tree.

\subsection{b-tree}
The general purpose B-tree\index{index,b-tree} index implements the Lehman and 
Yao's high-concurrency B-tree management algorithm. The B-tree can handle equality and range queries 
returning ordered data. As the data is actually stored in the page and because the index is not 
TOASTable  the max length for an index entry is 1/3 of the page size. \newline

\subsection{hash}

The hash indices\index{index,hash} can handle only equality and aren't WAL 
logged. That means their changes are not replayed if the crash recovery occurs and aren't 
propagated to the standby servers.\newline

\subsection{GiST}
The GiST indices\index{index,GiST} are the Generalised Search Tree. The GiST 
is a collection of indexing strategies organised under a common infrastructure. They can implement 
arbitrary indexing schemes like B-trees, R-trees  or other. The operator classes shipped with 
PostgreSQL are for the geometrical data with two elements and for the nearest-neighbour searches. 
The GiST indices don't perform an exact match. The false positive need a second match on 
the table's data.\newline

\subsection{GIN}
The GIN indices \index{index,GIN} are the Generalised Inverted Indices. This kind of index is 
optimised for indexing the composite data types or vectors like the full text search elements. The 
GIN are exact indices, when scanned the returned set doesn't require recheck.


\begin{lstlisting}[style=pgsql]
 CREATE INDEX idx_test ON t_test USING hash (t_contents);
\end{lstlisting}


As the index maintenance is a delicate matter, the argument is described in 
depth in \ref{cha:MAINTENANCE}.


\section{Views}
\index{views}
\label{sec:VIEWS}
A view is the representation of a query, stored in the system catalogue 
for quick access. All the objects involved in the view are translated to the 
internal identifiers at the creation time; the same happens for any wild card 
which is expanded to the column list.

An example will explain better the concept. Let's create a simple table. Using 
the generate\_series() function let's put some data into it.

\begin{lstlisting}[style=pgsql]


CREATE TABLE t_data 
        ( 
                i_id serial,
                t_content       text
        );

ALTER TABLE t_data 
ADD CONSTRAINT pk_t_data PRIMARY KEY (i_id);


INSERT INTO t_data
        (
                t_content
        )
SELECT
        md5(i_counter::text)
FROM
        (
                SELECT
                        i_counter
                FROM
                        generate_series(1,200) as i_counter
        ) t_series;

CREATE OR REPLACE VIEW v_data 
AS 
  SELECT 
          *
  FROM 
        t_data;


\end{lstlisting}

The SELECT * from t\_data or v\_data looks exactly the same, the view 
simply runs the stored SQL used at creation time.
If we look to the stored definition in pg\_views we'll find the wildcard is 
expanded into the table's columns.


\begin{lstlisting}[style=pgsql]
 db_test=# SELECT * FROM pg_views where viewname='v_data';
-[ RECORD 1 ]--------------------
schemaname | public
viewname   | v_data
viewowner  | postgres
definition |  SELECT t_data.i_id,
           |     t_data.t_content
           |    FROM t_data;


\end{lstlisting}

Now let's add a new column to the t\_data table and run again the select on the 
table and the view.

\begin{lstlisting}[style=pgsql]
 ALTER TABLE t_data ADD COLUMN d_date date NOT NULL default now()::date;
 
 db_test=# SELECT * FROM t_data LIMIT 1;
 i_id |            t_content             |   d_date   
------+----------------------------------+------------
    1 | c4ca4238a0b923820dcc509a6f75849b | 2014-05-21
(1 row)


db_test=# SELECT * FROM v_data LIMIT 1;
 i_id |            t_content             
------+----------------------------------
    1 | c4ca4238a0b923820dcc509a6f75849b
(1 row)


 
\end{lstlisting}

The view doesn't show the new column. To update the view definition a new 
CREATE OR REPLACE VIEW statement must be issued.

\begin{lstlisting}[style=pgsql]
 CREATE OR REPLACE VIEW v_data 
AS 
  SELECT 
        *
  FROM 
        t_data;
        
db_test=# SELECT * FROM v_data LIMIT 1;
 i_id |            t_content             |   d_date   
------+----------------------------------+------------
    1 | c4ca4238a0b923820dcc509a6f75849b | 2014-05-21
(1 row)

\end{lstlisting}

Because the views are referring the objects identifiers they will never 
invalidate when the referred objects are altered. 
The CREATE OR REPLACE statement updates the view definition only if the column 
list adds new attributes in the end. 
Otherwise, any change to the existing columns requires the view's drop and 
recreate. 

When one or more view are pointing a relation this cannot be dropped. 
The option CASCADE in the drop statement will drop the dependant objects before 
the final drop. This is a dangerous approach though. Dropping objects 
regardless can result in data or functionality loss.

When a drop is blocked by dependant objects the database emits a message with 
the informations about the dependencies. If the amount of objects is too much 
big it's better to query the pg\_depend table to find out the correct 
dependencies. This table lists all the dependencies for each object using a 
peculiar logic.


As seen before a view is a logical short cut to a pre saved query. This means 
the database will follow all the steps to execute exactly the same way if the 
entire query has been sent via client, except for the network overhead.

Nothing forbids a view to point another view inside the definition or join 
the one or more views in a different query. This can cause massive 
regression on the overall performance because each view require an execution 
plan and mixing the views will cause not efficient planning. 

To mark a relation is a view it's a good idea to use a naming prefix like v\_. 
This will distinguish them from the tables marked with the prefix t\_.
In \ref{cha:COUPLETHINGS} we'll take a to the naming conventions to let the 
database schema self explanatory.

\index{view, updatable}
PostgreSQL from the version 9.3 supports the updatable simple views. 
A view is simple if

\begin{itemize}


 \item   Have exactly one entry in its FROM list, which must be a table or 
another updatable view

 \item Does not contain WITH, DISTINCT, GROUP BY, HAVING,LIMIT, or OFFSET 
clauses at the top level

 \item  Does not contain set operations (UNION, INTERSECT or EXCEPT) at the 
top level

 \item   All columns in the view's select list must be simple references to 
columns of the underlying relation. They cannot be expressions, literals or 
functions. System columns cannot be referenced, either

 \item   columns of the underlying relation do not appear more than once in 
the view's select list

 \item   does not have the security\_barrier property

\end{itemize}

If the view doesn't fit those rules it's still possible to make it updatable 
using the triggers with the INSTEAD OF clause.

\index{view, materialised}
The major version 9.3 introduces also the materialised view concept. This is a 
physical snapshot of the saved SQL and can be refreshed with the statement 
REFRESH MATERIALIZED VIEW.  


\section{Tablespaces}\index{tablespaces,logical}
\label{sub:TBS-LOGICAL}
A tablespace\index{tablespace} is a logical shortcut for a physical location. 
This feature was first introduced with the major release 8.0, recently with the 
9.2 had a small adjustment to make the dba life easier.

When a new relation is created without tablespace specification, the relation's
tablespace is set to the GUC prameter default\_tablespace or, if this is 
missing, is set to the database's default tablespace. 
Anyway, without any specification the default tablespace is the pg\_default, 
corresponding to the \$PGDATA/base directory.

In order to create a new tablespace the chosen directory must be owned by 
the os user which started the postgres process and must be specified as 
absolute path. 

For example, having a folder named /var/lib/postgresql/pg\_tbs/ts\_test a 
tablespace we can create a new tablespace ts\_test.

\begin{lstlisting}[style=pgsql]
CREATE TABLESPACE ts_test 
OWNER postgres
LOCATION '/var/lib/postgresql/pg_tbs/ts_test' ;

\end{lstlisting}

Only superusers can create tablespaces. The OWNER clause is optional, if 
omitted the tablespace is owned by the user issuing the command.

The tablespaces are cluster wide, each database sees the same list in the 
pg\_tablespace system table.

To create a relation into the tablespace ts\_test just add the TABLESPACE 
clause followed by the tablespace name at creation 
time.

\begin{lstlisting}[style=pgsql]
CREATE TABLE t_ts_test
        (
                i_id serial,
                v_value text
        )
TABLESPACE ts_test ;

\end{lstlisting}

It's possible to move a relation from a tablespace to another using the 
ALTER command.

For example, this is the command to move the previously created table to the 
pg\_default tablespace.

\begin{lstlisting}[style=pgsql]
ALTER TABLE t_ts_test SET TABLESPACE pg_default;
\end{lstlisting}

The move is transaction safe but requires an exclusive lock on the affected 
relation. If the relation have a significant size this means no access to 
the data for the time required by the move.

In addition, changing the tablespaces is not permitted when the backup is in 
progress, the exclusive lock is not compatible with the locks issued by the 
schema and data export.

The tablespace feature adds flexibility to the space management. Even if is 
still primitive a careful design can improve sensibly the performances, for 
example, putting tables and indices on different devices to maximise the disks 
bandwidth.

To remove a tablespace there is the  DROP TABLESPACE command. The tablespace 
must be empty before the drop. There's no CASCADE clause to have the 
tablespace's contents dropped with the tablespace.

\begin{lstlisting}[style=pgsql]
postgres=# DROP TABLESPACE ts_test;
ERROR:  tablespace "ts_test" is not empty

postgres=# ALTER TABLE t_ts_test SET TABLESPACE pg_default;
ALTER TABLE
postgres=# DROP TABLESPACE ts_test;
DROP TABLESPACE

\end{lstlisting}

In \ref{sub:TBS-PHYSICAL} we'll take a look to the how PostgreSQL 
implements the tablespaces on the physical side.

\section{Transactions}
\label{sec:TRANSACTION}
\index{transactions}
PostgreSQL implements the MVCC\index{MVCC} which stands for Multi Version 
Concurrency Control\index{Multi Version Concurrency Control}. 
This offers high efficiency in multi user access for read and write queries.
When a new query starts a transaction identifier is assigned, the XID 
\index{XID} a 32 bit quantity. To determine the transaction's snapshot 
visibility, all the committed transactions with XID lesser than the current XID 
are in the past and then visible. Otherwise, all the transactions with XID 
greater than the current XID are in the future and not visible.\newline

This comparison happens at tuple level using two system fields xmin and xmax 
having the xid data type. 
When a transaction creates a new tuple then the transaction's xid is put into 
the tuple's xmin value. 
When a transaction deletes a tuple then the xmax value is set to the 
transaction's xid leaving the tuple in place for read consistency. 
When a tuple is visible to any transaction is called a live tuple, a tuple 
which is no longer visible is a dead tuple.\newline

PostgreSQL have no dedicated field for the update's xid. That's because when an 
UPDATE is issued PostgreSQL creates a new tuple's version with the updated 
data and sets the xmax value in the old version making it disappear.\newline

A dead tuple can be reclaimed by VACUUM if no longer required by running 
transactions, anyway tables updated often can result in data bloat for the dead 
tuples and for the eventual indices.
Look to \ref{sec:TUPLES} for more information on the 
tuples.\newline

When designing a new data model, the PostgreSQL's peculiar behaviour on the 
update should be the first thing to consider, in order to limit the table 
and index bloat. \newline

Among the xmin,xmax two other system fields the cmin and cmax which data type 
is CID, command id. Those are similar to the xmin/xmax quantities and usage 
and their usage is to track the internal transaction's commands, in order to 
avoid the command execution on the same tuple more than one time. The pratical 
issue is explained in the well known Halloween Problem. For more informations 
take a look here 
\href{http://en.wikipedia.org/wiki/Halloween_Problem}{
http://en.wikipedia.org/wiki/Halloween\_Problem}.\newline

The SQL standard defines four level of transaction's isolation levels where 
some phenomena are permitted or forbidden.
\index{transactions, isolation levels}
Those phenomena are the following.

\begin{itemize}
 \item \textbf{dirty read} A transaction reads data written by a concurrent 
uncommitted transaction

\item \textbf{nonrepeatable read} A transaction re reads the data previously 
read and finds the data changed by another transaction which has
committed since the initial read

\item \textbf{phantom read} A transaction re executes a query returning a set 
of rows satisfying a search condition and finds that the set of rows 
satisfying the condition has changed because another recently-committed 
transaction

\end{itemize}

Table \ref{tab:TRNISOLATION} shows the isolation levels with the 
allowed phenomena. In PostgreSQL it's possible to set all the four 
isolation levels but only the three more strict are supported. Setting the 
isolation level to read uncommited fallback to the read committed in any case.

\begin{table}[H]
  \begin{tabular}{cccc}
    Isolation Level & Dirty Read    &    Nonrepeatable Read   &   Phantom 
Read\\ 
    \hline
    Read uncommitted  &  Possible    &    Possible     &   Possible\\
    Read committed    &  Not possible &  Possible     &   Possible\\
    Repeatable read   &  Not possible  & Not possible  &  Possible\\
    Serializable      &  Not possible  & Not possible   & Not possible\\
  \end{tabular}
  \caption{\label{tab:TRNISOLATION}SQL Transaction isolation levels}
\end{table}

By default the global isolation level is set to read committed, it's possible 
to change the session's transaction isolation level using the command:
\begin{lstlisting}[style=pgsql]
SET TRANSACTION ISOLATION LEVEL { SERIALIZABLE | REPEATABLE READ | READ 
COMMITTED | READ UNCOMMITTED }; 
\end{lstlisting}

To change the default transaction isolation level cluster wide there is the GUC 
parameter transaction\_isolation.


\subsection{Snapshot exports}
\label{sub:SNAPEXPORT}\index{transactions, snapshot export}
Since PostgreSQL 9.2 are supported the transaction's snapshot exports. A session with an open 
transaction, can export its consistent snapshot to any other session. The snapshot remains valid 
meanwhile the transaction is open. Using this functionality offers a way to run multiple backends 
on a consistent data set frozen in time. This feature resulted in the brilliant parallel export in 
the 9.3's pg\_dump as described in \ref{sec:PGDUMPINT}.\newline

In the following example, let's consider the table created in \ref{sec:VIEWS}. We'll first start an 
explicit transaction and then we'll export the current snapshot.

\begin{lstlisting}[style=pgsql]
postgres=# BEGIN TRANSACTION ISOLATION LEVEL REPEATABLE READ;
BEGIN
postgres=# SELECT pg_export_snapshot();
 pg_export_snapshot 
--------------------
 00001369-1
(1 row)

postgres=# SELECT count(*) FROM t_data;
 count 
-------
   200
(1 row)

\end{lstlisting}

We are first starting a transaction with the REPEATABLE READ isolation level. The second 
statement exports the current snapshot using the function pg\_export\_snapshot(). Finally we are 
checking with a simple row count the table t\_data have data inside.\newline

We can now login with in a different session and delete all the rows from the t\_data table.

\begin{lstlisting}[style=pgsql]
postgres=# DELETE FROM t_data;
DELETE 200
postgres=# SELECT count(*) FROM t_data;
 count 
-------
     0
(1 row)

\end{lstlisting}

With the table now empty let's import the snapshot exported by the first backend.

\begin{lstlisting}[style=pgsql]
postgres=# BEGIN TRANSACTION ISOLATION LEVEL REPEATABLE READ;
BEGIN
postgres=# SET TRANSACTION SNAPSHOT '00001369-1';
SET
postgres=# SELECT count(*) FROM t_data;
 count 
-------
   200
(1 row)

\end{lstlisting}

The function pg\_export\_snapshot saves the current snapshot returning the text string which 
identifies the snapshot. Passing the string to clients that want to import the snapshot gives to 
independent sessions a single consistent vision. The import is possible only until the end of the 
transaction that exported it. The export is useful only in the READ COMMITTED transactions, 
because the REPEATABLE READ and higher isolation levels use the same snapshot within their 
lifetime. 

