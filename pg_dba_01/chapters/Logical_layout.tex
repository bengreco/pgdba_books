\chapter{The logical layout}
\label{cha:LOGICLAY}\index{Logical layout}
In this we'll take a look to the PostgreSQL logical layout.
We'll start with the connection process. Then we'll see the logical relations like tables, indices
and views. The chapter will end with the tablespaces and the MVCC.

\section{The connection}
When a client starts a connection to a running cluster, the process pass through few steps. \newline

The first connection's stage is the check using the host based authentication. The cluster scans the
pg\_hba.conf file searching a  match for the connection's parameters. Those are, for example, the
client's host, the user etc. The host file is usually saved inside the the data area alongside the
configuration file postgresql.conf. The pg\_hba.conf is read from the top to the bottom and the
first matching row for the client's parameters is used to determine the authentication method to
use. If PostgreSQL reaches the end of the file without match the connection is refused.\newline

The pg\_hba.conf structure is shown in \ref{tab:PGHBA}

\begin{table}[H]
  \begin{tabular}{ccccc}
    Type & Database & User & Address & Method \\ 
    \hline
    local & name & name & ipaddress/network mask & trust\\
    host & * & * & host name & reject\\
    hostssl & &  &  & md5\\
    hostnossl & &  &  & password \\
    & & &  & gss \\
    & & &  & sspi \\
    & & &  & krb5 \\
    & & &  & ident \\
    & & &  & peer \\
    & & &  & pam \\
    & & &  & ldap \\
    & & &  & radius \\
    & & &  & cert \\
  \end{tabular}
  \caption{\label{tab:PGHBA}pg\_hba.conf}
\end{table}

The column type specifies if the connection is local or host. The former is when the connection is
made using a socket. The latter when the connection uses the network. It's also possible to
specify if the host connection should be secure or plain using hostssl and hostnossl.\newline

The Database and User columns are used to match specific databases and users.\newline

The column address have sense only if the connection is host, hostssl or hostnossl. The value can
be an ip address plus the network mask. Is also possible to specify the hostname. There is the
full support for ipv4 and ipv6.

The pg\_hba.conf's last column is the authentication method for the matched row. The action to
perform after the match is done. PostgreSQL supports many methods ranging from the plain password
challenge to kerberos.\newline

We'll now take a look to the built in methods.

\begin{itemize}
 \item \textbf{trust}: The connection is authorised without any further action. Is quite useful 
if the password is lost. Use it with caution.

\item \textbf{peer}: The connection is authorised if the OS user matches the 
database user. It's useful for the local connections. 

\item \textbf{password}: The connection establishes if the connection's user and the password
matches with the values stored in the pg\_shadow system table. This method sends the password in
clear text. Should be used only on trusted networks.

\item \textbf{md5}: This method is similar to password. It uses a better security encoding the
passwords using the md5 algorithm. Because md5 is deterministic, there is pseudo random
subroutine which prevents to have the same string sent over the network.

\item \textbf{reject}: The connection is rejected. This method is very useful to keep the sessions
out of the database. e.g. maintenance requiring single user mode.

\end{itemize}

When the connection establishes the postgres main process forks a new backend process attached to
the shared buffer. The fork process is expensive. This makes the connection a potential
bottleneck. Opening new connections can degrade the operating system performance and eventually
produce zombie processes. Keeping the connections constantly connected maybe is a reasonable fix.
Unfortunately this approach have a couple of unpleasant side effects.\newline

Changing any connection related parameter like the max\_connections, requires a cluster restart.
For this reason planning the resources is absolutely vital. For each connection present in
 max\_connections the cluster allocates 400 bytes of shared memory. For each connection established 
the cluster allocates a per user memory area wich size is determined by the parameter
work\_mem.\newline

For example let's consider a cluster with a shared\_buffer set to 512 MB and the work\_mem 
set to 100MB. Setting the max\_connections to only 500 requires a potentially 49 GB of total memory
if all the connections are in use. Because the work\_mem can affects the performances, its
value should be determined carefully. Being a per user memory any change to work\_mem does not
require the cluster's start but a simple reload.\newline 

In this kind of situations a connection pooler can be a good solutions. The sophisticated
\href{http://www.pgpool.net/}{pgpool}  or the
lightweight \href{http://pgfoundry.org/projects/pgbouncer/}{pgbouncer}  can help to boost the
connection's performance.\newline

By default a fresh data area initialisation listens only on the localhost. The GUC parameter
governing this aspect is listen\_addresses. In order to have the cluster accepting connections from
the rest of the network the values should change to the correct listening addresses specified
as values separated by commas. It's also possible to set it to * as wildcard.

Changing the parameters max\_connections and listen\_addresses require the cluster restart.



\section{Databases}
\label{sec:DATABASES}
Unlikely other DBMS, PostgreSQL requires a database name in the connection's parameters.
Sometimes this can be omitted when, for example, psql have this information supplied in
another way.\newline

When psql tries to connect and the database name is omitted the program checks if the environment
variable \$PGDATABASE \index{\$PGDATABASE variable} is set. If \$PGDATABASE is missing then psql
uses the supplied username  as target database. This leads to confusing error messages. For
example, if we have a username named test but not a database named test this the result.

\begin{verbatim}
postgres@tardis:~$ psql -U test -h localhost
Password for user test: 
psql: FATAL:  database "test" does not exist
\end{verbatim}

This error appears because the pg\_hba.conf allow the connection for any database even a not
existing one. The connection is terminated when the backend ask to connect to the test database
test which doesn't exists.\newline

This is a very common case for the new users. The solution is incredibly simple because in a
PostgreSQL cluster there are at least three databases. Passing the name template1 as last parameter
will establish the connection.

\begin{verbatim}
postgres@tardis:~$ psql -U test -h localhost template1
Password for user test: 
psql (9.3.4)
SSL connection (cipher: DHE-RSA-AES256-SHA, bits: 256)
Type "help" for help.
\end{verbatim}

When the connection is made we can query the system table pg\_database to get the full list of
databases.

\begin{lstlisting}[style=pgsql]
template1=> SELECT datname FROM pg_database;
    datname    
---------------
 template1
 template0
 postgres
(3 rows)

\end{lstlisting}

The DBA coming from other DBMS, open or closed source can be confused by the postgres database.
This database have nothing special. Its creation was added in an older PostgreSQL version because
was used as default database by specific tools like pg\_bench. You can leave it alone, but dropping
the postgres does not corrupts the cluster.\newline

The databases template0 and template1 \index{template1 database} \index{template0 
database} like the name suggests are the template databases. A template database \index{template 
database} is used to build new database copies via the physical file copy. 

When initdb initialises the data area the database template1 is populated with the correct
references to the WAL records, the system views and the procedural language PL/PgSQL. When
this is done the database template0 and the postgres databases are then created using the template1
database.

The database template0 doesn't allow the connections. It's main usage is to rebuild the
database template1 if gets corrupted. It's also used for creating databases with a character
encoding or ctype, different from the cluster's settings. 
\index{CREATE DATABASE}

\begin{lstlisting}[style=pgsql]
postgres=# CREATE DATABASE db_test WITH ENCODING 'UTF8' LC_CTYPE 'en_US.UTF-8';
ERROR:  new LC_CTYPE (en_US.UTF-8) is incompatible with the LC_CTYPE of the 
template database (en_GB.UTF-8)
HINT:  Use the same LC_CTYPE as in the template database, or use template0 as 
template.

postgres=# CREATE DATABASE db_test WITH ENCODING 'UTF8' LC_CTYPE 'en_US.UTF-8' 
TEMPLATE template0;
CREATE DATABASE
postgres=# 

\end{lstlisting}

If the template is omitted the CREATE DATABASE statement will use template1. 
\index{ALTER DATABASE}\index{DROP DATABASE}

A database can be renamed or dropped with ALTER DATABASE and DROP DATABASE 
statements. Those operations require the exclusive access to the affected database. If there are
connections on the affected database the drop or rename operation will abort.

\begin{lstlisting}[style=pgsql]
postgres=# ALTER DATABASE db_test RENAME TO db_to_drop;
ALTER DATABASE

postgres=# DROP DATABASE db_to_drop;
DROP DATABASE

\end{lstlisting}




\section{Tables}\index{Tables}
\label{sec:TABLES}
In our top down approach to the PostgreSQL's logical model, the next step is the relation.
In the PostgreSQL jargon a relation is an object which carries the data or the way to
retrieve the data. We'll take a look in particular to three of them starting with the
tables.\newline

A table is the fundamental storage unit for the data. PostgreSQL implements many kind of tables
with different levels of durability. A table is created using the SQL command CREATE TABLE. The data
is stored into a table without any specific order. Because the MVCC implementation a row update can
change the row's physical position. For more informations look to \ref{sec:MVCC}. PostgreSQL
implements three kind of tables.

\subsection{Logged tables}\index{Logged tables}
By default CREATE TABLE creates a logged table. This kind of table implements the durability
logging any change to the write ahead log. The data pages are loaded in the shared buffer and any
change is logged first to the WAL. The consolidation to the to the data file happens later. 

\subsection{Unlogged tables}\index{Unlogged tables}
\label{sub:UNLOGGEDTABLES}
An unlogged table is not crash safe. The data is still consolidated to the data file but the pages
modified in memory do not write their changes to the WAL. The main advantage is the write operations
are considerably faster at the cost of the data durability. The data stored into an ulogged table
should be considered partially volatile. The database will truncate those tables when the crash
recovery occurs. Because the unlogged table don't write to the WAL, the data in those tables is not
accessible to the physical standby. 


\subsection{Temporary tables}\index{Temporary tables}
A temporary table's lifespan lasts the time of the connection. This kind of 
tables are useful for any in memory operation. The temporary table stays in 
memory as long as the amount of data is no bigger than 
temp\_buffers seen in \ref{sub:TEMPBUF}. 


\subsection{Foreign tables}\index{Foreign tables}
The foreign tables were first introduced with PostgreSQL 9.1, improving 
considerably the way the remote data can be accessed. 
A foreign table requires a called foreign data wrapper to define a foreign 
server. This can be literally anything. Between the contrib modules PostgreSQL 
has the file\_fdw to create foreign tables referring CSV or COPY formatted flat 
files. In the the version 9.3 finally appeared the postgres\_fdw with the read 
write for the foreign tables. The postgres\_fdw implementation is similar to 
dblink with a more efficient performance management and the connection caching.

\section{Table inheritance}\index{Table inheritance}
As PostgreSQL is an Object Relational Database Management System, some of the 
object oriented programming concepts are implemented. The relations are 
referred generally as classes and the columns as attributes. 

The inheritance binds a parent table to one or more child tables which have 
the same parent's attribute structure. The inheritance can be defined at 
creation time or later. If a manually defined table shall inherit another the 
attribute structure shall be the same as the parent's.

The PostgreSQL implementation is rather confusing as the unique constraints 
aren't globally enforced on the inheritance tree and this prevents the foreign 
key to refer inherited tables. This limitation makes the table 
partitioning tricky.

\section{Indices}
An index is a relation capable to map the values in an structured way pointing
the table's position where the rows are stored. The presence of an index 
doesn't mean this will be used for read. By default and index block read have 
an estimated cost four times than the table block read. However the optimiser
can be controlled using the two GUC parameters seq\_page\_cost for the table 
 sequential read read and random\_page\_cost for the index. Lowering the latter 
will make more probable the optimiser will choose the indices when building the 
execution plan.

Creating indices is a complex task. At first sight adding an index could seem a 
harmless action. Unfortunately their presence adds overhead to the write 
operations unpredictably. The rule of thumb is \textit{add an index only 
if really needed}. Monitoring the index usage is crucial. Querying the 
statistics view pg\_stat\_all\_indexes is possible to find out if the indices 
are used or not.

For example, the following query finds all the indices in che public 
schema, with zero usage from the last database statistics reset.

\begin{lstlisting}[style=pgsql]
SELECT
        schemaname,
        relname,
        indexrelname,
        idx_scan
FROM
         pg_stat_all_indexes
WHERE
                schemaname='public'
        AND     idx_scan=0
;


\end{lstlisting}



PostgreSQL supports many index types. 

The general purpose B-tree\index{index,b-tree}, implementing the Lehman and 
Yao's high-concurrency B-tree management algorithm. The B-tree can handle 
equality and range queries and returns ordered data. As the data is actually 
stored in the page and because the index is not TOASTable, the max length for an 
index entry is 1/3 of the page size. This is the limitation for the variable 
length indexed data (e.g. text). \newline

The hash indices\index{index,hash} can handle only equality and aren't WAL 
logged. That means their changes are not replayed if the crash recovery occurs, 
requiring a reindex in case of unclean shutdown.\newline

The GiST indices\index{index,GiST} are the Generalised Search Tree. The GiST 
is a collection of indexing strategies organized under an 
infrastructure. They can implement arbitrary indexing schemes like B-trees, 
R-trees  or other. The operator classes shipped with PostgreSQL are for the two 
elements geometrical data and for the nearest-neighbor search. As the GiST 
indices are not exact , when scanned the returned set doesn't requires a to 
remove the false positives.\newline

The GIN indices \index{index,GIN} are the Generalised Inverted Indices. This 
kind of index is optimised for indexing the composite data types or vectors 
like the full text search elements. The GIN are exact indices, when scanned the 
returned set doesn't require recheck.

There's no bitmap index implementation in PostgreSQL. 
At runtime the executor can emulate partially the bitmap indices reading the 
B-tree sequentially and matching the occurrences in the on the fly generated 
bitmap. 

The index type shall be specified in the create statement. If the type is 
omitted then the index will default to the B-tree.

\begin{lstlisting}[style=pgsql]
 CREATE INDEX idx_test ON t_test USING hash (t_contents);
\end{lstlisting}


As the index maintenance is a delicate matter, the argument is described in 
depth in \ref{cha:MAINTENANCE}.


\section{Views}
\index{views}
\label{sec:VIEWS}
A view is the representation of a query, stored in the system catalogue 
for quick access. All the objects involved in the view are translated to the 
internal identifiers at the creation time; the same happens for any wild card 
which is expanded to the column list.

An example will explain better the concept. Let's create a simple table. Using 
the generate\_series() function let's put some data into it.

\begin{lstlisting}[style=pgsql]


CREATE TABLE t_data 
        ( 
                i_id serial,
                t_content       text
        );

ALTER TABLE t_data 
ADD CONSTRAINT pk_t_data PRIMARY KEY (i_id);


INSERT INTO t_data
        (
                t_content
        )
SELECT
        md5(i_counter::text)
FROM
        (
                SELECT
                        i_counter
                FROM
                        generate_series(1,200) as i_counter
        ) t_series;

CREATE OR REPLACE VIEW v_data 
AS 
  SELECT 
          *
  FROM 
        t_data;


\end{lstlisting}

The SELECT * from t\_data or v\_data looks exactly the same, the view 
simply runs the stored SQL used at creation time.
If we look to the stored definition in pg\_views we'll find the wildcard is 
expanded into the table's columns.


\begin{lstlisting}[style=pgsql]
 db_test=# SELECT * FROM pg_views where viewname='v_data';
-[ RECORD 1 ]--------------------
schemaname | public
viewname   | v_data
viewowner  | postgres
definition |  SELECT t_data.i_id,
           |     t_data.t_content
           |    FROM t_data;


\end{lstlisting}

Now let's add a new column to the t\_data table and run again the select on the 
table and the view.

\begin{lstlisting}[style=pgsql]
 ALTER TABLE t_data ADD COLUMN d_date date NOT NULL default now()::date;
 
 db_test=# SELECT * FROM t_data LIMIT 1;
 i_id |            t_content             |   d_date   
------+----------------------------------+------------
    1 | c4ca4238a0b923820dcc509a6f75849b | 2014-05-21
(1 row)


db_test=# SELECT * FROM v_data LIMIT 1;
 i_id |            t_content             
------+----------------------------------
    1 | c4ca4238a0b923820dcc509a6f75849b
(1 row)


 
\end{lstlisting}

The view doesn't show the new column. To update the view definition a new 
CREATE OR REPLACE VIEW statement must be issued.

\begin{lstlisting}[style=pgsql]
 CREATE OR REPLACE VIEW v_data 
AS 
  SELECT 
        *
  FROM 
        t_data;
        
db_test=# SELECT * FROM v_data LIMIT 1;
 i_id |            t_content             |   d_date   
------+----------------------------------+------------
    1 | c4ca4238a0b923820dcc509a6f75849b | 2014-05-21
(1 row)

\end{lstlisting}

Because the views are referring the objects identifiers they will never 
invalidate when the referred objects are altered. 
The CREATE OR REPLACE statement updates the view definition only if the column 
list adds new attributes in the end. 
Otherwise, any change to the existing columns requires the view's drop and 
recreate. 

When one or more view are pointing a relation this cannot be dropped. 
The option CASCADE in the drop statement will drop the dependant objects before 
the final drop. This is a dangerous approach though. Dropping objects 
regardless can result in data or functionality loss.

When a drop is blocked by dependant objects the database emits a message with 
the informations about the dependencies. If the amount of objects is too much 
big it's better to query the pg\_depend table to find out the correct 
dependencies. This table lists all the dependencies for each object using a 
peculiar logic.


As seen before a view is a logical short cut to a pre saved query. This means 
the database will follow all the steps to execute exactly the same way if the 
entire query has been sent via client, except for the network overhead.

Nothing forbids a view to point another view inside the definition or join 
the one or more views in a different query. This can cause massive 
regression on the overall performance because each view require an execution 
plan and mixing the views will cause not efficient planning. 

To mark a relation is a view it's a good idea to use a naming prefix like v\_. 
This will distinguish them from the tables marked with the prefix t\_.
In \ref{cha:COUPLETHINGS} we'll take a to the naming conventions to let the 
database schema self explanatory.

\index{view, updatable}
PostgreSQL from the version 9.3 supports the updatable simple views. 
A view is simple if

\begin{itemize}


 \item   Have exactly one entry in its FROM list, which must be a table or 
another updatable view

 \item Does not contain WITH, DISTINCT, GROUP BY, HAVING,LIMIT, or OFFSET 
clauses at the top level

 \item  Does not contain set operations (UNION, INTERSECT or EXCEPT) at the 
top level

 \item   All columns in the view's select list must be simple references to 
columns of the underlying relation. They cannot be expressions, literals or 
functions. System columns cannot be referenced, either

 \item   columns of the underlying relation do not appear more than once in 
the view's select list

 \item   does not have the security\_barrier property

\end{itemize}

If the view doesn't fit those rules it's still possible to make it updatable 
using the triggers with the INSTEAD OF clause.

\index{view, materialised}
The major version 9.3 introduces also the materialised view concept. This is a 
physical snapshot of the saved SQL and can be refreshed with the statement 
REFRESH MATERIALIZED VIEW.  


\section{Tablespaces}\index{tablespaces,logical}
\label{sub:TBS-LOGICAL}
A tablespace\index{tablespace} is a logical shortcut for a physical location. 
This feature was first introduced with the major release 8.0, recently with the 
9.2 had a small adjustment to make the dba life easier.

When a new relation is created without tablespace specification, the relation's
tablespace is set to the GUC prameter default\_tablespace or, if this is 
missing, is set to the database's default tablespace. 
Anyway, without any specification the default tablespace is the pg\_default, 
corresponding to the \$PGDATA/base directory.

In order to create a new tablespace the chosen directory must be owned by 
the os user which started the postgres process and must be specified as 
absolute path. 

For example, having a folder named /var/lib/postgresql/pg\_tbs/ts\_test a 
tablespace we can create a new tablespace ts\_test.

\begin{lstlisting}[style=pgsql]
CREATE TABLESPACE ts_test 
OWNER postgres
LOCATION '/var/lib/postgresql/pg_tbs/ts_test' ;

\end{lstlisting}

Only superusers can create tablespaces. The OWNER clause is optional, if 
omitted the tablespace is owned by the user issuing the command.

The tablespaces are cluster wide, each database sees the same list in the 
pg\_tablespace system table.

To create a relation into the tablespace ts\_test just add the TABLESPACE 
clause followed by the tablespace name at creation 
time.

\begin{lstlisting}[style=pgsql]
CREATE TABLE t_ts_test
        (
                i_id serial,
                v_value text
        )
TABLESPACE ts_test ;

\end{lstlisting}

It's possible to move a relation from a tablespace to another using the 
ALTER command.

For example, this is the command to move the previously created table to the 
pg\_default tablespace.

\begin{lstlisting}[style=pgsql]
ALTER TABLE t_ts_test SET TABLESPACE pg_default;
\end{lstlisting}

The move is transaction safe but requires an exclusive lock on the affected 
relation. If the relation have a significant size this means no access to 
the data for the time required by the move.

In addition, changing the tablespaces is not permitted when the backup is in 
progress, the exclusive lock is not compatible with the locks issued by the 
schema and data export.

The tablespace feature adds flexibility to the space management. Even if is 
still primitive a careful design can improve sensibly the performances, for 
example, putting tables and indices on different devices to maximise the disks 
bandwidth.

To remove a tablespace there is the  DROP TABLESPACE command. The tablespace 
must be empty before the drop. There's no CASCADE clause to have the 
tablespace's contents dropped with the tablespace.

\begin{lstlisting}[style=pgsql]
postgres=# DROP TABLESPACE ts_test;
ERROR:  tablespace "ts_test" is not empty

postgres=# ALTER TABLE t_ts_test SET TABLESPACE pg_default;
ALTER TABLE
postgres=# DROP TABLESPACE ts_test;
DROP TABLESPACE

\end{lstlisting}

In \ref{sub:TBS-PHYSICAL} we'll take a look to the how PostgreSQL 
implements the tablespaces on the physical side.

\section{Transactions}
\label{sec:TRANSACTION}
\index{transactions}
PostgreSQL implements the MVCC\index{MVCC} which stands for Multi Version 
Concurrency Control\index{Multi Version Concurrency Control}. 
This offers high efficiency in multi user access for read and write queries.
When a new query starts a transaction identifier is assigned, the XID 
\index{XID} a 32 bit quantity. To determine the transaction's snapshot 
visibility, all the committed transactions with XID lesser than the current XID 
are in the past and then visible. Otherwise, all the transactions with XID 
greater than the current XID are in the future and not visible.\newline

This comparison happens at tuple level using two system fields xmin and xmax 
having the xid data type. 
When a transaction creates a new tuple then the transaction's xid is put into 
the tuple's xmin value. 
When a transaction deletes a tuple then the xmax value is set to the 
transaction's xid leaving the tuple in place for read consistency. 
When a tuple is visible to any transaction is called a live tuple, a tuple 
which is no longer visible is a dead tuple.\newline

PostgreSQL have no dedicated field for the update's xid. That's because when an 
UPDATE is issued PostgreSQL creates a new tuple's version with the updated 
data and sets the xmax value in the old version making it disappear.\newline

A dead tuple can be reclaimed by VACUUM if no longer required by running 
transactions, anyway tables updated often can result in data bloat for the dead 
tuples and for the eventual indices.
Look to \ref{sec:TUPLES} for more information on the 
tuples.\newline

When designing a new data model, the PostgreSQL's peculiar behaviour on the 
update should be the first thing to consider, in order to limit the table 
and index bloat. \newline

Among the xmin,xmax two other system fields the cmin and cmax which data type 
is CID, command id. Those are similar to the xmin/xmax quantities and usage 
and their usage is to track the internal transaction's commands, in order to 
avoid the command execution on the same tuple more than one time. The pratical 
issue is explained in the well known Halloween Problem. For more informations 
take a look here 
\href{http://en.wikipedia.org/wiki/Halloween_Problem}{
http://en.wikipedia.org/wiki/Halloween\_Problem}.\newline

The SQL standard defines four level of transaction's isolation levels where 
some phenomena are permitted or forbidden.
\index{transactions, isolation levels}
Those phenomena are the following.

\begin{itemize}
 \item \textbf{dirty read} A transaction reads data written by a concurrent 
uncommitted transaction

\item \textbf{nonrepeatable read} A transaction re reads the data previously 
read and finds the data changed by another transaction which has
committed since the initial read

\item \textbf{phantom read} A transaction re executes a query returning a set 
of rows satisfying a search condition and finds that the set of rows 
satisfying the condition has changed because another recently-committed 
transaction

\end{itemize}

Table \ref{tab:TRNISOLATION} shows the isolation levels with the 
allowed phenomena. In PostgreSQL it's possible to set all the four 
isolation levels but only the three more strict are supported. Setting the 
isolation level to read uncommited fallback to the read committed in any case.

\begin{table}[H]
  \begin{tabular}{cccc}
    Isolation Level & Dirty Read    &    Nonrepeatable Read   &   Phantom 
Read\\ 
    \hline
    Read uncommitted  &  Possible    &    Possible     &   Possible\\
    Read committed    &  Not possible &  Possible     &   Possible\\
    Repeatable read   &  Not possible  & Not possible  &  Possible\\
    Serializable      &  Not possible  & Not possible   & Not possible\\
  \end{tabular}
  \caption{\label{tab:TRNISOLATION}SQL Transaction isolation levels}
\end{table}

By default the global isolation level is set to read committed, it's possible 
to change the session's transaction isolation level using the command:
\begin{lstlisting}[style=pgsql]
SET TRANSACTION ISOLATION LEVEL { SERIALIZABLE | REPEATABLE READ | READ 
COMMITTED | READ UNCOMMITTED }; 
\end{lstlisting}

To change the default transaction isolation level cluster wide there is the GUC 
parameter transaction\_isolation.


\subsection{Snapshot exports}
\label{sub:SNAPEXPORT}\index{transactions, snapshot export}
Since PostgreSQL 9.2 are supported the transaction's snapshot exports. A session with an open 
transaction, can export its consistent snapshot to any other session. The snapshot remains valid 
meanwhile the transaction is open. Using this functionality offers a way to run multiple backends 
on a consistent data set frozen in time. This feature resulted in the brilliant parallel export in 
the 9.3's pg\_dump as described in \ref{sec:PGDUMPINT}.\newline

In the following example, let's consider the table created in \ref{sec:VIEWS}. We'll first start an 
explicit transaction and then we'll export the current snapshot.

\begin{lstlisting}[style=pgsql]
postgres=# BEGIN TRANSACTION ISOLATION LEVEL REPEATABLE READ;
BEGIN
postgres=# SELECT pg_export_snapshot();
 pg_export_snapshot 
--------------------
 00001369-1
(1 row)

postgres=# SELECT count(*) FROM t_data;
 count 
-------
   200
(1 row)

\end{lstlisting}

We are first starting a transaction with the REPEATABLE READ isolation level. The second 
statement exports the current snapshot using the function pg\_export\_snapshot(). Finally we are 
checking with a simple row count the table t\_data have data inside.\newline

We can now login with in a different session and delete all the rows from the t\_data table.

\begin{lstlisting}[style=pgsql]
postgres=# DELETE FROM t_data;
DELETE 200
postgres=# SELECT count(*) FROM t_data;
 count 
-------
     0
(1 row)

\end{lstlisting}

With the table now empty let's import the snapshot exported by the first backend.

\begin{lstlisting}[style=pgsql]
postgres=# BEGIN TRANSACTION ISOLATION LEVEL REPEATABLE READ;
BEGIN
postgres=# SET TRANSACTION SNAPSHOT '00001369-1';
SET
postgres=# SELECT count(*) FROM t_data;
 count 
-------
   200
(1 row)

\end{lstlisting}

The function pg\_export\_snapshot saves the current snapshot returning the text string which 
identifies the snapshot. Passing the string to clients that want to import the snapshot gives to 
independent sessions a single consistent vision. The import is possible only until the end of the 
transaction that exported it. The export is useful only in the READ COMMITTED transactions, 
because the REPEATABLE READ and higher isolation levels use the same snapshot within their 
lifetime. 

