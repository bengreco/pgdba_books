\chapter{Backup}
\label{cha:BACKUP}
The hardware is subject to faults. In particular if the storage is lost the entire data 
infrastructure becomes inaccessible, sometime for good. Also human errors, like wrong delete or table drop 
can happen. A solid backup strategy is the best protection against these problems and much more. The 
chapter covers the logical backup with pg\_dump. 

\section{pg\_dump at glance}
\label{sec:PGDUMP}
As seen in \ref{sub:PGDUMP}, pg\_dump\index{pg\_dump} is the PostgreSQL's utility for saving 
consistent snapshots of the databases. The usage is quite simple and if launched without options it tries 
to connect to the local cluster with the current user redirecting the dump's output to the standard 
output.\newline

The help gives many useful information.

\begin{verbatim}
postgres@tardis:~/dump pg_dump --help
pg_dump dumps a database as a text file or to other formats.

Usage:
  pg_dump [OPTION]... [DBNAME]

General options:
  -f, --file=FILENAME          output file or directory name
  -F, --format=c|d|t|p         output file format (custom, directory, tar,
                               plain text (default))
  -j, --jobs=NUM               use this many parallel jobs to dump
  -v, --verbose                verbose mode
  -V, --version                output version information, then exit
  -Z, --compress=0-9           compression level for compressed formats
  --lock-wait-timeout=TIMEOUT  fail after waiting TIMEOUT for a table lock
  -?, --help                   show this help, then exit

Options controlling the output content:
  -a, --data-only              dump only the data, not the schema
  -b, --blobs                  include large objects in dump
  -c, --clean                  clean (drop) database objects before recreating
  -C, --create                 include commands to create database in dump
  -E, --encoding=ENCODING      dump the data in encoding ENCODING
  -n, --schema=SCHEMA          dump the named schema(s) only
  -N, --exclude-schema=SCHEMA  do NOT dump the named schema(s)
  -o, --oids                   include OIDs in dump
  -O, --no-owner               skip restoration of object ownership in
                               plain-text format
  -s, --schema-only            dump only the schema, no data
  -S, --superuser=NAME         superuser user name to use in plain-text format
  -t, --table=TABLE            dump the named table(s) only
  -T, --exclude-table=TABLE    do NOT dump the named table(s)
  -x, --no-privileges          do not dump privileges (grant/revoke)
  --binary-upgrade             for use by upgrade utilities only
  --column-inserts             dump data as INSERT commands with column names
  --disable-dollar-quoting     disable dollar quoting, use SQL standard quoting
  --disable-triggers           disable triggers during data-only restore
  --exclude-table-data=TABLE   do NOT dump data for the named table(s)
  --inserts                    dump data as INSERT commands, rather than COPY
  --no-security-labels         do not dump security label assignments
  --no-synchronized-snapshots  do not use synchronized snapshots in parallel jobs
  --no-tablespaces             do not dump tablespace assignments
  --no-unlogged-table-data     do not dump unlogged table data
  --quote-all-identifiers      quote all identifiers, even if not key words
  --section=SECTION            dump named section (pre-data, data, or post-data)
  --serializable-deferrable    wait until the dump can run without anomalies
  --use-set-session-authorization
                               use SET SESSION AUTHORIZATION commands instead of
                               ALTER OWNER commands to set ownership

Connection options:
  -d, --dbname=DBNAME      database to dump
  -h, --host=HOSTNAME      database server host or socket directory
  -p, --port=PORT          database server port number
  -U, --username=NAME      connect as specified database user
  -w, --no-password        never prompt for password
  -W, --password           force password prompt (should happen automatically)
  --role=ROLENAME          do SET ROLE before dump

\end{verbatim}

\subsection{Connection options}
\index{pg\_dump, connection options}
The connection options are used to specify the way the program connects to the cluster. All the options are 
straightforward except for the password. Usually the PostgreSQL clients don't accept the plain password as 
parameter. However is still possible to connect without specifying the password using the 
environmental variable PGPASSWORD\index{PGPASSWORD} or using the password file.\newline

Using the variable PGPASSWORD is considered not secure and shouldn't be used if  not trusted users are 
accessing the server. The password file is a text file named saved in the users's home directory as 
.pgpass . The file must be readable only by the user, otherwise the client will refuse to read it.\newline
Each line specifies a connection in a fixed format.
\index{password file}
\begin{verbatim}
hostname:port:database:username:password
\end{verbatim}

The following example specifies the password for the connection to the host tardis, port 5432, database 
db\_test and user usr\_test.

\begin{verbatim}
tardis:5432:db_test:usr_test:testpwd
\end{verbatim}

\subsection{General options}
\index{pg\_dump, general options}
The general options are used to control the backup's output and format. 

The switch -f sends the backup on the specified FILENAME.\newline

The switch \index{pg\_dump, output formats} -F specifies the backup format and requires a second 
option to tell pg\_dump which format to use. The allowed formats are \textit{c d t p} respectively 
\textit{custom directory tar plain}.\newline

If the parameter is omitted pg\_dump uses the plain text format. not compressed and suitable for the direct 
load using psql. \newline

The the custom and the directory format are the most versatile backup formats. They give compression and 
flexibility at restore time. Both have the parallel and selective restore option.\newline

The directory format stores the schema dump in a toc file. Each table's content is then saved in a 
compressed file inside the target directory specified with the -f switch. From the version 9.3 this format 
allows the parallel dump functionality\index{pg\_dump, parallel export}. \newline

The tar format stores the dump in the well known tape archive format. This format is compatible 
with the directory format, does not compress the data and there is the limit of 8 GB for the individual
table.\newline

The -j option specifies the number of jobs to run in parallel when dumping the data. This feature is 
available from the version 9.3 and uses the transaction's snapshot export to have a consistent dump over 
the multiple export jobs. The switch is usable only with the directory format and only with PostgreSQL 9.2 
and later.\newline 

The option -Z specifies the compression level for the compressed formats. The default is 5 
resulting in a dumped archive from 5 to 8 times smaller than the original database. 

The option --lock-wait-timeout is the number of milliseconds for the table's lock acquisition. 
When expired the dump will fail. Is useful to avoid the program to wait forever for a table lock 
but can result in failed backups if value is too much low.

\subsection{Output options}
\index{pg\_dump, output options}
\label{sub:PGDUMPOUTPUT}
The output options control backup output. Some of those options are meaningful only under certain 
conditions, some others are quite obvious.\newline

The -a option sets the data only export. Separating schema and data have some effects at restore 
time, in particular with the performance. We'll see in the detail in \ref{cha:RESTORE} how to 
build an efficient two phase restore.\newline

The -b option exports the large objects. This is the default setting except if the -n switch is 
used. In this case the -b is required to export the large objects.\newline

The options -c and -C are meaningful only for the plain output format. They respectively add the 
DROP and CREATE command before the object's DDL. For the archive formats the same option exists for 
pg\_restore.\newline

The -E specifies the character encoding for the archive. If not set the database's encoding is 
used.\newline 

The -n switch is used to dump the named schema only. It's possible to specify multiple -n switches 
to select many schema or using the wildcards. However despite the efforts of pg\_dump to get all 
the dependencies resolved, something could be missing. There's no guarantee the resulting archive 
can be successfully restored.\newline

The -N switch does the opposite of the -n switch. Excludes the named database schema from the backup. The 
switch accepts wildcards and it's possible to specify multiple schema with multiple -N switches. When both 
-n and -N are given, the behaviour is to dump just the schema that match at least one -n switch but no -N 
switches. \newline

The -o switch option dumps the object id as part of the table for every table. This options should be 
used only if the OIDs are part of the design. \newline

The -O switch have effects only on plain text exports and does not dump statements setting object 
ownership.\newline

The -s switch option dumps only the database schema.\newline

The -S switch is meaningful only for plain text exports. The switch specifies the super user for disabling 
and enabling the triggers if the export is performed with the option --disable-triggers. \newline

The -t switch is used to dump the named table only. It's possible to specify multiple tables using 
the wildcards or specifying the -t many times.\newline

The -T skips the named table in the dump. It's possible to exclude multiple tables using 
the wildcards or specifying the -T many times.\newline

The switch -x does not save the grant/revoke commands for setting the privileges.\newline

The switch --binary-upgrade is used only for the in place upgrade program pg\_upgrade. Is not intended for 
general usage. 

The switch --insert option dumps the data as INSERT command instead of the COPY. The restore with this 
option is very slow because each statement is parsed and executed individually.\newline

The switch --column-inserts results in the data exported as INSERT commands with all the column 
names specified.\newline

The switch --disable-dollar-quoting disables the dollar quoting for the function's body and uses the 
standard SQL quoting.\newline

The switch --disable-triggers save the statements for disabling  the triggers before the data load and the 
enabling them back after the data load. Disabling the triggers will ensure the foreign keys will not cause 
errors during the data load. This switch have effect only for the plain text export.\newline

The switch --exclude-table-data=TABLE skips the data dump for the named table. The same rules of the -t and 
-T apply to this switch.\newline


The switch --no-security-labels doesn't include the security labels into the dump file.\newline

The switch --no-synchronized-snapshots  is used to run a parallel export with the pre 9.2 
databases. Because the snapshot export feature is missing this means the database shall not change 
state until all the exporting jobs are connected.\newline

The switch --no-tablespaces skips the tablespace assignments.\newline

The switch  --no-unlogged-table-data does not export data for the unlogged relations.\newline

The switch  --quote-all-identifiers  cause all the identifiers to be enclosed in double quotes. \newline

The switch --section option specifies one of the three export's sections. The first section is the 
pre-data, which saves the definitions for the tables, the views and the functions. The second section is the 
data which saves the table's contents. The third section is the post-data which saves the constraints, the 
indices and the eventual GRANT REVOKE commands . This switch applies only to the plain format. \newline

The switch --serializable-deferrable uses a serializable transaction for the dump, to ensure the database 
state is consistent. The dump execution waits for a point in the transaction stream without 
anomalies to avoid the risk of serialization\_failure. The option is not useful for the backup used 
only for disaster recovery and should be used only when the dump should reload into a read only database 
which needs to get a consistent state compatible with the origin's database.\newline

The switch --use-set-session-authorization sets the objects ownership using the command SET SESSION 
AUTHORIZATION instead of the ALTER OWNER. SET SESSION AUTHORIZATION requires the super user privileges 
whereas ALTER OWNER doesn't.


\section{Performance tips}
\index{pg\_dump, performance tips}
Despite the fact pg\_dump doesn't affects the running queries, its strict transactional approach 
have some effects on the affected schema. Any alter schema is blocked until the backup's end. The 
vacuum efficiency is affected as well, because all the dead rows generated during the backup's run, 
cannot be reclaimed being potentially required by the backup's running transaction.\newline

There are some tips to improve the backup's speed.

\subsection{Avoid remote backups}
The pg\_dump can connect to remote databases same like any other PostgreSQL client.
It seems reasonable then to use the program installed on a centralised storage and to dump locally 
from the remote cluster.\newline 
Unfortunately even using the compressed format, the entire database flows uncompressed and in 
clear, from the server to the remote pg\_dump. The compression happens locally when the data is 
received.\newline
This approach expose also a network security issue. If the environment is not trusted then the 
remote connection must happen on a secure channel. This add an extra overhead to the transfer and 
any failure on this layer will fail the entire backup.\newline
A far better approach is to save locally the database, using the local connection if possible, and 
then copy the entire dump file using a secure transfer protocol like scp or sshfs.

\subsection{Skip replicated tables}
If the database is configured as logical slave in slony or londiste for example, backing up the 
replicated table's data is not important as the contents are re synchronised from the master when 
the node is attached to the replication system. The switch --exclude-table-data=TABLE is then 
useful for dumping the table's definition only without the contents.

\subsection{Check for slow cpu cores}
PostgreSQL is a multitasking but not a multithreaded database system. Each backend is attached 
to just one cpu. The pg\_dump opens one backend connection to the cluster in order to 
export the database objects. The pg\_dump process receives the data flow from the backend and it 
saves performing also the optional compression. In this scenario the cpu power is critical in order 
to avoid a bottleneck. This could be helped using the parallel export offered by pg\_dump from the 
version 9.3. The functionality is implemented via  the snapshot exports. As this was introduced 
with PostgreSQL 9.2 the paralle export can happen only from this version and only if the output 
format is directory . 

\subsection{Check for the available locks}
PostgreSQL at various level uses the locks to ensure the data consistency at various levels. For 
example, when a table is read an access share lock on the relation is put in order to avoid any 
structure change. Any backend issuing an ALTER TABLE which affect the table structure, will wait 
for the lock to be released before acquiring itself an exclusive lock and then perform the change.
The relation's locks are stored into the pg\_locks table. This table is quite unique because have 
a limited number of rows. The maximum number of table's lock slot is determine with this simple 
formula.\newline
\begin{math}
 max_locks_per_transaction * (max_connections + max_prepared_transactions)
\end{math}\newline

The default configuration permits have only 6400 table's lock slots. This value is generally OK. 
However, if the database have a great number of relations, a full backup, pg\_dump could hit the 
slot limit and fail with an out of memory error.\newline
All the three GUC parameters require a restart to apply the new changes so is very important to 
plan the change before the limit is reached. 

\subsection{Try to use flexible formats}
This is more a good practice suggestion rather a performance tip. Exporting the database in plain 
text have some advantages. Is possible to load the dump just using psql and any file's corruption 
can be managed in a simple way; if the damage is limited of course. The custom and directory 
formats need the pg\_restore utility for the restoration. We'll take a look to this approach in 
\ref{cha:RESTORE}. Anyway, the custom and directory formats have alongside with the compression, 
the parallel restore feature and the selective restoration. The compression can be fine tuned to 
suit the export's nature. In this era of ``big data'' is something to consider seriously.


\section{pg\_dump under the bonnet}
\label{sec:PGDUMPINT}
\index{pg\_dump, internals}
The pg\_dump source code gives a very good picture of what exactly the backup 
software does. The process runs into with fixed transaction's isolation level accordingly with the 
server's version. 
The distinction is required because, becoming PostgreSQL more sophisticated at each major release, 
the isolation levels became more and more strict with their meanings.\newline
More informations about the transactions are in \ref{sec:TRANSACTION}.\newline
From PostgreSQL 9.1 the transaction serializable became a real serialisation. The 
transaction's behaviour offered by the serializable in the version up to 9.0 were assigned to the 
REPETABLE READ transaction's isolation, its real kind. The SERIALIZABLE transaction's isolation 
level is still used with DEFERRABLE option when pg\_dump is executed with 
the option --serializable-deferrable as seen in \ref{sub:PGDUMPOUTPUT}. The switch have effect only 
on the remote server with version 9.1 and later. The transaction is also set to READ ONLY, when 
supported by the server, in order to limit the XID generation. \newline

\begin{table}[H]
  \begin{tabular}{ll}
    Server version & Command    \\ 
    \hline
    \textgreater=  9.1 &  REPEATABLE READ, READ ONLY        \\
    \textgreater= 9.1 with --serializable-deferrable  &  SERIALIZABLE, READ ONLY, DEFERRABLE  \\
    \textgreater= 7.4 &  SERIALIZABLE READ ONLY   \\
    \textless 7.4 &  SERIALIZABLE \\
  \end{tabular}
  \caption{\label{tab:TRNPGDUMP}pg\_dump's transaction isolation levels }
\end{table}

From the version 9.3 pg\_dump supports the parallel dump using the feature seen in  
\ref{sub:SNAPEXPORT}. As the snapshot export is supported from the version 9.2 this permit a 
parallel dump from the older major version if using the newer backup program. However, 
pg\_dump accepts the option --no-synchronized-snapshots in order to dump in parallel jobs 
from the older versions. The data is not supposed to be consistent if there are  read write 
transactions during this kind of export. To have a consistent export in this case all the 
transactions which modify the data must be stopped meanwhile the export is in progress.\newline 

When exporting in parallel the only permitted format is the directory. The pg\_restore program 
since the version 9.3 supports also the directory format for the paralelle data restoration.
The combination of the parallel jobs backup and the parallel jobs restore, can improve massively 
either the backup and the recovery speed in case of disaster.

\section{pg\_dumpall}
\index{pg\_dumpall}
The pg\_dumpall program is more very basic version of pg\_dump. It's usage is mainly for 
dumping all the databases in the cluster. Having lesser options than pg\_dump is also less 
flexible. However one option is absolutely useful and should be included in any backup plan to 
ensure a rapid disaster recovery.\newline

The\index{pg\_dumpall, global objects} --globals-only option saves on the standard output all the 
cluster wide options like the tablespaces, the roles and the privileges. The passwords are dumped as 
well, encrypted in md5 format if stored this way. The utility output is just plain text. 
The best way to save the globals is to specify the -f option followed by the file name. This file 
can be loaded into an empty cluster to recreate the global objects. The tablespaces, if any present, 
must have the filesystem already in place before running the sql as PostgreSQL doesn't create the 
filesystem structure.\newline

This example shows the program call and the contents of the output file.
\begin{lstlisting}[style=pgsql]
postgres@tardis:~/dmp$ pg_dumpall --globals-only -f main_globals.sql
postgres@tardis:~/dmp$ cat main_globals.sql 
--
-- PostgreSQL database cluster dump
--

SET default_transaction_read_only = off;

SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;

--
-- Roles
--

CREATE ROLE postgres;
ALTER ROLE postgres WITH SUPERUSER INHERIT CREATEROLE CREATEDB LOGIN REPLICATION;

--
-- PostgreSQL database cluster dump complete
--

postgres@tardis:~/dmp

\end{lstlisting}


\section{Backup validation}
\index{backup validation}
There's little advantage in having a backup if this is not restorable. The corruption danger is at 
various levels and unfortunately the problem appears only when the restore is needed.\newline 

The corruption have many causes. If the dump is saved locally a damaged filesystem can hide the 
problem meanwhile the corrupted block is not read. Also the disk subsystem with undetected problems 
will result in a silent corruption. In order to limit this kind of problems the filesystem of 
choice should be a solid one with strong journaling support.\newline

The disk subsystem should guarantee the data reliability rather the speed. Slow disks when backing 
up the data, in particular if in the compressed format don't limit the speed, being the cpu power 
the real bottleneck.\newline
If the dump file is transferred over the network is a good idea to generate a md5 checksum to 
check the file integrity after the transfer.\newline

All those measures don't give the security the backup is restorable. The only test capable to 
ensure the backup is good is a test restore on separate server. This can be a single test or a more 
structured check. Which strategy to adopt is determined by the amount of data, the time required 
for the restore and the backup schedule.\newline

The general purpose databases, which size is measurable in hundreds of gigabytes, the restore can 
complete in few hours and the continuous test is feasible. For the VLDB, which size is measured in 
terabytes, the restore can take more than one day, in particular if there are big indices requiring 
expensive sort on disk for the build. In this scenario a weekly restore gives a good perception if 
there are potential problems with the saved data. 