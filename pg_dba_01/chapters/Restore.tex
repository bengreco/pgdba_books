\chapter{Restore}
\label{cha:RESTORE}
There's little advantage in saving the data if the restore is not possible. In this chapter we'll take a 
look to the fastest and possibly the safest way to restore the saved dump.\newline

The program used for the restore is determined by the dump format. We'll first take a look to the restore 
using a plain format then the custom and  the directory formats. Finally we'll the way to improve 
the restore performances with a temporary sacrifice of the cluster's reliability.

\section{The plain format}
\label{sec:PLAINFORMAT}
As seen in \ref{cha:BACKUP} the pg\_dump's output is plain SQL. The generated script gives no choice but 
loading it into psql. The SQL statements are parsed and executed in sequence.\newline

This format have few advantages. For example it's possible to edit the statements using a common 
text editor. This of course if the dump is reasonably small. Even loading a file with vim when its size is 
measured in gigabytes becomes a stressful experience though.\newline

The data contents are saved using the COPY command. At restore time this choice have the best performance.

It's possible to save the data contents using the inserts. The restore is indeed very slow because each 
statement has to be parsed, planned and executed.\newline

If the backup saves the schema and the data in two separate files this requires extra care at dump time if 
there are triggers and foreign keys in the database schema. 

The data only backup should include the switch --disable-triggers which writes emit the DISABLE 
TRIGGER statements before the data load and the ENABLE TRIGGER after the data is restored.\newline

The following example shows a dump/reload session using the separate schema and data dump files.

Let's create a new database with a simple data structure. Two tables storing a city and the address 
and a foreign key between them enforcing the referential integrity.

\begin{lstlisting}[style=pgsql]
postgres=# CREATE DATABASE db_addr;
CREATE DATABASE
postgres=# \c db_addr 
You are now connected to database "db_addr" as user "postgres".
db_addr=# CREATE TABLE t_address
        (
                i_id_addr serial,
                i_id_city integer NOT NULL,
                t_addr text,
                CONSTRAINT pk_id_address PRIMARY KEY (i_id_addr)
        )
;
CREATE TABLE
db_addr=# CREATE TABLE t_city
        (
                i_id_city       serial,
                v_city          character varying (255),
                v_postcode      character varying (20),
                CONSTRAINT pk_i_id_city PRIMARY KEY (i_id_city)
        )
;
CREATE TABLE
db_addr=# ALTER TABLE t_address ADD 
	    CONSTRAINT fk_t_city_i_id_city FOREIGN KEY (i_id_city)  
	    REFERENCES t_city(i_id_city) 
	      ON DELETE CASCADE 
	      ON UPDATE RESTRICT;
ALTER TABLE

\end{lstlisting}

Now let's put some data into the tables.

\begin{lstlisting}[style=pgsql]
INSERT INTO t_city
	    ( 
	      v_city,	
	      v_postcode
	     )
      VALUES
	    (
	      'Leicester - Stoneygate',
	      'LE2 2BH'
	    )
      RETURNING i_id_city
;

 i_id_city 
-----------
         3
(1 row)

db_addr=# INSERT INTO t_address
	    ( 
	      i_id_city,	
	      t_addr
	     )
      VALUES
	    (
	      3,
	      '4, malvern road '
	    )
      RETURNING i_id_addr
;

 i_id_addr 
-----------
         1
(1 row)


\end{lstlisting}

We'll now execute dump the schema and the data in two separate plain files. Please note we are not using 
the --disable-triggers switch.

\begin{verbatim}
postgres@tardis:~/dmp$ pg_dump --schema-only db_addr > db_addr.schema.sql
postgres@tardis:~/dmp$ pg_dump --data-only db_addr > db_addr.data.sql

\end{verbatim}

Looking to the schema dump it's quite obvious what it does. All the DDL are saved in the correct 
order to restore the same database structure .\newline
The data is then saved by pg\_dump in the correct order for having the referential integrity 
guaranteed. In our very simple example the table t\_city is dumped before the table t\_address. 
This way the data will not violate the foreign key. In a complex scenario where multiple foreign keys are 
referring the same table, the referential order is not guaranteed. Let's run the same 
dump with the option --disable-trigger.

\begin{verbatim}
postgres@tardis:~/dmp$ pg_dump --disable-triggers --data-only db_addr > db_addr.data.sql

\end{verbatim}

The copy statements in this case are enclosed by two extra statements which disable and then re enable the 
triggers.

\begin{lstlisting}[style=pgsql]

ALTER TABLE t_address DISABLE TRIGGER ALL;

COPY t_address (i_id_addr, i_id_city, t_addr) FROM stdin;
1	3	4, malvern road 
\.


ALTER TABLE t_address ENABLE TRIGGER ALL;

\end{lstlisting}

The foreign keys and all the user defined trigger will not fire during the data restore, ensuring 
the data will be safely stored and improving the speed.\newline

Let's then create a new database where we'll restore the dump starting from the saved schema.

\begin{lstlisting}[style=pgsql]
postgres=# CREATE DATABASE db_addr_restore;
CREATE DATABASE
postgres=# \c db_addr_restore 
You are now connected to database "db_addr_restore" as user "postgres".
db_addr_restore=# \i db_addr.schema.sql 
SET
...
SET
CREATE EXTENSION
COMMENT
SET
SET
CREATE TABLE
ALTER TABLE
CREATE SEQUENCE
ALTER TABLE
ALTER SEQUENCE
CREATE TABLE
ALTER TABLE
CREATE SEQUENCE
ALTER TABLE
ALTER SEQUENCE
ALTER TABLE
...
ALTER TABLE
REVOKE
REVOKE
GRANT
GRANT
db_addr_restore=# \i db_addr.data.sql 
SET
...
SET
ALTER TABLE
...
ALTER TABLE
 setval 
--------
      1
(1 row)

 setval 
--------
      3
(1 row)

db_addr_restore=# \d
                   List of relations
 Schema |          Name           |   Type   |  Owner   
--------+-------------------------+----------+----------
 public | t_address               | table    | postgres
 public | t_address_i_id_addr_seq | sequence | postgres
 public | t_city                  | table    | postgres
 public | t_city_i_id_city_seq    | sequence | postgres
(4 rows)

\end{lstlisting}



\section{The binary formats}
\label{sec:PGDUMPBINFMT}
The three binary formats supported by pg\_dump are the custom, the directory and the tar format. 
The first two can be accessed randomly by the restore program and have the parallel restore 
support, being the best choice for having a flexible and reliable restore. Before the the 9.3 the 
only format supporting the parallel restore was the custom. With this version the directory 
format accepts the -j switch. This feature, combined with the parallel dump seen in 
\ref{sec:PGDUMPINT} is a massive improvement for saving big amount of data. The tar format does 
have the limit of 12 GB in the archive's file size and doesn't offer the parallel restore nor the 
selective restore. \newline

The custom format is a binary archive with a table of contents pointing the various archive 
sections. The directory format is a directory which name is the value provided with the -f switch. 
The directory contents are a toc.dat file, where the table of contents and the schema are stored. 
For each table there is a gzip file which name is a number corresponding to the toc entry for the 
saved relation. Those files store the data restore for the relation.\newline


The restore from the binary formats requires the pg\_restore usage. Because almost 
all the pg\_dump's switches are supported by pg\_restore we'll not repeat the look out. Take a look 
to  \ref{sec:PGDUMP} for the switch meanings. Anyway this is the pg\_restore's help output.
\newline 

\begin{verbatim}
pg_restore restores a PostgreSQL database from an archive created by pg_dump.

Usage:
  pg_restore [OPTION]... [FILE]

General options:
  -d, --dbname=NAME        connect to database name
  -f, --file=FILENAME      output file name
  -F, --format=c|d|t       backup file format (should be automatic)
  -l, --list               print summarized TOC of the archive
  -v, --verbose            verbose mode
  -V, --version            output version information, then exit
  -?, --help               show this help, then exit

Options controlling the restore:
  -a, --data-only              restore only the data, no schema
  -c, --clean                  clean (drop) database objects before recreating
  -C, --create                 create the target database
  -e, --exit-on-error          exit on error, default is to continue
  -I, --index=NAME             restore named index
  -j, --jobs=NUM               use this many parallel jobs to restore
  -L, --use-list=FILENAME      use table of contents from this file for
                               selecting/ordering output
  -n, --schema=NAME            restore only objects in this schema
  -O, --no-owner               skip restoration of object ownership
  -P, --function=NAME(args)    restore named function
  -s, --schema-only            restore only the schema, no data
  -S, --superuser=NAME         superuser user name to use for disabling triggers
  -t, --table=NAME             restore named table(s)
  -T, --trigger=NAME           restore named trigger
  -x, --no-privileges          skip restoration of access privileges (grant/revoke)
  -1, --single-transaction     restore as a single transaction
  --disable-triggers           disable triggers during data-only restore
  --no-data-for-failed-tables  do not restore data of tables that could not be
                               created
  --no-security-labels         do not restore security labels
  --no-tablespaces             do not restore tablespace assignments
  --section=SECTION            restore named section (pre-data, data, or post-data)
  --use-set-session-authorization
                               use SET SESSION AUTHORIZATION commands instead of
                               ALTER OWNER commands to set ownership

Connection options:
  -h, --host=HOSTNAME      database server host or socket directory
  -p, --port=PORT          database server port number
  -U, --username=NAME      connect as specified database user
  -w, --no-password        never prompt for password
  -W, --password           force password prompt (should happen automatically)
  --role=ROLENAME          do SET ROLE before restore

If no input file name is supplied, then standard input is used.

Report bugs to <pgsql-bugs@postgresql.org>.
 
\end{verbatim}


If the database connection is omitted pg\_restore sends the output to the standard output. The 
switch -f sends the output to a filename though. This is very useful if we want to check the 
original dump file is readable, executing a restore onto the /dev/null device.\newline
The the custom and directory formats show their power when restoring on a database 
connection in a multi core system. Using the -j switch it's possible to specify the number of 
parallel jobs for the data and the post data section. This can improve massively the 
recovery time, running the most time consuming actions in multiple processes.\newline

The word \textit{parallel} can be confusing in some way. PostgreSQL does not supports 
multithreading. That means each backend process will use just only one cpu. In this context, each 
job take care of a different area of the restore's table of contents, The TOC is split in 
many queues with a fixed object list to process. For example one queue will contain the data 
restoration for a table, and the relation's indices and constraints.\newline

The switch --section offers a fine grain control on which section of the archived data will be 
restored. In a custom and directory format there are three distinct sections. 
\begin{itemize}
 \item \textbf{pre-data} This section restores only the schema definitions not affecting the speed 
and reliability of the data restore. e.g. table's DDL, functions creation, extensions, etc.
\item  \textbf{data} The data restore itself, by default saved as COPY statements to speed up the 
process
\item  \textbf{post-data} This section runs the restore for all the objects enforcing the data 
integrity, like the primary and foreign keys, triggers and the indices which presence during the 
restore slow down the data reload massively.
\end{itemize}

The switch -C creates the target database before starting the restoration. To do this the 
connection must happen first on another database. \newline

We'll now will see how to restore the database seen in \ref{sec:PLAINFORMAT} in the same two 
steps approach, using the custom format.\newline

Let's start with a complete database dump using the custom format. 

\begin{verbatim}
postgres@tardis:~/dump$ pg_dump -Fc -f db_addr.dmp  db_addr
pg_dump: reading schemas
pg_dump: reading user-defined tables
pg_dump: reading extensions
pg_dump: reading user-defined functions
pg_dump: reading user-defined types
pg_dump: reading procedural languages
pg_dump: reading user-defined aggregate functions
pg_dump: reading user-defined operators
pg_dump: reading user-defined operator classes
pg_dump: reading user-defined operator families
pg_dump: reading user-defined text search parsers
pg_dump: reading user-defined text search templates
pg_dump: reading user-defined text search dictionaries
pg_dump: reading user-defined text search configurations
pg_dump: reading user-defined foreign-data wrappers
pg_dump: reading user-defined foreign servers
pg_dump: reading default privileges
pg_dump: reading user-defined collations
pg_dump: reading user-defined conversions
pg_dump: reading type casts
pg_dump: reading table inheritance information
pg_dump: reading event triggers
pg_dump: finding extension members
pg_dump: finding inheritance relationships
pg_dump: reading column info for interesting tables
pg_dump: finding the columns and types of table "t_address"
pg_dump: finding default expressions of table "t_address"
pg_dump: finding the columns and types of table "t_city"
pg_dump: finding default expressions of table "t_city"
pg_dump: flagging inherited columns in subtables
pg_dump: reading indexes
pg_dump: reading indexes for table "t_address"
pg_dump: reading indexes for table "t_city"
pg_dump: reading constraints
pg_dump: reading foreign key constraints for table "t_address"
pg_dump: reading foreign key constraints for table "t_city"
pg_dump: reading triggers
pg_dump: reading triggers for table "t_address"
pg_dump: reading triggers for table "t_city"
pg_dump: reading rewrite rules
pg_dump: reading large objects
pg_dump: reading dependency data
pg_dump: saving encoding = UTF8
pg_dump: saving standard_conforming_strings = on
pg_dump: saving database definition
pg_dump: dumping contents of table t_address
pg_dump: dumping contents of table t_city

\end{verbatim}

We'll use a second database for the restore.

\begin{lstlisting}[style=pgsql]
postgres=# CREATE DATABASE db_addr_restore_bin;
CREATE DATABASE

\end{lstlisting}

We'll then restore just the schema using the following command.

\begin{verbatim}
postgres@tardis:~/dump$ pg_restore -v -s -d db_addr_restore_bin db_addr.dmp 
pg_restore: connecting to database for restore
pg_restore: creating SCHEMA public
pg_restore: creating COMMENT SCHEMA public
pg_restore: creating EXTENSION plpgsql
pg_restore: creating COMMENT EXTENSION plpgsql
pg_restore: creating TABLE t_address
pg_restore: creating SEQUENCE t_address_i_id_addr_seq
pg_restore: creating SEQUENCE OWNED BY t_address_i_id_addr_seq
pg_restore: creating TABLE t_city
pg_restore: creating SEQUENCE t_city_i_id_city_seq
pg_restore: creating SEQUENCE OWNED BY t_city_i_id_city_seq
pg_restore: creating DEFAULT i_id_addr
pg_restore: creating DEFAULT i_id_city
pg_restore: creating CONSTRAINT pk_i_id_city
pg_restore: creating CONSTRAINT pk_id_address
pg_restore: creating FK CONSTRAINT fk_t_city_i_id_city
pg_restore: setting owner and privileges for DATABASE db_addr
pg_restore: setting owner and privileges for SCHEMA public
pg_restore: setting owner and privileges for COMMENT SCHEMA public
pg_restore: setting owner and privileges for ACL public
pg_restore: setting owner and privileges for EXTENSION plpgsql
pg_restore: setting owner and privileges for COMMENT EXTENSION plpgsql
pg_restore: setting owner and privileges for TABLE t_address
pg_restore: setting owner and privileges for SEQUENCE t_address_i_id_addr_seq
pg_restore: setting owner and privileges for SEQUENCE OWNED BY t_address_i_id_addr_seq
pg_restore: setting owner and privileges for TABLE t_city
pg_restore: setting owner and privileges for SEQUENCE t_city_i_id_city_seq
pg_restore: setting owner and privileges for SEQUENCE OWNED BY t_city_i_id_city_seq
pg_restore: setting owner and privileges for DEFAULT i_id_addr
pg_restore: setting owner and privileges for DEFAULT i_id_city
pg_restore: setting owner and privileges for CONSTRAINT pk_i_id_city
pg_restore: setting owner and privileges for CONSTRAINT pk_id_address
pg_restore: setting owner and privileges for FK CONSTRAINT fk_t_city_i_id_city

\end{verbatim}

The dump file is specified as last parameter. The -d switch tells pg\_restore which database to 
connect for the restore. By default the postgres user usually connects using the ident 
operating system daemon or the trust authentication method, when connected as local. That's the 
reason why in this example there's no need of specifying the username or enter the 
password.\newline 

The second restore's step is the data load. In the example seen in \ref{sec:PLAINFORMAT} we used 
the pg\_dump with --disable-triggers switch in order to avoid failures caused by constraint 
violation. With the custom format the switch is used at restore time.

\begin{verbatim}
postgres@tardis:~/dump$ pg_restore --disable-triggers -v -a -d db_addr_restore_bin db_addr.dmp 
pg_restore: connecting to database for restore
pg_restore: disabling triggers for t_address
pg_restore: processing data for table "t_address"
pg_restore: enabling triggers for t_address
pg_restore: executing SEQUENCE SET t_address_i_id_addr_seq
pg_restore: disabling triggers for t_city
pg_restore: processing data for table "t_city"
pg_restore: enabling triggers for t_city
pg_restore: executing SEQUENCE SET t_city_i_id_city_seq
pg_restore: setting owner and privileges for TABLE DATA t_address
pg_restore: setting owner and privileges for SEQUENCE SET t_address_i_id_addr_seq
pg_restore: setting owner and privileges for TABLE DATA t_city
pg_restore: setting owner and privileges for SEQUENCE SET t_city_i_id_city_seq
 
\end{verbatim}

However, this approach does not prevent the slowness caused by the indices when reloading the data.
If a restore with multiple steps is required (e.g. creating the database schema and check all the 
relations are in place before starting) the section switch is a better choice. Let's see how it 
works with the example seen before.\newline

We'll first restore the pre-data section\footnote{Don't forget to clear the existing objects in 
the database.}.

\begin{verbatim}
postgres@tardis:~/dump$ pg_restore --section=pre-data -v  -d db_addr_restore_bin db_addr.dmp 
pg_restore: connecting to database for restore
pg_restore: creating SCHEMA public
pg_restore: creating COMMENT SCHEMA public
pg_restore: creating EXTENSION plpgsql
pg_restore: creating COMMENT EXTENSION plpgsql
pg_restore: creating TABLE t_address
pg_restore: creating SEQUENCE t_address_i_id_addr_seq
pg_restore: creating SEQUENCE OWNED BY t_address_i_id_addr_seq
pg_restore: creating TABLE t_city
pg_restore: creating SEQUENCE t_city_i_id_city_seq
pg_restore: creating SEQUENCE OWNED BY t_city_i_id_city_seq
pg_restore: creating DEFAULT i_id_addr
pg_restore: creating DEFAULT i_id_city
pg_restore: setting owner and privileges for DATABASE db_addr
pg_restore: setting owner and privileges for SCHEMA public
pg_restore: setting owner and privileges for COMMENT SCHEMA public
pg_restore: setting owner and privileges for ACL public
pg_restore: setting owner and privileges for EXTENSION plpgsql
pg_restore: setting owner and privileges for COMMENT EXTENSION plpgsql
pg_restore: setting owner and privileges for TABLE t_address
pg_restore: setting owner and privileges for SEQUENCE t_address_i_id_addr_seq
pg_restore: setting owner and privileges for SEQUENCE OWNED BY t_address_i_id_addr_seq
pg_restore: setting owner and privileges for TABLE t_city
pg_restore: setting owner and privileges for SEQUENCE t_city_i_id_city_seq
pg_restore: setting owner and privileges for SEQUENCE OWNED BY t_city_i_id_city_seq
pg_restore: setting owner and privileges for DEFAULT i_id_addr
pg_restore: setting owner and privileges for DEFAULT i_id_city
 
\end{verbatim}


Again the pg\_restore loads the objects with the ownership and privileges. What's missing is the 
constraints creation. The second step is the data section's load.

\begin{verbatim}
postgres@tardis:~/dump$ pg_restore --section=data -v  -d db_addr_restore_bin db_addr.dmp 
pg_restore: connecting to database for restore
pg_restore: implied data-only restore
pg_restore: processing data for table "t_address"
pg_restore: executing SEQUENCE SET t_address_i_id_addr_seq
pg_restore: processing data for table "t_city"
pg_restore: executing SEQUENCE SET t_city_i_id_city_seq
pg_restore: setting owner and privileges for TABLE DATA t_address
pg_restore: setting owner and privileges for SEQUENCE SET t_address_i_id_addr_seq
pg_restore: setting owner and privileges for TABLE DATA t_city
pg_restore: setting owner and privileges for SEQUENCE SET t_city_i_id_city_seq

\end{verbatim}

This section simply loads the table's data and sets the sequence values. Apart for the ownership no 
further action is performed. Finally we'll run the post-data section.

\begin{verbatim}
postgres@tardis:~/dump$ pg_restore --section=post-data -v  -d db_addr_restore_bin db_addr.dmp 
pg_restore: connecting to database for restore
pg_restore: creating CONSTRAINT pk_i_id_city
pg_restore: creating CONSTRAINT pk_id_address
pg_restore: creating FK CONSTRAINT fk_t_city_i_id_city
pg_restore: setting owner and privileges for CONSTRAINT pk_i_id_city
pg_restore: setting owner and privileges for CONSTRAINT pk_id_address
pg_restore: setting owner and privileges for FK CONSTRAINT fk_t_city_i_id_city

\end{verbatim}

With this run the constrains (and eventually all the indices) are created in the best 
approach possible when dealing with the bulk data processing. \newline

Loading the data contents without indices maximise the speed. The constraint and index build with 
the data already in place results in a faster build and a fresh index without any bloat.

\section{Restore performances}

When restoring a database, in particular in a disaster recovery scenario, the main goal is 
to have the data back on line as fast as possible. Usually the data section's restore 
is fast. If the dump has been taken using the copy statements, which are enabled by default, 
the reload requires a fraction of the entire restore's time. Taking the advantage of the parallel 
jobs, available for the custom and directory format, it's still possible to improve the data 
section's reload.\newline

The other face of the coin is the post-data section. Because the objects in this section are mostly 
random access operations, the completion can require more time than the data section itself; even 
if the size of the resulting objects is smaller than the table's data. This happens because the 
unavoidable sort operations are CPU and memory bound. The parallel restore gives some advantage, 
but as seen in \ref{sec:PGDUMPBINFMT} each loop's process is single threaded. \newline

Setting up an emergency postgresql.conf file can speed up the restore, reducing the time up to 40\% 
than the production's configuration. What it follows assumes the production's database is lost and 
the data is restored from a custom format's backup. 

\subsection{shared\_buffers}
When reloading the data from the dump, the database performs a so called bulk load operation. The 
PostgreSQL's memory manager have a subroutine which protects the shared segment from the block 
eviction caused by IO intensive operations. It's then very likely the ring buffer strategy will be 
triggered by the restore, sticking the IO in a small 4 MB buffer protecting the rest of the memory. 
A big shared buffer it can cache the data pages when in production but becomes useless when 
restoring. A smaller shared buffer, enough to hold the IO from  the restore processes will result 
in more memory available for the backends when processing the post-data section. There's no fixed 
rule for the sizing. A gross approximation could be 10 MB for each parallel job with a minimum cap 
of 512 MB.   

\subsection{wal\_level}
The wal\_level parameter sets the level of redo informations stored in the WAL segments. By default 
is set to minimal, enabling the xlog skip. Having the database in with a standby, or simply using 
the point in time recovery as alternate backup strategy requires the parameter to be set to archive 
or hot\_standby. If this is the case and you have a PITR or standby to failover, stop reading 
this book and act immediately. Restoring from a physical backup is several time faster than a 
logical restore. If you have lost the standby or PITR snapshot then before starting the reload the 
wal\_level must be set to minimal.

\subsection{fsync}
Turning off fsync can improve massively the restore's speed. Having this parameter turned off
is not safe, unless the cache is have the backup battery to prevent data loss in case of power
failure. However, even without the battery at restore time having the fsync off is not critical.
After all the database is lost, what else can happen?

\subsection{checkpoint\_segments, checkpoint\_timeout}
The checkpoint is a vital event in the database activity. When occurs all the pages not yet written
to the data files are synced to disk. This in the restore context is a disturbance. Increasing the
checkpoint segments and the timeout to the maximum allowed values will avoid any extra IO. In any
case the dirtied blocks will be written on disk when the buffer manager will need to free some
space.


\subsection{autovacuum}
There's no point in having vacuumed the tables after a complete reload. Unfortunately autovacuum
does not know if a table is being restored. When the limit for the updated tuples is recognised the
daemon starts a new process wasting precious CPU cycles. Turning off temporarily the setting will
let the backends to stay focused on the main goal. The data restore.


\subsection{max\_connections}
Limiting the max connections to number of restore jobs is a good idea. It's ok also giving a slight
headroom for one or two connections, just in case there's need to log in and check the database
status. This way the available memory can be shared efficiently between the backends.

\subsection{maintenance\_work\_memory}
This parameter affects the index builds which are stored in the restore's post-data section. Low 
values will results in the backends sorting on disk and slowing down the entire process. Higher
values will keep the index build in memory with great speed gain. The value should be carefully
sized keeping in mind the memory available on the system. This value should be reduced by a 20\% if
the total ram is up to 10 GB and by 10\% if bigger. This reduction is needed to consider the memory
consumed by the operating system and the other processes. From the remaining ram must be subtracted
the shared\_buffer's memory. The remaining value must be divided by the expected backends to
perform the restore. For example if we have a system with 26GB a shared\_buffer of 2 GB and 10
parallel jobs to execute the restore, the maintenance\_work\_mem is  2.14 GB.
\newpage
\begin{verbatim}
26 - 10% =  23.4
23.4 - 2 = 21.4
21.4 / 10 = 2.14
\end{verbatim}

Ignoring this recommendation can trigger the swap usage resulting in a slower restore process.
