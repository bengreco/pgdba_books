\chapter{Managing the instance}
In this chapter we'll look how to initialise a PostgreSQL cluster and the 
management tasks.
Later we'll take a look to the data area structure, the processes and the memory.

\section{Initialising the data directory}
A PostgreSQL instance is composed by a shared memory process and a data area 
managed by the postgres processes.
The data area is initialised by initdb\index{initdb} which requires an empty 
and writable directory to succeed. The binary location depends on the 
installation method, take a look to the chapters \ref{cha:INSTALLSTRUCT} and 
\ref{cha:DB_INSTALL} for further informations.

Initdb accepts various parameters. If not supplied then the probam will try to 
get the informations from the environment variables.\newline

Its basic usage is 

\textit{initdb DATADIRECTORY}

If the variable PGDATA is set then the DATADIRECTORY can be omitted.

\begin{verbatim}
 e.g
 
postgres@tardis:~/tempdata$ export PGDATA=`pwd`
postgres@tardis:~/tempdata$ /usr/lib/postgresql/9.3/bin/initdb 
The files belonging to this database system will be owned by user "postgres".
This user must also own the server process.

The database cluster will be initialized with locale "en_GB.UTF-8".
The default database encoding has accordingly been set to "UTF8".
The default text search configuration will be set to "english".

Data page checksums are disabled.

fixing permissions on existing directory /var/lib/postgresql/tempdata ... ok
creating subdirectories ... ok
selecting default max_connections ... 100
selecting default shared_buffers ... 128MB
creating configuration files ... ok
creating template1 database in /var/lib/postgresql/tempdata/base/1 ... ok
initializing pg_authid ... ok
initializing dependencies ... ok
creating system views ... ok
loading system objects' descriptions ... ok
creating collations ... ok
creating conversions ... ok
creating dictionaries ... ok
setting privileges on built-in objects ... ok
creating information schema ... ok
loading PL/pgSQL server-side language ... ok
vacuuming database template1 ... ok
copying template1 to template0 ... ok
copying template1 to postgres ... ok
syncing data to disk ... ok

WARNING: enabling "trust" authentication for local connections
You can change this by editing pg_hba.conf or using the option -A, or
--auth-local and --auth-host, the next time you run initdb.

Success. You can now start the database server using:

    /usr/lib/postgresql/9.3/bin/postgres -D /var/lib/postgresql/tempdata
or
    /usr/lib/postgresql/9.3/bin/pg_ctl -D /var/lib/postgresql/tempdata -l 
logfile start

 
 
\end{verbatim}

After initialising the data area initdb shows the two methods to start the 
database instance. The first one is useful for debugging and development 
purposes as starts the database directly from the command line without 
daemonising the process.

\begin{verbatim}
postgres@tardis:~/tempdata$ /usr/lib/postgresql/9.3/bin/postgres -D 
/var/lib/postgresql/tempdata
LOG:  database system was shut down at 2014-03-23 18:52:07 UTC
LOG:  database system is ready to accept connections
LOG:  autovacuum launcher started

\end{verbatim}

To stop it simply press ctrl+c

The second method is more interesting as the PostgreSQL process is managed as 
daemon via pg\_ctl.

The option \textit{-l logfile} is to enable the logfile redirection. Without 
this option the server's output will appear on the standard output.

\begin{verbatim}
without -l

postgres@tardis:~/tempdata$ /usr/lib/postgresql/9.3/bin/pg_ctl -D 
/var/lib/postgresql/tempdata  start
server starting
postgres@tardis:~/tempdata$ LOG:  database system was shut down at 2014-03-23 
19:00:36 UTC
LOG:  database system is ready to accept connections
LOG:  autovacuum launcher started

with -l 

postgres@tardis:~/tempdata$ /usr/lib/postgresql/9.3/bin/pg_ctl -D 
/var/lib/postgresql/tempdata -l logfile start
server starting

postgres@tardis:~/tempdata$ tail logfile 
LOG:  database system was shut down at 2014-03-23 19:01:19 UTC
LOG:  database system is ready to accept connections
LOG:  autovacuum launcher started


\end{verbatim}

As seen in the  subsection \ref{sub:PGCTL} pg\_ctl accepts the command to 
perform on the cluster.

In order to stop the running instance simply run
\begin{verbatim}
postgres@tardis:~$ /usr/lib/postgresql/9.3/bin/pg_ctl -D 
/var/lib/postgresql/tempdata -l logfile stop
waiting for server to shut down.... done
server stopped

\end{verbatim}

\section{The startup sequence}
\label{sec:STARTUP}

When the server process is started allocates the shared segment in memory. In
the versions before the 9.3 this was the a potential point of failure because,
if the shared\_buffers was bigger than the kernel's max allowed shared memory
segment the startup will fail with this sort of error

\begin{verbatim}
FATAL: could not create shared memory segment: Cannot allocate memory

DETAIL: Failed system call was shmget(key=X, size=XXXXXX, XXXXX).

HINT: This error usually means that PostgreSQL's request for a shared memory
segment exceeded available memory or swap space, or exceeded your kernel's
SHMALL parameter. You can either reduce the request size or reconfigure the
kernel with larger SHMALL. To reduce the request size (currently XXXXX bytes),
reduce PostgreSQL's shared memory usage, perhaps by reducing shared_buffers or
max_connections.
\end{verbatim}

In this case the kernel parameters needs adjustment.
As from my Oracle experience I take no chance and I do use the values suggested
for an Oracle installation which incidentally works perfectly for PostgreSQL.

\begin{verbatim}
kernel.shmall = 2621440
kernel.shmmax = 18253611008
kernel.shmmni = 4096
kernel.sem = 250 32000 100 128
fs.file-max = 658576
\end{verbatim}

Put those values into the file /etc/sysctl.conf and, as root, run 
\textit{sysctl -p}.

The latest major version PostgreSQL switched from the SysV to Posix shared
memory and mmap. This solved completely the shared memory tuning on Linux.

When the memory is allocated the postmaster reads the pg\_control
file to check if the instance requires recovery.

The pg\_control contains references to the last checkpoint and the last known
status for the instance. 

If  the instance is in dirty state, because a crash or an unclean shutdown, the
startup replays the blocks from the WAL segments in the pg\_xlog directory
starting from the last checkpoint position read from the pg\_control file.

Any corruption in the wal files or, even worse, the pg\_control file results in
a not startable instance.

After the recovery is complete or the cluster is in clean state, then  the
startup completes opening the database in production state. 


\section{The shutdown sequence} 
\label{sec:SHUTDOWN_SEQ}
\index{shutdown sequence}
For those coming from Oracle, the PostgreSQL's shutdown sequence will sound 
familiar with few, but very important, exceptions.

A PostgreSQL process enters the shutdown status when receive a specific OS 
signal. 

This can happen using the os kill or via pg\_ctl. 

As seen in the section \ref{sub:PGCTL} the latter accepts the -m switch to 
specify the shutdown mode. If not specified defaults to the mode smart which 
sends the signal SIGTERM to the running PostgreSQL process. This way the 
database will not accept new connections and will wait for the existing 
connections to exit before the shutdown. 

The most useful PostgreSQL shutdown mode is the fast mode. This way the SIGQUIT 
is sent to the postgresql process and the database will disconnect all the 
sessions rolling back the open transactions and then will enter the shutdown 
sequence. 

Either modes, smart and fast, make the database shutdown clean, leaving the 
cluster in consistent state when the postgres process exits. 

As soon as the postgres process enters the shutdown sequence will issue a last 
checkpoint consolidating the updated physical blocks in the shared buffers to 
the data files.

As soon as the checkpoint is complete and the last consistent position is 
logged on the pg\_control file, the database main process exits the accessory 
processes like the walwriter or the checkpointer and terminates removing the 
pid file.

The last checkpoint can slow down the entire sequence because is commonly 
spread through time to avoid any disk IO activity spike. If the shared\_buffer 
is big and contains many dirty blocks, the checkpoint can run for a very long 
time. In addition, if at the shutdown time, another checkpoint is running the 
database will wait the completion before starting the final checkpoint. This 
meanwhile the new connections are forbidden, whatever shutdown mode you decided 
to use. 

In order to have an idea  of what's happening on the running cluster, is a good 
practice to change the GUC log\_checkpoints = off to log\_checkpoints = on.


For more informations about the database processes take a look to the section 
\ref{sec:PROCESSES}.

If the cluster is taking too long to stop, as last resort is possible to 
use the immediate mode. This will send a SIGQUIT to the running process causing 
the immediate exit of the main process and all the sessions. 

The shutdown in this case is not clean and the subsequent start will perform a 
crash recovery starting from the last consistent state read from the 
pg\_control file. The process is usually harmless with one important exception.

If the cluster have unlogged tables those relations are recreated from scratch 
when the recovery happens and all the data in those table \textbf{is lost}.

This is the main reason I suggest to avoid the pg\_ctlcluster shipped with 
debian to stop the cluster. The program doesn't offer any control on the 
shutdown mode and can result in disastrous data loss. For more details take a 
look to the subsection \ref{sub:PGCTLDEB}.

A last word about the SIGKILL signal. It can happen the cluster refuse to stop 
even using the immediate mode. In this case, as last resort the SIGKILL or kill 
-9 can be used. The online manual is very clear on this point. As the SIGKILL 
cannot be trapped and evicts from the ram the process with the shared memory, 
its use will results in not freeing the resources used by the killed process.

This will very likely affect the start of a fresh instance. Please refer to 
your sysadmin to find out the best way to perform a memory and semaphore 
cleanup before starting PostgreSQL after a SIGKILL. 


\section{The processes}
\label{sec:PROCESSES}
Since the early version 7.4, when the only process running the cluster were the 
old loved postmaster, PostgreSQL has enriched with new dedicated processes, 
becoming more complex but even more efficient.

With a running cluster there are at least six postgres processes, the one 
without colon in the process name is is the main database's process, started 
as seen in the section \ref{sec:STARTUP}.


\subsection{postgres: checkpointer process}
As the name suggest this process take care of the cluster's 
checkpoint\index{checkpoint}. The checkpoint is an important event in the 
database activity. When a checkpoint starts all the dirty 
pages in memory are written to the data files. 
The checkpoint by the time and the number of cluster's WAL switches.
To adjust the checkpoin's frequency the GUC parameters checkpoint\_timeout and 
checkpoint\_segments are used. A third parameter, 
checkpoint\_completion\_target is used to spread the checkpoint over a 
percentage of the checkpoint\_timeout, in order to avoid a massive disk IO 
spike.

\subsection{postgres: writer process}
To ease down the checkpoint activity the background writer scans the shared 
buffer for dirty pages to write down to the disk. The process is designed to 
have a minimal impact on the database activity. It's possible to tune the rounds
length and delay using the GUC parameters bgwriter\_delay, time between two 
rounds, and bgwriter\_lru\_maxpages, the number of buffers after the writer's 
sleep.


\subsection{postgres: wal writer process}
This background process has been introduced recently to have a more efficient 
wal writing. The process works in rounds were write down the wal buffers to the 
wal files. The GUC parameter wal\_writer\_delay\index{wal\_writer\_delay} sets 
the milliseconds to sleep between the rounds. 

\subsection{postgres: autovacuum launcher process}
This process is present if the GUC parameter autovacuum is set to on.
It's scope is to launch the autovacuum backends at need.
Anyway autovacuum can run even if autovacuum is turned of, when there's risk 
of the XID wraparound failure\index{XID wraparound failure}.

\subsection{postgres: stats collector process}
The process gathers the database's usage statistics for human usage and stores 
the informations into the location indicated by the GUC stats\_temp\_directory, 
by default pg\_stat\_temp. Those statistics are useful to understand 
how the database is performing, from pyshical and logical point of view.

\subsection{postgres: postgres postgres [local] idle}
This kind of process is the database backend, one for each established 
connection. The values after the colon square brackets show useful 
informations like the connected database, the username, the host and the 
executing query. The same informations are stored into the pg\_stat\_activity 
table.


\section{The memory}
\label{sec:MEMORY}
The PostgreSQL's memory structure is not complex like other databases.
In this section we'll take a to the various parts. 

\subsection{The shared buffer}
\index{shared buffer}
The shared buffer, as the name suggests is the segment of shared memory used by 
PostgreSQL to manage the data pages. 

Its size is set using the GUC\footnote{GUC, Grand Unified Configuration, this 
acronym refers to the parameters used to configure the instance} parameter 
shared\_buffers and is allocated 
during the startup process.Any change requires the instance restart.

The segment is formatted in blocks with the same size of the data file's 
blocks, usually 8192 bytes. Each backend connected to the cluster is attached 
to this segment. Because usually its size is a fraction of the cluster's size, 
a simple but very efficient mechanism keeps in memory the blocks using a 
combination of LRU and MRU.

Since the the version 8.3 is present a protection mechanism to avoid the 
massive block eviction when intensive IO operations, like vacuum or big 
sequential reads, happens.

Each database operation, read or write, is performed moving the blocks via the 
shared buffer. This ensure an effective caching process and the memory routines 
guarantee the consistent read and write at any time.

PostgreSQL, in order to protect the shared buffer from potential corruption, if 
any unclean disconnection happens, resets by default all the connections. 

This behaviour can be disabled in the configuration file but exposes the shared 
buffer to data corruption if the unclean disconnections are not correctly 
managed.



\subsection{The work memory}\index{work memory}
\label{sub:WORKMEM}
This memory segment is allocated per user and its default value is set using 
the GUC parameter work\_mem. The value can be altered for the session on the 
fly. When changed in the global configuration file becomes effective to the 
next transaction after the instance reload. 

This segment is used mainly for expensive operations like the sort or the 
hash.

If the operation's memory usage exceeds the work\_mem value then the PostgreSQL 
switches to a disk sort/hash. 

Increasing the work\_mem value results generally in better performances for 
sort/hash operations. 

Because is a per user memory segment, the potential amount of memory 
required in a running instance is max\_connections * work\_mem. It's very 
important to set this value to a reasonable size in order to avoid any risk of 
out of memory error or unwanted swap.

In complex queries is likely to have many sort or hash operations in parallel 
and each one consumes the amount of work\_mem for the 
session.


\subsection{The maintenance work memory}\index{maintenance work memory}
The maintenance work memory is set using the GUC parameter 
maintenance\_work\_mem and follow the same rules of work\_mem. This memory 
segment is allocated per user and is used for the maintenance operations 
like VACUUM or REINDEX. As usually this kind of operations happens on one 
relation at time, this parameter can be safely set to a bigger value than 
work\_mem.

\subsection{The temporary memory}
\label{sub:TEMPBUF}
The temporary memory is set using the GUC parameter temp\_buffers. This is the 
amount of memory per user for the temporary table creation before the disk is 
used. Same as for the work memory and the maintenance work memory it's possible 
to change the value for the current session but only before any temporary table 
creation. After this the parameter cannot be changed anymore.


\section{The data area}
\label{sec:PGDATA}\index{data area}
As seen before the data storage area is initialized by initdb \index{initdb}.
Its structure didn't change too much from the old fashioned 7.4.
In this section we'll take a look to the various subdirectories and how their 
usage can affect the performances. 


\subsection{base}\index{data area,base}
\label{sub:BASE}
As the name suggests, the base directory contains the database files. Each 
database have a dedicated subdirectory, named after the internal database's 
object id.
A freshly initialised data directory shows only three 
subdirectories in the base folder.

Those corresponds to the two template databases,template0 and template1, plus 
the postgres database. Take a look to chapter \ref{cha:LOGICLAY} for more 
informations.

The numerical directories contains various files, also with the numerical name 
which are actualy the database's relations, tables and indices. 

The relation's name is set initially from the relation's object id. Any file 
altering operation like VACUUM FULL or REINDEX, will generate a new file 
with a different name. To find out the real relation's file name the 
relfilenode inside the pg\_class system table  must be queried.

\subsection{global}\index{data area,global}
The global directory contains all the cluster wide relations.
In addition there's the very critical  control file mentioned in 
\ref{sub:PGCONTROLDATA} \index{control file}.

This small file is big exactly one database block, usually 8192 bytes, and 
contains critical informations for the cluster. 
With a corrupted control file the instance cannot start. 
The control file is written usually when a checkpoint occurs.

\subsection{pg\_xlog}\index{data area,pg\_xlog}
This is the most important and critical directory, for the performances and for 
the reliability. 

The directory contains the transaction's logs, \index{wal files} named wal 
file. Each file is usually 16 Mb and contains all the data blocks changed during 
the database activity. 
The blocks are written first on this not volatile area to ensure the cluster's 
recovery in case of cras. The data blocks are then written later to the 
corresponding data files. If the cluster's shutdown is not clean then the wal 
files are replayed during the startup process from the last known consistent 
location read from control file.

In order to ensure good performance this location should stay on a dedicated 
device. 

\subsection{pg\_clog}\index{data area, pg\_clog}
This directory contains the committed transactions in small 8k files, except 
for the serializable transactions.
The the files are managed by the cluster and the amount is related with 
the two GUC parameters autovacuum\_freeze\_max\_age and 
vacuum\_freeze\_table\_age.
Increasing the values for the two parameters the pg\_clog must store the commit 
status to the ``event horizon'' of the oldest frozen transaction id. 
More informations about vacuum and the maintenance are in 
the chapter \ref{cha:MAINTENANCE}.

\subsection{pg\_serial}\index{data area, pg\_serial}
Same as pg\_clog this directory stores the informations about the commited 
transactions in serializable transaction isolation level.

\subsection{pg\_multixact}\index{data area, pg\_multixact}
Stores the informations about the multi transaction status, used generally for  
the row share locks.

\subsection{pg\_notify}\index{data area, pg\_notify}
Stores informations about the LISTEN/NOTIFY operations.

\subsection{pg\_snapshots}\index{data area, pg\_snapshots}
This directory is used to store the exported snapshots. From the version 9.2 
PostgreSQL offers the transaction's snapshot export where one session can 
open a transaction and export a consistent snapshot. This way different session 
can access the snapshot and read all togheter the same consistent data 
snapshot. This feature is used, for example, by pg\_dump for the parallel 
export.

\subsection{pg\_stat}\index{data area, pg\_stat}
This directory contains the permanent files for the statistic subsystem. 

\subsection{pg\_stat\_tmp}\index{data area, pg\_stat\_tmp}
This directory contains the temporary files for the statistic subsystem. 
As this directory is constantly written, is very likely to become an 
IO bottleneck. Setting the GUC parameter stats\_temp\_directory to a ramdisk 
speeds can improve the database performances.


\subsection{pg\_subtrans}\index{data area, pg\_subtrans}
Stores the subtransactions status data. 

\subsection{pg\_twophase}\index{data area, pg\_twophase}
Stores the two phase commit data. The two phase commit allows the transaction 
opening independently from the session. This way even a different session can 
commit or rollback the transaction later.

\subsection{pg\_tblspc}\index{data area, pg\_tblspc}
\label{sub:TABLESPACE}
The directory contains the symbolic links to the tablespace locations.
A tablespace is a logical name pointing a physical location. As from PostgreSQL 
9.2 the location is read directly from the symbolic link. This make possible 
to change the tablespace's position simply stopping the cluster, moving the 
data files in the new location, creating the new symlink and starting the 
cluster.
More informations about the tablespace management in the 
chapter \ref{cha:LOGICLAY}. 


