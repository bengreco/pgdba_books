\chapter{Managing the cluster}
\label{cha:MANAGING}
A PostgreSQL cluster is made of two components. A physical location initialised as data area 
and the postgres process attached to a shared memory segment, the shared buffer. The debian's 
package's installation, automatically set up a fully functional PostgreSQL cluster in the directory 
/var/lib/postgresql. This is good because it's possible to explore the product immediately.
However, 
it's not uncommon to find clusters used in production with the minimal default configuration's 
values, just because the binary installation does not make it clear what happens \textit{under the 
bonnet}.

This chapter will explain how a PostgreSQL cluster works and how critical is its 
management. 

\section{Initialising the data directory}

The data area is initialised by initdb\index{initdb}. The program requires an empty directory to 
write into to successful complete. Where the initdb binary is located depends from the installation 
method. We already discussed of this in \ref{cha:INSTALLSTRUCT} and \ref{cha:DB_INSTALL}. 

The accepted parameters for customising cluster's data area are various. Anyway, running 
initdb without parameters will make the program to use the value stored into the environment 
variable PGDATA. If the variable is unset the program will exit without any further action.\newline

For example, using the initdb shipped with the debian archive requires the following commands.

\begin{verbatim}
postgres@tardis:~/$ mkdir tempdata
postgres@tardis:~/$ cd tempdata
postgres@tardis:~/tempdata$ export PGDATA=`pwd`
postgres@tardis:~/tempdata$ /usr/lib/postgresql/9.3/bin/initdb 
The files belonging to this database system will be owned by user "postgres".
This user must also own the server process.

The database cluster will be initialized with locale "en_GB.UTF-8".
The default database encoding has accordingly been set to "UTF8".
The default text search configuration will be set to "english".

Data page checksums are disabled.

fixing permissions on existing directory /var/lib/postgresql/tempdata ... ok
creating subdirectories ... ok
selecting default max_connections ... 100
selecting default shared_buffers ... 128MB
creating configuration files ... ok
creating template1 database in /var/lib/postgresql/tempdata/base/1 ... ok
initializing pg_authid ... ok
initializing dependencies ... ok
creating system views ... ok
loading system objects' descriptions ... ok
creating collations ... ok
creating conversions ... ok
creating dictionaries ... ok
setting privileges on built-in objects ... ok
creating information schema ... ok
loading PL/pgSQL server-side language ... ok
vacuuming database template1 ... ok
copying template1 to template0 ... ok
copying template1 to postgres ... ok
syncing data to disk ... ok

WARNING: enabling "trust" authentication for local connections
You can change this by editing pg_hba.conf or using the option -A, or
--auth-local and --auth-host, the next time you run initdb.

Success. You can now start the database server using:

    /usr/lib/postgresql/9.3/bin/postgres -D /var/lib/postgresql/tempdata
or
    /usr/lib/postgresql/9.3/bin/pg_ctl -D /var/lib/postgresql/tempdata -l 
logfile start

\end{verbatim}

PostgreSQL 9.3 introduces\index{checksums, data page}\index{data page checksums} the data page 
checksums used for detecting the data page corruption. This great feature can be enabled only when 
initialising the data area with initdb and is cluster wide. The extra overhead caused by the 
checksums is something to consider because the only way to disable the data checksums is a dump 
and reload on a fresh data area.\newline

After initialising the data directory initdb emits the message with the commands to start the 
database cluster. The first form is useful for debugging and development purposes because it starts 
the database directly from the command line with the output displayed on the terminal. 

\begin{verbatim}
postgres@tardis:~/tempdata$ /usr/lib/postgresql/9.3/bin/postgres -D 
/var/lib/postgresql/tempdata
LOG:  database system was shut down at 2014-03-23 18:52:07 UTC
LOG:  database system is ready to accept connections
LOG:  autovacuum launcher started

\end{verbatim}

Pressing CTRL+C stops the cluster with a fast shutdown.\newline

Another reason for running postgres directly is when it needs to be started in single user mode. 
The --single option is a lifesaver if the cluster refuses to start because one or more 
databases are near the XID wraparound failure. 
\begin{verbatim}

postgres@tardis:~/tempdata$ /usr/lib/postgresql/9.3/bin/postgres --single -D /home/postgres/tempdata

PostgreSQL stand-alone backend 9.3.5
backend> 

\end{verbatim}

The database interface in single user mode and does not have all the sophisticated features 
like the client psql. Anywat with a little knowledge of SQL it's possible to find the database(s) 
causing the shutdown and fix it.
\index{postgres, single user mode}\index{XID wraparound failure, fix}

\begin{verbatim}
backend> SELECT datname,age(datfrozenxid) FROM pg_database ORDER BY 2 DESC;
         1: datname     (typeid = 19, len = 64, typmod = -1, byval = f)
         2: age (typeid = 23, len = 4, typmod = -1, byval = t)
        ----
         1: datname = "template1"       (typeid = 19, len = 64, typmod = -1, byval = f)
         2: age = "2146435072"  (typeid = 23, len = 4, typmod = -1, byval = t)
        ----
         1: datname = "template0"       (typeid = 19, len = 64, typmod = -1, byval = f)
         2: age = "10"  (typeid = 23, len = 4, typmod = -1, byval = t)
        ----
         1: datname = "postgres"        (typeid = 19, len = 64, typmod = -1, byval = f)
         2: age = "10"  (typeid = 23, len = 4, typmod = -1, byval = t)
        ----

\end{verbatim}

The age function shows how old is the last XID not yet frozen. In our example the template1
database have an age of 2146435072, one million transactions to the wraparound. We can then exit 
the backend with CTRL+D and restart it again in the in single user mode specifying the database 
name. A VACUUM will get rid of the problematic xid.

\begin{verbatim}
postgres@tardis:~/tempdata$ /usr/lib/postgresql/9.3/bin/postgres --single \
-D /home/postgres/tempdata template1

backend> VACUUM;
\end{verbatim}

This procedure must be repeated for any database with very old XID.\newline

Starting the cluster with pg\_ctl usage is very simple. This program also accepts the data area as 
parameter or using the environment variable PGDATA. It's also required to provide the command to 
execute. The start command for example is used to start the cluster in multi user mode.

\begin{verbatim}

postgres@tardis:~/tempdata$ /usr/lib/postgresql/9.3/bin/pg_ctl -D 
/var/lib/postgresql/tempdata -l logfile start
server starting

postgres@tardis:~/tempdata$ tail logfile 
LOG:  database system was shut down at 2014-03-23 19:01:19 UTC
LOG:  database system is ready to accept connections
LOG:  autovacuum launcher started

\end{verbatim}
Omitting the logfile with the -l will display the alerts and warnings on the terminal.

The command stop will end the cluster's activity.

\begin{verbatim}
postgres@tardis:~$ /usr/lib/postgresql/9.3/bin/pg_ctl -D 
/var/lib/postgresql/tempdata -l logfile stop
waiting for server to shut down.... done
server stopped
\end{verbatim}

\section{The startup sequence}
\label{sec:STARTUP}

When the postgres process starts it allocates the shared memory segment called shared buffer. The
size of this segment is specified with the GUC parameter shared\_buffers.The version 9.3, on the
unix systems, changed the memory allocation method to mmap(). This eliminated any need to adjust
the kernel's parameters.

For the versions up to the 9.2, if the requested memory is bigger than the kernel's maximum
allowed size, the startup sequence will abort with an error like this.

\begin{verbatim}
FATAL: could not create shared memory segment: Cannot allocate memory

DETAIL: Failed system call was shmget(key=X, size=XXXXXX, XXXXX).

HINT: This error usually means that PostgreSQL's request for a shared memory
segment exceeded available memory or swap space, or exceeded your kernel's
SHMALL parameter. You can either reduce the request size or reconfigure the
kernel with larger SHMALL. To reduce the request size (currently XXXXX bytes),
reduce PostgreSQL's shared memory usage, perhaps by reducing shared_buffers or
max_connections.
\end{verbatim}

\index{kernel resources}
The kernel's parameters governing this limit is SHMMAX. It sets the maximum allowed size of a shared
memory segment. The value is measured in bytes and must be big enogh to contain the
requested shared\_buffers. Another parameter which needs adjustment is SHMALL. This value sets the
amount of total shared memory available. On linux is usually measured in pages. Unless the
kernel is configured to allow the huge pages the page size is 4096 byes. The value should be the
same as SHMMAX. \newline

If we want to set a pre 9.3 with a shared buffer to 1 GB the SHMMAX should be at least 1073741824.
The value 1258291200 (1200 MB) is a reasonable setting ang gives us some extra headroom. The
corresponding SHMALL is 307200. The value SHMMNI is the minimum value of the shared memory, is safe
to set to 4096, just one memory page. 

The settings can be changed on the fly, simply echoing in the corresponding proc entries or setting
the values persistently into the file /etc/sysctl.conf.

Here's the file from the previous example.
\begin{verbatim}
kernel.shmmax = 1258291200
kernel.shmall = 307200
kernel.shmmni = 4096
kernel.sem = 250 32000 100 128
fs.file-max = 658576
\end{verbatim}

To apply the changes login as root and run \textit{sysctl -p}.\newline


When the memory is allocated the postmaster reads the pg\_control
file to check if the instance requires recovery. The pg\_control file is used to store the locations
to the last checkpoint and the last known status for the instance.\newline

If  the instance is in dirty state, because a crash or an unclean shutdown, the startup
process reads the last checkpoint location and replays the blocks from the corresponding WAL
segment in the pg\_xlog directory. Any corruption in the wal files during the recovery or the
pg\_control file results in a not startable instance.\newline

When the recovery is complete or if the cluster's state is clean the postgres process completes the
startup and sets the cluster in production state. 


\section{The shutdown sequence} 
\label{sec:SHUTDOWN_SEQ}
\index{shutdown sequence}

The PostgreSQL process enters the shutdown status when receive a specific OS 
signal. The signal can be sent via the os kill or using the program pg\_ctl. \newline

As seen in \ref{sub:PGCTL} pg\_ctl for the stop action accepts the -m switch used to specify the
shutdown mode. If  omitted the default mode is smart, corresponding to the SIGTERM signal. With
the smart shuthdown the cluster stops accepting new connections waiting for the established
connections to quit. 

When the shutdown mode is set to fast the pg\_ctl program sends the SIGQUIT signal to the running
process. Again the cluster will stop accepting new connections and the existing backends
are terminated. Any open transaction is rolled back. \newline

Either the smart and the fast fast shutdown result in a clean cluster's status. When the postgres
process starts the final shutdown starts a final checkpoint which consolidates any dirty block on
disk. The latest checkpoint's location is the written to the pg\_control file alongside with the
clean status. The postgres process finally exits and the pid file is removed.\newline

The final checkpoint can slow down the entire shutdown sequence because is commonly spread through
time to avoid any disk IO activity spike. If the shared\_buffer is big and contains many dirty
blocks, the checkpoint can run for a very long time. In addition, if at the shutdown time, another
checkpoint is running the postgres process will start the final checkpoint only after the
running checkpoint is complete. During this time nothing is connected to the cluster and the new
connections are forbidden.\newline

Enabling the log checkpoints in the configuration some visibility on what the cluster is
actually doing. The GUC parameter governing the setting is log\_checkpoints.\newline


If the cluster is taking too long to stop, as last resort is possible to 
use the immediate mode. This will send a SIGQUIT to the running process causing 
the immediate exit of the main process and all the sessions. 

The shutdown in this case is not clean and the subsequent start will perform a 
crash recovery starting from the last consistent state read from the 
pg\_control file. The process is usually harmless with one important exception.

If the cluster have unlogged tables those relations are recreated from scratch 
when the recovery happens and all the data in those table \textbf{is lost}.

This is the main reason I suggest to avoid the pg\_ctlcluster shipped with 
debian to stop the cluster. The program doesn't offer any control on the 
shutdown mode and can result in disastrous data loss. For more details take a 
look to the subsection \ref{sub:PGCTLDEB}.

A last word about the SIGKILL signal. It can happen the cluster refuse to stop 
even using the immediate mode. In this case, as last resort the SIGKILL or kill 
-9 can be used. The online manual is very clear on this point. As the SIGKILL 
cannot be trapped and evicts from the ram the process with the shared memory, 
its use will results in not freeing the resources used by the killed process.

This will very likely affect the start of a fresh instance. Please refer to 
your sysadmin to find out the best way to perform a memory and semaphore 
cleanup before starting PostgreSQL after a SIGKILL. 


\section{The processes}
\label{sec:PROCESSES}
Since the early version 7.4, when the only process running the cluster were the 
old loved postmaster, PostgreSQL has enriched with new dedicated processes, 
becoming more complex but even more efficient.

With a running cluster there are at least six postgres processes, the one 
without colon in the process name is is the main database's process, started 
as seen in the section \ref{sec:STARTUP}.


\subsection{postgres: checkpointer process}
As the name suggest this process take care of the cluster's 
checkpoint\index{checkpoint}. The checkpoint is an important event in the 
database activity. When a checkpoint starts all the dirty 
pages in memory are written to the data files. 
The checkpoint by the time and the number of cluster's WAL switches.
To adjust the checkpoin's frequency the GUC parameters checkpoint\_timeout and 
checkpoint\_segments are used. A third parameter, 
checkpoint\_completion\_target is used to spread the checkpoint over a 
percentage of the checkpoint\_timeout, in order to avoid a massive disk IO 
spike.

\subsection{postgres: writer process}
To ease down the checkpoint activity the background writer scans the shared 
buffer for dirty pages to write down to the disk. The process is designed to 
have a minimal impact on the database activity. It's possible to tune the rounds
length and delay using the GUC parameters bgwriter\_delay, time between two 
rounds, and bgwriter\_lru\_maxpages, the number of buffers after the writer's 
sleep.


\subsection{postgres: wal writer process}
This background process has been introduced recently to have a more efficient 
wal writing. The process works in rounds were write down the wal buffers to the 
wal files. The GUC parameter wal\_writer\_delay\index{wal\_writer\_delay} sets 
the milliseconds to sleep between the rounds. 

\subsection{postgres: autovacuum launcher process}
This process is present if the GUC parameter autovacuum is set to on.
It's scope is to launch the autovacuum backends at need.
Anyway autovacuum can run even if autovacuum is turned of, when there's risk 
of the XID wraparound failure\index{XID wraparound failure}.

\subsection{postgres: stats collector process}
The process gathers the database's usage statistics for human usage and stores 
the informations into the location indicated by the GUC stats\_temp\_directory, 
by default pg\_stat\_temp. Those statistics are useful to understand 
how the database is performing, from pyshical and logical point of view.

\subsection{postgres: postgres postgres [local] idle}
This kind of process is the database backend, one for each established 
connection. The values after the colon square brackets show useful 
informations like the connected database, the username, the host and the 
executing query. The same informations are stored into the pg\_stat\_activity 
table.


\section{The memory}
\label{sec:MEMORY}
The PostgreSQL's memory structure is not complex like other databases.
In this section we'll take a to the various parts. 

\subsection{The shared buffer}
\index{shared buffer}
The shared buffer, as the name suggests is the segment of shared memory used by 
PostgreSQL to manage the data pages. 

Its size is set using the GUC\footnote{GUC, Grand Unified Configuration, this 
acronym refers to the parameters used to configure the instance} parameter 
shared\_buffers and is allocated 
during the startup process.Any change requires the instance restart.

The segment is formatted in blocks with the same size of the data file's 
blocks, usually 8192 bytes. Each backend connected to the cluster is attached 
to this segment. Because usually its size is a fraction of the cluster's size, 
a simple but very efficient mechanism keeps in memory the blocks using a 
combination of LRU and MRU.

Since the the version 8.3 is present a protection mechanism to avoid the 
massive block eviction when intensive IO operations, like vacuum or big 
sequential reads, happens.

Each database operation, read or write, is performed moving the blocks via the 
shared buffer. This ensure an effective caching process and the memory routines 
guarantee the consistent read and write at any time.

PostgreSQL, in order to protect the shared buffer from potential corruption, if 
any unclean disconnection happens, resets by default all the connections. 

This behaviour can be disabled in the configuration file but exposes the shared 
buffer to data corruption if the unclean disconnections are not correctly 
managed.



\subsection{The work memory}\index{work memory}
\label{sub:WORKMEM}
This memory segment is allocated per user and its default value is set using 
the GUC parameter work\_mem. The value can be altered for the session on the 
fly. When changed in the global configuration file becomes effective to the 
next transaction after the instance reload. 

This segment is used mainly for expensive operations like the sort or the 
hash.

If the operation's memory usage exceeds the work\_mem value then the PostgreSQL 
switches to a disk sort/hash. 

Increasing the work\_mem value results generally in better performances for 
sort/hash operations. 

Because is a per user memory segment, the potential amount of memory 
required in a running instance is max\_connections * work\_mem. It's very 
important to set this value to a reasonable size in order to avoid any risk of 
out of memory error or unwanted swap.

In complex queries is likely to have many sort or hash operations in parallel 
and each one consumes the amount of work\_mem for the 
session.


\subsection{The maintenance work memory}\index{maintenance work memory}
The maintenance work memory is set using the GUC parameter 
maintenance\_work\_mem and follow the same rules of work\_mem. This memory 
segment is allocated per user and is used for the maintenance operations 
like VACUUM or REINDEX. As usually this kind of operations happens on one 
relation at time, this parameter can be safely set to a bigger value than 
work\_mem.

\subsection{The temporary memory}
\label{sub:TEMPBUF}
The temporary memory is set using the GUC parameter temp\_buffers. This is the 
amount of memory per user for the temporary table creation before the disk is 
used. Same as for the work memory and the maintenance work memory it's possible 
to change the value for the current session but only before any temporary table 
creation. After this the parameter cannot be changed anymore.


\section{The data area}
\label{sec:PGDATA}\index{data area}
As seen before the data storage area is initialized by initdb \index{initdb}.
Its structure didn't change too much from the old fashioned 7.4.
In this section we'll take a look to the various subdirectories and how their 
usage can affect the performances. 


\subsection{base}\index{data area,base}
\label{sub:BASE}
As the name suggests, the base directory contains the database files. Each 
database have a dedicated subdirectory, named after the internal database's 
object id.
A freshly initialised data directory shows only three 
subdirectories in the base folder.

Those corresponds to the two template databases,template0 and template1, plus 
the postgres database. Take a look to chapter \ref{cha:LOGICLAY} for more 
informations.

The numerical directories contains various files, also with the numerical name 
which are actualy the database's relations, tables and indices. 

The relation's name is set initially from the relation's object id. Any file 
altering operation like VACUUM FULL or REINDEX, will generate a new file 
with a different name. To find out the real relation's file name the 
relfilenode inside the pg\_class system table  must be queried.

\subsection{global}\index{data area,global}
The global directory contains all the cluster wide relations.
In addition there's the very critical  control file mentioned in 
\ref{sub:PGCONTROLDATA} \index{control file}.

This small file is big exactly one database block, usually 8192 bytes, and 
contains critical informations for the cluster. 
With a corrupted control file the instance cannot start. 
The control file is written usually when a checkpoint occurs.

\subsection{pg\_xlog}\index{data area,pg\_xlog}
This is the most important and critical directory, for the performances and for 
the reliability. 

The directory contains the transaction's logs, \index{wal files} named wal 
file. Each file is usually 16 Mb and contains all the data blocks changed during 
the database activity. 
The blocks are written first on this not volatile area to ensure the cluster's 
recovery in case of cras. The data blocks are then written later to the 
corresponding data files. If the cluster's shutdown is not clean then the wal 
files are replayed during the startup process from the last known consistent 
location read from control file.

In order to ensure good performance this location should stay on a dedicated 
device. 

\subsection{pg\_clog}\index{data area, pg\_clog}
This directory contains the committed transactions in small 8k files, except 
for the serializable transactions.
The the files are managed by the cluster and the amount is related with 
the two GUC parameters autovacuum\_freeze\_max\_age and 
vacuum\_freeze\_table\_age.
Increasing the values for the two parameters the pg\_clog must store the commit 
status to the ``event horizon'' of the oldest frozen transaction id. 
More informations about vacuum and the maintenance are in 
the chapter \ref{cha:MAINTENANCE}.

\subsection{pg\_serial}\index{data area, pg\_serial}
Same as pg\_clog this directory stores the informations about the commited 
transactions in serializable transaction isolation level.

\subsection{pg\_multixact}\index{data area, pg\_multixact}
Stores the informations about the multi transaction status, used generally for  
the row share locks.

\subsection{pg\_notify}\index{data area, pg\_notify}
Stores informations about the LISTEN/NOTIFY operations.

\subsection{pg\_snapshots}\index{data area, pg\_snapshots}
This directory is used to store the exported snapshots. From the version 9.2 
PostgreSQL offers the transaction's snapshot export where one session can 
open a transaction and export a consistent snapshot. This way different session 
can access the snapshot and read all togheter the same consistent data 
snapshot. This feature is used, for example, by pg\_dump for the parallel 
export.

\subsection{pg\_stat}\index{data area, pg\_stat}
This directory contains the permanent files for the statistic subsystem. 

\subsection{pg\_stat\_tmp}\index{data area, pg\_stat\_tmp}
This directory contains the temporary files for the statistic subsystem. 
As this directory is constantly written, is very likely to become an 
IO bottleneck. Setting the GUC parameter stats\_temp\_directory to a ramdisk 
speeds can improve the database performances.


\subsection{pg\_subtrans}\index{data area, pg\_subtrans}
Stores the subtransactions status data. 

\subsection{pg\_twophase}\index{data area, pg\_twophase}
Stores the two phase commit data. The two phase commit allows the transaction 
opening independently from the session. This way even a different session can 
commit or rollback the transaction later.

\subsection{pg\_tblspc}\index{data area, pg\_tblspc}
\label{sub:TABLESPACE}
The directory contains the symbolic links to the tablespace locations.
A tablespace is a logical name pointing a physical location. As from PostgreSQL 
9.2 the location is read directly from the symbolic link. This make possible 
to change the tablespace's position simply stopping the cluster, moving the 
data files in the new location, creating the new symlink and starting the 
cluster.
More informations about the tablespace management in the 
chapter \ref{cha:LOGICLAY}. 



