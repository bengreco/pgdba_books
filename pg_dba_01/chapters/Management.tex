\chapter{Managing the cluster}
\label{cha:MANAGING}
A PostgreSQL cluster is made of two components. A physical location initialised as data area 
and the postgres process attached to a shared memory segment, the shared buffer. The debian's 
package's installation, automatically set up a fully functional PostgreSQL cluster in the directory 
/var/lib/postgresql. This is good because it's possible to explore the product immediately.
However, 
it's not uncommon to find clusters used in production with the minimal default configuration's 
values, just because the binary installation does not make it clear what happens \textit{under the 
bonnet}.

This chapter will explain how a PostgreSQL cluster works and how critical is its 
management. 

\section{Initialising the data directory}

The data area is initialised by initdb\index{initdb}. The program requires an empty directory to 
write into to successful complete. Where the initdb binary is located depends from the installation 
method. We already discussed of this in \ref{cha:INSTALLSTRUCT} and \ref{cha:DB_INSTALL}. 

The accepted parameters for customising cluster's data area are various. Anyway, running 
initdb without parameters will make the program to use the value stored into the environment 
variable PGDATA. If the variable is unset the program will exit without any further action.\newline

For example, using the initdb shipped with the debian archive requires the following commands.

\begin{verbatim}
postgres@tardis:~/$ mkdir tempdata
postgres@tardis:~/$ cd tempdata
postgres@tardis:~/tempdata$ export PGDATA=`pwd`
postgres@tardis:~/tempdata$ /usr/lib/postgresql/9.3/bin/initdb 
The files belonging to this database system will be owned by user "postgres".
This user must also own the server process.

The database cluster will be initialized with locale "en_GB.UTF-8".
The default database encoding has accordingly been set to "UTF8".
The default text search configuration will be set to "english".

Data page checksums are disabled.

fixing permissions on existing directory /var/lib/postgresql/tempdata ... ok
creating subdirectories ... ok
selecting default max_connections ... 100
selecting default shared_buffers ... 128MB
creating configuration files ... ok
creating template1 database in /var/lib/postgresql/tempdata/base/1 ... ok
initializing pg_authid ... ok
initializing dependencies ... ok
creating system views ... ok
loading system objects' descriptions ... ok
creating collations ... ok
creating conversions ... ok
creating dictionaries ... ok
setting privileges on built-in objects ... ok
creating information schema ... ok
loading PL/pgSQL server-side language ... ok
vacuuming database template1 ... ok
copying template1 to template0 ... ok
copying template1 to postgres ... ok
syncing data to disk ... ok

WARNING: enabling "trust" authentication for local connections
You can change this by editing pg_hba.conf or using the option -A, or
--auth-local and --auth-host, the next time you run initdb.

Success. You can now start the database server using:

    /usr/lib/postgresql/9.3/bin/postgres -D /var/lib/postgresql/tempdata
or
    /usr/lib/postgresql/9.3/bin/pg_ctl -D /var/lib/postgresql/tempdata -l 
logfile start

\end{verbatim}

PostgreSQL 9.3 introduces\index{checksums, data page}\index{data page checksums} the data page 
checksums used for detecting the data page corruption. This great feature can be enabled only when 
initialising the data area with initdb and is cluster wide. The extra overhead caused by the 
checksums is something to consider because the only way to disable the data checksums is a dump 
and reload on a fresh data area.\newline

After initialising the data directory initdb emits the message with the commands to start the 
database cluster. The first form is useful for debugging and development purposes because it starts 
the database directly from the command line with the output displayed on the terminal. 

\begin{verbatim}
postgres@tardis:~/tempdata$ /usr/lib/postgresql/9.3/bin/postgres -D 
/var/lib/postgresql/tempdata
LOG:  database system was shut down at 2014-03-23 18:52:07 UTC
LOG:  database system is ready to accept connections
LOG:  autovacuum launcher started

\end{verbatim}

Pressing CTRL+C stops the cluster with a fast shutdown.\newline

Another reason for running postgres directly is when it needs to be started in single user mode. 
The --single option is a lifesaver if the cluster refuses to start because one or more 
databases are near the XID wraparound failure. 
\begin{verbatim}

postgres@tardis:~/tempdata$ /usr/lib/postgresql/9.3/bin/postgres --single -D /home/postgres/tempdata

PostgreSQL stand-alone backend 9.3.5
backend> 

\end{verbatim}

The database interface in single user mode and does not have all the sophisticated features 
like the client psql. Anywat with a little knowledge of SQL it's possible to find the database(s) 
causing the shutdown and fix it.
\index{postgres, single user mode}\index{XID wraparound failure, fix}

\begin{verbatim}
backend> SELECT datname,age(datfrozenxid) FROM pg_database ORDER BY 2 DESC;
         1: datname     (typeid = 19, len = 64, typmod = -1, byval = f)
         2: age (typeid = 23, len = 4, typmod = -1, byval = t)
        ----
         1: datname = "template1"       (typeid = 19, len = 64, typmod = -1, byval = f)
         2: age = "2146435072"  (typeid = 23, len = 4, typmod = -1, byval = t)
        ----
         1: datname = "template0"       (typeid = 19, len = 64, typmod = -1, byval = f)
         2: age = "10"  (typeid = 23, len = 4, typmod = -1, byval = t)
        ----
         1: datname = "postgres"        (typeid = 19, len = 64, typmod = -1, byval = f)
         2: age = "10"  (typeid = 23, len = 4, typmod = -1, byval = t)
        ----

\end{verbatim}

The age function shows how old is the last XID not yet frozen. In our example the template1
database have an age of 2146435072, one million transactions to the wraparound. We can then exit 
the backend with CTRL+D and restart it again in the in single user mode specifying the database 
name. A VACUUM will get rid of the problematic xid.

\begin{verbatim}
postgres@tardis:~/tempdata$ /usr/lib/postgresql/9.3/bin/postgres --single \
-D /home/postgres/tempdata template1

backend> VACUUM;
\end{verbatim}

This procedure must be repeated for any database with very old XID.\newline

Starting the cluster with pg\_ctl usage is very simple. This program also accepts the data area as 
parameter or using the environment variable PGDATA. It's also required to provide the command to 
execute. The start command for example is used to start the cluster in multi user mode.

\begin{verbatim}

postgres@tardis:~/tempdata$ /usr/lib/postgresql/9.3/bin/pg_ctl -D 
/var/lib/postgresql/tempdata -l logfile start
server starting

postgres@tardis:~/tempdata$ tail logfile 
LOG:  database system was shut down at 2014-03-23 19:01:19 UTC
LOG:  database system is ready to accept connections
LOG:  autovacuum launcher started

\end{verbatim}
Omitting the logfile with the -l will display the alerts and warnings on the terminal.

The command stop will end the cluster's activity.

\begin{verbatim}
postgres@tardis:~$ /usr/lib/postgresql/9.3/bin/pg_ctl -D 
/var/lib/postgresql/tempdata -l logfile stop
waiting for server to shut down.... done
server stopped
\end{verbatim}

\section{The startup sequence}
\label{sec:STARTUP}

When the postgres process starts it allocates the shared memory segment called shared buffer. The
size of this segment is specified with the GUC parameter shared\_buffers.The version 9.3, on the
unix systems, changed the memory allocation method to mmap(). This eliminated any need to adjust
the kernel's parameters.

For the versions up to the 9.2, if the requested memory is bigger than the kernel's maximum
allowed size, the startup sequence will abort with an error like this.

\begin{verbatim}
FATAL: could not create shared memory segment: Cannot allocate memory

DETAIL: Failed system call was shmget(key=X, size=XXXXXX, XXXXX).

HINT: This error usually means that PostgreSQL's request for a shared memory
segment exceeded available memory or swap space, or exceeded your kernel's
SHMALL parameter. You can either reduce the request size or reconfigure the
kernel with larger SHMALL. To reduce the request size (currently XXXXX bytes),
reduce PostgreSQL's shared memory usage, perhaps by reducing shared_buffers or
max_connections.
\end{verbatim}

\index{kernel resources}
The kernel's parameters governing this limit is SHMMAX. It sets the maximum allowed size of a shared
memory segment. The value is measured in bytes and must be big enogh to contain the
requested shared\_buffers. Another parameter which needs adjustment is SHMALL. This value sets the
amount of total shared memory available. On linux is usually measured in pages. Unless the
kernel is configured to allow the huge pages the page size is 4096 byes. The value should be the
same as SHMMAX. \newline

If we want to set a pre 9.3 with a shared buffer to 1 GB the SHMMAX should be at least 1073741824.
The value 1258291200 (1200 MB) is a reasonable setting ang gives us some extra headroom. The
corresponding SHMALL is 307200. The value SHMMNI is the minimum value of the shared memory, is safe
to set to 4096, just one memory page. 

The settings can be changed on the fly, simply echoing in the corresponding proc entries or setting
the values persistently into the file /etc/sysctl.conf.

Here's the file from the previous example.
\begin{verbatim}
kernel.shmmax = 1258291200
kernel.shmall = 307200
kernel.shmmni = 4096
kernel.sem = 250 32000 100 128
fs.file-max = 658576
\end{verbatim}

To apply the changes login as root and run \textit{sysctl -p}.\newline


When the memory is allocated the postmaster reads the pg\_control
file to check if the instance requires recovery. The pg\_control file is used to store the locations
to the last checkpoint and the last known status for the instance.\newline

If  the instance is in dirty state, because a crash or an unclean shutdown, the startup
process reads the last checkpoint location and replays the blocks from the corresponding WAL
segment in the pg\_xlog directory. Any corruption in the wal files during the recovery or the
pg\_control file results in a not startable instance.\newline

When the recovery is complete or if the cluster's state is clean the postgres process completes the
startup and sets the cluster in production state. 


\section{The shutdown sequence} 
\label{sec:SHUTDOWN_SEQ}
\index{shutdown sequence}

The PostgreSQL process enters the shutdown status when a specific OS signal is received. The signal
can be sent via the os kill or using the program pg\_ctl. \newline

As seen in \ref{sub:PGCTL} pg\_ctl accepts the -m switch when the command is stop. The -m switch
is used to specify the shutdown mode and if is omitted it defaults to smart which corresponds to
the SIGTERM signal. With the smart shuthdown the cluster stops accepting new connections and
waits for all backends to quit. \newline

When the shutdown mode is set to fast pg\_ctl sends the SIGQUIT signal to the postgres main process.
Same as for the smart shutdown the cluster does not accepts new connections terminates the existing
backends. Any open transaction is rolled back as well. \newline

When the smart and the fast shutdown are complete they leave the cluster in clean state. This is
true because when the postgres process initiate the final part of the shutdown it starts a
last checkpoint which consolidates any dirty block on the disk. Before quitting the postgres
process saves the latest checkpoint's location to the pg\_control file and marks the
cluster as clean.\newline

The checkpoint can slow down the entire shutdown sequence. In particular if the shared\_buffer is
big and contains many dirty blocks, the checkpoint can run for a very long time. Also if at
the shutdown time, another checkpoint is running the postgres process will wait for this
checkpoint to complete before starting the final checkpoint.\newline

Enabling the log checkpoints in the configuration gives us some visibility on what the cluster is
actually doing. The GUC parameter governing the setting is log\_checkpoints.\newline


If the cluster doesn't stop, there is a shutdown mode which leaves the cluster in dirty state.
The immiediate shutdown. The equivalent signal is the SIGQUIT and it causes the main process
alongside with the backends to quit immediately without the checkpoint.\newline

The subsequent start will require a crash recovery. The recovery is usually harmless with one
important exception. If the cluster contains unlogged tables those relations are recreated from
scratch when the recovery happens and all the data in those table is lost.

A final word about the SIGKILL signal, the dreaded kill -9. It can happen the cluster refuse to
stop even using the immediate mode. In this case, as last resort the SIGKILL. Because this signal
cannot be trapped in any way, the resources like the shared memory and the inter process semaphores
will stay in place after the kill. This will very likely affect the start of a fresh instance.
Please refer to your sysadmin to find out the best way to cleanup the memory after the
SIGKILL.

\section{The processes}
\label{sec:PROCESSES}
Alongside with postgres process there are a number of accessory processes. With a running 9.3
cluster ps shows at least six postgres processes. 

\subsection{postgres: checkpointer process}
As the name suggests this process take care of the cluster's checkpoint\index{checkpoint} activity.
A checkpoint is an important event in the cluster's life. When it starts all the dirty pages in
memory are written to the data files. The checkpoint frequency is regulated by the time and the
number of cluster's WAL switches.The GUC parameters governing this metrics are respectively
checkpoint\_timeout\index{checkpoint\_timeout} and checkpoint\_segments\index{checkpoint\_segments}.
There is a third parameter, the checkpoint\_completion\_target\index{checkpoint\_completion\_target}
which sets the percentage of the checkpoint\_timeout. The cluster uses this value to spread the
checkpoint over this time in order to avoid a big disk IO spike.

\subsection{postgres: writer process}
The background writer scans the shared buffer searching for dirty pages which writes on the data
files. The process is designed to have a minimal impact on the database activity. It's possible to
tune the length of a run and the delay between the writer's runs using the GUC parameters
bgwriter\_lru\_maxpages\index{bgwriter\_lru\_maxpages} and bgwriter\_delay\index{bgwriter\_delay}.
They are respectively the number of dirty buffers written before the writer's sleep and the time
between two runs.

\subsection{postgres: wal writer process}
This background process has been introduced with the 9.3 in order to make the WAL writes a more
efficient. The process works in rounds and writes down the wal buffers to the  wal files. The GUC
parameter wal\_writer\_delay\index{wal\_writer\_delay} sets the milliseconds to sleep between the
rounds. 

\subsection{postgres: autovacuum launcher process}
This process is present if the autovacuum\index{autovacuum} is enabled. It's purpose is to launch
the autovacuum backends when needed. 

\subsection{postgres: stats collector process}
The process gathers the database's usage statistics and stores the information to the location
indicated by the GUC stats\_temp\_directory. This is by default pg\_stat\_temp, a relative path to
the data area. 

\subsection{postgres: postgres postgres [local] idle}
This is a database backend. There is one backend for each established connection. The values after
the colon show useful information. In particular between the square brackets there is the query
the backend is executing. 

\section{The memory}
\label{sec:MEMORY}
The PostgreSQL's memory structure is not complex like other databases.
In this section we'll take a to the various parts. 

\subsection{The shared buffer}
\index{shared buffer}
The shared buffer, as the name suggests is the segment of shared memory used by 
PostgreSQL to manage the data pages. 

Its size is set using the GUC\footnote{GUC, Grand Unified Configuration, this 
acronym refers to the parameters used to configure the instance} parameter 
shared\_buffers and is allocated 
during the startup process.Any change requires the instance restart.

The segment is formatted in blocks with the same size of the data file's 
blocks, usually 8192 bytes. Each backend connected to the cluster is attached 
to this segment. Because usually its size is a fraction of the cluster's size, 
a simple but very efficient mechanism keeps in memory the blocks using a 
combination of LRU and MRU.

Since the the version 8.3 is present a protection mechanism to avoid the 
massive block eviction when intensive IO operations, like vacuum or big 
sequential reads, happens.

Each database operation, read or write, is performed moving the blocks via the 
shared buffer. This ensure an effective caching process and the memory routines 
guarantee the consistent read and write at any time.

PostgreSQL, in order to protect the shared buffer from potential corruption, if 
any unclean disconnection happens, resets by default all the connections. 

This behaviour can be disabled in the configuration file but exposes the shared 
buffer to data corruption if the unclean disconnections are not correctly 
managed.



\subsection{The work memory}\index{work memory}
\label{sub:WORKMEM}
This memory segment is allocated per user and its default value is set using 
the GUC parameter work\_mem. The value can be altered for the session on the 
fly. When changed in the global configuration file becomes effective to the 
next transaction after the instance reload. 

This segment is used mainly for expensive operations like the sort or the 
hash.

If the operation's memory usage exceeds the work\_mem value then the PostgreSQL 
switches to a disk sort/hash. 

Increasing the work\_mem value results generally in better performances for 
sort/hash operations. 

Because is a per user memory segment, the potential amount of memory 
required in a running instance is max\_connections * work\_mem. It's very 
important to set this value to a reasonable size in order to avoid any risk of 
out of memory error or unwanted swap.

In complex queries is likely to have many sort or hash operations in parallel 
and each one consumes the amount of work\_mem for the 
session.


\subsection{The maintenance work memory}\index{maintenance work memory}
The maintenance work memory is set using the GUC parameter 
maintenance\_work\_mem and follow the same rules of work\_mem. This memory 
segment is allocated per user and is used for the maintenance operations 
like VACUUM or REINDEX. As usually this kind of operations happens on one 
relation at time, this parameter can be safely set to a bigger value than 
work\_mem.

\subsection{The temporary memory}
\label{sub:TEMPBUF}
The temporary memory is set using the GUC parameter temp\_buffers. This is the 
amount of memory per user for the temporary table creation before the disk is 
used. Same as for the work memory and the maintenance work memory it's possible 
to change the value for the current session but only before any temporary table 
creation. After this the parameter cannot be changed anymore.


\section{The data area}
\label{sec:PGDATA}\index{data area}
As seen before the data storage area is initialized by initdb \index{initdb}.
Its structure didn't change too much from the old fashioned 7.4.
In this section we'll take a look to the various subdirectories and how their 
usage can affect the performances. 


\subsection{base}\index{data area,base}
\label{sub:BASE}
As the name suggests, the base directory contains the database files. Each 
database have a dedicated subdirectory, named after the internal database's 
object id.
A freshly initialised data directory shows only three 
subdirectories in the base folder.

Those corresponds to the two template databases,template0 and template1, plus 
the postgres database. Take a look to chapter \ref{cha:LOGICLAY} for more 
informations.

The numerical directories contains various files, also with the numerical name 
which are actualy the database's relations, tables and indices. 

The relation's name is set initially from the relation's object id. Any file 
altering operation like VACUUM FULL or REINDEX, will generate a new file 
with a different name. To find out the real relation's file name the 
relfilenode inside the pg\_class system table  must be queried.

\subsection{global}\index{data area,global}
The global directory contains all the cluster wide relations.
In addition there's the very critical  control file mentioned in 
\ref{sub:PGCONTROLDATA} \index{control file}.

This small file is big exactly one database block, usually 8192 bytes, and 
contains critical informations for the cluster. 
With a corrupted control file the instance cannot start. 
The control file is written usually when a checkpoint occurs.

\subsection{pg\_xlog}\index{data area,pg\_xlog}
This is the most important and critical directory, for the performances and for 
the reliability. 

The directory contains the transaction's logs, \index{wal files} named wal 
file. Each file is usually 16 Mb and contains all the data blocks changed during 
the database activity. 
The blocks are written first on this not volatile area to ensure the cluster's 
recovery in case of cras. The data blocks are then written later to the 
corresponding data files. If the cluster's shutdown is not clean then the wal 
files are replayed during the startup process from the last known consistent 
location read from control file.

In order to ensure good performance this location should stay on a dedicated 
device. 

\subsection{pg\_clog}\index{data area, pg\_clog}
This directory contains the committed transactions in small 8k files, except 
for the serializable transactions.
The the files are managed by the cluster and the amount is related with 
the two GUC parameters autovacuum\_freeze\_max\_age and 
vacuum\_freeze\_table\_age.
Increasing the values for the two parameters the pg\_clog must store the commit 
status to the ``event horizon'' of the oldest frozen transaction id. 
More informations about vacuum and the maintenance are in 
the chapter \ref{cha:MAINTENANCE}.

\subsection{pg\_serial}\index{data area, pg\_serial}
Same as pg\_clog this directory stores the informations about the commited 
transactions in serializable transaction isolation level.

\subsection{pg\_multixact}\index{data area, pg\_multixact}
Stores the informations about the multi transaction status, used generally for  
the row share locks.

\subsection{pg\_notify}\index{data area, pg\_notify}
Stores informations about the LISTEN/NOTIFY operations.

\subsection{pg\_snapshots}\index{data area, pg\_snapshots}
This directory is used to store the exported snapshots. From the version 9.2 
PostgreSQL offers the transaction's snapshot export where one session can 
open a transaction and export a consistent snapshot. This way different session 
can access the snapshot and read all togheter the same consistent data 
snapshot. This feature is used, for example, by pg\_dump for the parallel 
export.

\subsection{pg\_stat}\index{data area, pg\_stat}
This directory contains the permanent files for the statistic subsystem. 

\subsection{pg\_stat\_tmp}\index{data area, pg\_stat\_tmp}
This directory contains the temporary files for the statistic subsystem. 
As this directory is constantly written, is very likely to become an 
IO bottleneck. Setting the GUC parameter stats\_temp\_directory to a ramdisk 
speeds can improve the database performances.


\subsection{pg\_subtrans}\index{data area, pg\_subtrans}
Stores the subtransactions status data. 

\subsection{pg\_twophase}\index{data area, pg\_twophase}
Stores the two phase commit data. The two phase commit allows the transaction 
opening independently from the session. This way even a different session can 
commit or rollback the transaction later.

\subsection{pg\_tblspc}\index{data area, pg\_tblspc}
\label{sub:TABLESPACE}
The directory contains the symbolic links to the tablespace locations.
A tablespace is a logical name pointing a physical location. As from PostgreSQL 
9.2 the location is read directly from the symbolic link. This make possible 
to change the tablespace's position simply stopping the cluster, moving the 
data files in the new location, creating the new symlink and starting the 
cluster.
More informations about the tablespace management in the 
chapter \ref{cha:LOGICLAY}. 



