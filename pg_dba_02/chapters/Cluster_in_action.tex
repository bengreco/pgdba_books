\chapter{The cluster in action}
PostgreSQL delivers his services ensuring the ACID rules are enforced at any time. This chapter will give 
an outlook of a ``day in the life'' of a PostgreSQL's cluster. The chapter approach is purposely generic. 
At this stage is very important to understand the global picture rather the technical details. 

\section{After the startup}
When the cluster completes the startup procedure it starts accepting the connections. When a connection  
is successful then the postgres main process forks into a new backend process which is assigned to the 
connection for the connection's lifetime. The fork is quite expensive and does not work very well for a 
high rate of connection's requests. The maximum number of connections is set at startup and cannot be 
changed dynamically. Whether the connection is used or not for each connection slot are consumed ~400 bytes 
of shared memory.\newline

Alongside the client's request the cluster have several subprocesses working in the background. 

\section{The write ahead log}\index{write ahead log} 
The data pages are stored into the shared buffer either for read and write. A mechanism called pinning 
ensures that only one backend at time is accessing the requested page. If the backend modifies the page then 
this becomes dirty\index{page, dirty}. A dirty page is not yet written on its data file. However the page's 
change is first saved on the write ahead log as WAL record and the commit status for the transactions is 
then in the directory clog or the directory pg\_serial, depending on the transaction isolation level. The 
wal records are stored into a shared buffer's area sized by the parameter wal\_buffers before the flush on 
disk into the pg\_xlog directory on fixed length segments. When a WAL segment is full then a a new one is 
created or recycled. When this happens there is a xlog switch. The writes on the WAL are managed by a 
background process called WAL writer\index{WAL writer}. This process were first introduced with PostgreSQL 
8.3.

\section{The checkpoint}
The cluster, on a regular basis, executes an important activity called checkpoint. The frequency 
of this action is governed by the time and space, measured respectively in seconds and log switches between 
two checkpoints. The checkpoint scans the shared buffer and writes down to the data files all the dirty 
pages. When the checkpoint is complete the process determines the checkpoint location and writes this 
information on the control file stored into the cluster's pg\_global tablespace. In the case of unclean 
shutdown this value is used to determine the WAL segment from where to start the crash recovery. \newline

Before the version 8.3 the checkpoint represented a potential bottleneck because the unavoidable IO spike 
generated during the writes. That's the reason why the version 8.3 introduced the concept of spread 
checkpoints. The cluster aims to a particular completion target time measured in percent of the checkpoint 
timeout. The default values are respectively 0.5 and 5 minutes. This way the checkpoint will spread over a 
target time of 2.5 minutes. From PostgreSQL 9.2 a new checkpointer process has been created to manage 
efficiently the checkpoint.


\section{The background writer}
Before the spread checkpoints the only solution to ease down the IO spike caused by the checkpoint was to 
tweak the background writer. This process were introduced with the revolutionary PostgreSQL 8.0. The 
writer, as the name suggests, works in the background searching for dirty buffers to write on the data 
files. The writer works in rounds. When the process awakes scans the shared buffer for dirty buffers. When 
the amount of buffers cleaned reaches the value set in bgwriter\_lru\_maxpages the process sleeps for the 
time set in bgwriter\_delay. 

\section{The autovacuum}
The routine vacuuming is an important task to prevent the table bloat and the dreaded XID wraparound 
failure. If enabled the autovacuum launcher starts one daemon for each relation with enough dead tuples to 
trigger the conditions set in autovacuum\_vacuum\_threshold and autovacuum\_vacuum\_scale\_factor. An 
autovacuum daemon is a normal backend and appears in the view pg\_stat\_activity. Because the XID wraparound 
failure is a really serious problem, the autovacuum to prevent wraparound starts even if the autovacuum is 
turned off.

\section{The backends}
The PostgreSQL backend architecture is the brilliant solution to a nasty problem. How to guarantee the 
buffers are read only by one session at time and avoid the bottleneck of a long waiting queue. When a 
backend needs to access a particular tuple, either for read or write, the relation's pages are accessed to 
find the tuple matching the search criteria. When a buffer is accessed then the backend sets a pin on the 
buffer which prevents the other backends requiring the same page to wait. As soon as the tuple is found and 
processed the pin is removed. If the tuple is modified the MVCC enforces the tuple's visibility to the other 
backends. The process is fine grained and very efficient. Even with an high concurrency rate on the same 
buffers is very difficult to have the backends entangled.\newline

A backend process is a fork of the main postgres process. It's very important to understand that the 
backend is not the connection but a server process which interacts with the connection. Usually the backend 
terminates when the connection disconnects. However, if a client disconnects ungracefully meanwhile a query 
is running without signalling the backend, the query will continue only to find there's nothing listening 
on the other side. This is bad for many reasons. First because is consuming a connection slot for nothing. 
Also the cluster is doing something useless consuming CPU cycles and memory. \newline

Like everything in PostgreSQL the backend architecture is oriented to protect the data and in particular 
the volatile shared buffer. If for some reasons one of the backend process crashes then the postgres 
process terminates all the backends in order to prevent the potential shared buffer corruption. The clients 
should be able to manage this exception resetting the connection.\newline

\section{Wrap up}
The cluster's background activity remains most of the time unnoticed. The users and developers can mostly 
ignore this aspect of the PostgreSQL architecture leaving the difficult business of understanding the 
database heartbeat to the DBA, which should have the final word on any potential mistake in the design 
specs. The next chapters will explore the PostgreSQL's architecture in details, starting with the memory.
